Lillian Diana Gish (14 de octubre de 1893 – 27 de febrero de 1993) fue una actriz, directora y guionista estadounidense.
Gish fue una estrella de cine prominente desde 1912 hasta la década de 1920, particularmente asociada con las películas del director D. W. Griffith.
También hizo un considerable trabajo en televisión desde principios de la década de 1950 hasta la década de 1980, y cerró su carrera interpretando junto a Bette Davis en la película de 1987 The Whales of August.
Las primeras generaciones de Gishes fueron ministros Dunkard.
Su madre abrió la Majestic Candy Kitchen, y las chicas ayudaron a vender palomitas de maíz y dulces a los clientes del antiguo Majestic Theater, ubicado al lado.
Lillian, de diecisiete años, viajó a Shawnee, Oklahoma, donde vivían el hermano de James, Alfred Grant Gish, y su esposa, Maude.
Su padre murió en Norman, Oklahoma, en 1912, pero ella había regresado a Ohio unos meses antes de esto.
Cuando Lillian y Dorothy tenían la edad suficiente se unieron al teatro, a menudo viajando por separado en diferentes producciones.
Gish continuó actuando en el escenario, y en 1913, durante una carrera de A Good Little Devil, se derrumbó de anemia.
Su desempeño en estas condiciones gélidas le causó daño nervioso duradero en varios dedos.
Utilizó sus talentos expresivos al máximo, convirtiéndola en una heroína sufriente pero fuerte.
Dirigió a su hermana Dorothy en una película, Remodeling Her Husband (1920), cuando D. W. Griffith tomó su unidad en el lugar.
Rechazó el dinero, solicitando un salario más modesto y un porcentaje para que el estudio pudiera usar los fondos para aumentar la calidad de sus películas, contratando a los mejores actores, guionistas, etc.
Muchas de las mujeres principales de la era silenciosa, como Gish y Pickford, habían sido sanas e inocentes, pero antes de principios de los años 1930 (después de la adopción llena del sonido y antes de que el Código de Producción de la Película se hiciera cumplir) estos papeles se percibieron como anticuados.
Louis Mayer quiso organizar un escándalo ("noquearla de su pedestal") para recoger la simpatía pública por Gish, pero Lillian no quiso actuar tanto en pantalla como fuera, y volvió a su primer amor, el teatro.
Volviendo a las películas, Gish fue nominado para el Premio de la Academia a la Mejor Actriz de Reparto en 1946 por Duel in the Sun.
Fue considerada para varios papeles en Lo que el viento se llevó, desde Ellen O'Hara, la madre de Scarlett (que fue a Barbara O'Neil), hasta la prostituta Belle Watling (que fue a Ona Munson).
Apareció como la emperatriz viuda María Feodorovna en el breve musical de Broadway Anya de 1965.
Fue entrevistada en la serie documental de televisión Hollywood: A Celebration of the American Silent Film (1980).
Tiene una estrella en el Paseo de la Fama de Hollywood en 1720 Vine Street.
En el festival de Cannes, Gish ganó una ovación de pie de 10 minutos de la audiencia.
El episodio "Marry for Murder" fue emitido el 9 de septiembre de 1943.
Fue galardonada con un Premio Honorario de la Academia en 1971, y en 1984 recibió un AFI Life Achievement Award.
La Universidad otorgó a Gish el grado honorario de Doctor en Artes Escénicas al día siguiente.
Después de la muerte de Gish en 1993, la Universidad recaudó fondos para ampliar su galería para mostrar los recuerdos recibidos de la finca de Gish.
La asociación entre ella y D. W. Griffith fue tan estrecha que algunos sospecharon una conexión romántica, un problema nunca reconocido por Gish, aunque varios de sus asociados estaban seguros de que estaban involucrados al menos brevemente.
En la década de 1920, la asociación de Gish con Duell se convirtió en un escándalo sensacionalista cuando la demandó e hizo públicos los detalles de su relación.
George Jean Nathan elogió la actuación de Gish, comparándola con Eleonora Duse.
Durante el período de agitación política en los Estados Unidos que duró desde el estallido de la Segunda Guerra Mundial en Europa hasta el ataque a Pearl Harbor, mantuvo una postura abiertamente no intervencionista.
Joseph Frank Keaton (4 de octubre de 1895 – 1 de febrero de 1966), conocido profesionalmente como Buster Keaton, fue un actor, comediante, director de cine, productor, guionista y doble de acción estadounidense.
Su carrera declinó cuando firmó con Metro-Goldwyn-Mayer y perdió su independencia artística.
Muchas de las películas de Keaton de la década de 1920 siguen siendo muy apreciadas, como Sherlock Jr. (1924), The General (1926) y The Cameraman (1928).
Su padre era Joseph Hallie "Joe" Keaton, que poseyó un espectáculo itinerante con Harry Houdini llamó Mohawk Indian Medicine Company o Keaton Houdini Medicine Show Company, que funcionó en la etapa y vendió la medicina patentada en el lado.
En el recuento de Keaton, tenía seis meses cuando ocurrió el incidente, y Harry Houdini le dio el apodo.
El acto fue principalmente un sketch cómico.
Se cosió un asa de maleta en la ropa de Keaton para ayudar con el constante lanzamiento.
Sin embargo, Buster siempre pudo demostrar a las autoridades que no tenía moretones ni huesos rotos.
Varias veces me habrían matado si no hubiera podido aterrizar como un gato.
Al darse cuenta de que esto hacía que el público se riera menos, adoptó su famosa expresión inexpresiva al actuar.
A pesar de enredos con la ley y un viaje desastroso de pasillos de la música en el Reino Unido, Keaton era una estrella en ascenso en el teatro.
En febrero de 1917, conoció a Roscoe "Fatty" Arbuckle en los Estudios Talmadge en la ciudad de Nueva York, donde Arbuckle estaba bajo contrato con Joseph M. Schenck.
Buster fue tan natural en su primera película, The Butcher Boy, que fue contratado en el acto.
Keaton más tarde afirmó que pronto era el segundo director de Arbuckle y todo su departamento de mordaza.
Estaba basado en un juego exitoso, La Nueva Henrietta, que se había filmado ya una vez, bajo el título El Cordero, con Douglas Fairbanks que juega el plomo.
Hizo una serie de comedias de dos carretes, incluso Una semana (1920), La Casita de muñecas (1921), Policías (1922), y La Casa Eléctrica (1922).
El director de comedia Leo McCarey, recordando los días despreocupados de hacer comedias, dijo: "Todos nosotros tratamos de robar las mordazas de los demás.
Durante la escena del tanque de agua del ferrocarril en Sherlock Jr., Keaton se rompió el cuello cuando un torrente de agua cayó sobre él desde una torre de agua, pero no se dio cuenta hasta años después.
El personaje de Keaton salió ileso, debido a una sola ventana abierta.
Además de Steamboat Bill, Jr. (1928), los largometrajes más duraderos de Keaton incluyen Our Hospitality (1923), The Navigator (1924), Sherlock Jr. (1924), Seven Chances (1925), The Cameraman (1928) y The General (1926).
Aunque llegaría a ser considerado como el mayor logro de Keaton, la película recibió críticas mixtas en ese momento.
Su distribuidor, United Artists, insistió en un gerente de producción que supervisara los gastos e interfiriera con ciertos elementos de la historia.
Los actores memorizaban fonéticamente los guiones en idioma extranjero unas pocas líneas a la vez y disparaban inmediatamente después.
El director era generalmente Jules White, cuyo énfasis en el slapstick y la farsa hizo que la mayoría de estas películas se parecieran a los famosos cortos de los Tres Chiflados de White.
Sin embargo, la insistencia del director White en bromas contundentes y violentas resultó en que los cortos de Columbia fueran las comedias menos inventivas que hizo.
Hizo su última película protagonizada por El Moderno Barba Azul (1946) en México; la película fue una producción de bajo presupuesto, y puede que no se haya visto en los Estados Unidos hasta su lanzamiento en VHS en la década de 1980, bajo el título Boom in the Moon.
En In the Good Old Summertime, Keaton dirigió personalmente a las estrellas Judy Garland y Van Johnson en su primera escena juntos, donde se encuentran en la calle.
La reacción fue lo suficientemente fuerte como para que una estación local de Los Ángeles le ofreciera a Keaton su propio programa, también transmitido en vivo, en 1950.
La esposa de Buster Keaton, Eleanor, también fue vista en la serie (notablemente como Julieta del Romeo de Buster en una pequeña viñeta de teatro).
Las apariciones periódicas de Keaton en televisión durante las décadas de 1950 y 1960 ayudaron a revivir el interés en sus películas mudas.
Bien entrados los cincuenta años, Keaton recreó con éxito sus viejas rutinas, incluido un truco en el que apoyó un pie en una mesa, luego balanceó el segundo pie junto a él y mantuvo la posición incómoda en el aire por un momento antes de estrellarse contra el piso del escenario.
Keaton tenía impresiones de los rasgos Tres Edades, Sherlock, Hijo, Steamboat Bill, Hijo, y Colegio (faltando un carrete), y los cortos "El Barco" y "Relaciones de Mi Esposa", que Keaton y Rohauer entonces transfirieron a la película del acetato de Celulosa de deteriorar la reserva de la película del nitrato.
En una serie de anuncios de televisión silenciosa para Simon Pure Beer hecha en 1962 por Jim Mohr en Buffalo, Nueva York, Keaton volvió a visitar algunos de los chistes de sus días de cine mudo.
En diciembre de 1958, Keaton fue una estrella invitada en el episodio "A Very Merry Christmas" de The Donna Reed Show en ABC.
En 1960, regresó a MGM por última vez, interpretando a un domador de leones en una adaptación de 1960 de Las aventuras de Huckleberry Finn de Mark Twain.
Trabajó con el comediante Ernie Kovacs en un piloto de televisión titulado tentativamente "Medicine Man", filmando escenas para él el 12 de enero de 1962, el día antes de que Kovacs muriera en un accidente automovilístico.
Viajó de un extremo de Canadá al otro en un coche de mano motorizado, vistiendo su tradicional sombrero de pastel de cerdo y realizando bromas similares a las de las películas que hizo 50 años antes.
También en 1965, viajó a Italia para desempeñar un papel en Due Marines e un Generale, coprotagonizada por Franco Franchi y Ciccio Ingrassia.
Una de sus parodias más mordaces es The Frozen North (1922), una versión satírica de los melodramas occidentales de William S. Hart, como Hell's Hinges (1916) y The Narrow Trail (1917).
El público de la década de 1920 reconoció la parodia y pensó que la película era histéricamente divertida.
El corto también presentó la impresión de un mono que realiza que probablemente se sacó del acto de un co-biller (llamó a Peter el Grande).
Nota: La fuente escribe mal la denominación frecuente de Keaton como "Great Stoneface".
Keaton salió con la actriz Dorothy Sebastian a principios de la década de 1920 y con Kathleen Key a principios de la década de 1930.
Se escapó de una camisa de fuerza con trucos aprendidos de Harry Houdini.
Ella solicitó el divorcio en 1935 después de encontrar a Keaton con Leah Clampitt Sewell, la esposa del millonario Barton Sewell, en un hotel en Santa Bárbara.
Dejó de beber durante cinco años.
El matrimonio duró hasta su muerte.
Confinado a un hospital durante sus últimos días, Keaton estaba inquieto y caminaba por la habitación sin cesar, deseando regresar a casa.
El guión, de Sidney Sheldon, quien también dirigió la película, se basó libremente en la vida de Keaton, pero contenía muchos errores de hechos y fusionó a sus tres esposas en un solo personaje.
Dedicada a atraer una mayor atención pública a la vida y el trabajo de Keaton, la membresía incluye a muchas personas de la industria de la televisión y el cine: actores, productores, autores, artistas, novelistas gráficos, músicos y diseñadores, así como aquellos que simplemente admiran la magia de Buster Keaton.
Hirschfeld dijo que las estrellas de cine modernas eran más difíciles de representar, que los comediantes del cine mudo como Laurel y Hardy y Keaton "parecían sus caricaturas".
El crítico de cine Roger Ebert declaró: "El más grande de los payasos silenciosos es Buster Keaton, no sólo por lo que hizo, sino por la forma en que lo hizo.
El cineasta Mel Brooks ha acreditado a Buster Keaton como una gran influencia, diciendo: "Le debo mucho a (Buster) en dos niveles: uno por ser un gran maestro para mí como cineasta, y el otro como un ser humano viendo a esta persona talentosa haciendo estas cosas increíbles.
El actor y doble Johnny Knoxville cita a Keaton como una inspiración cuando se le ocurren ideas para proyectos de Jackass.
Lewis estaba particularmente conmovido por el hecho de que Eleanor dijo que sus ojos se parecían a los de Keaton.
En 1964, le dijo a un entrevistador que al hacer "este pastel de cerdo en particular", "comenzó con un buen Stetson y lo cortó", endureciendo el borde con agua azucarada.
Sus bisabuelos paternos eran galeses.
Lloyd comenzó a colaborar con Roach, que había formado su propio estudio en 1913.
En 1919, dejó a Lloyd para perseguir sus aspiraciones dramáticas.
Según se informa, cuanto más observaba Lloyd a Davis, más le gustaba ella.
Harold Lloyd se alejaba de las personas tragicómicas y retrataba a un hombre común con confianza y optimismo inquebrantables.
Para crear su nuevo personaje, Lloyd se puso un par de gafas con montura de cuerno sin lentes, pero llevaba ropa normal; anteriormente, había usado un bigote falso y ropa mal ajustada como el Chaplinesque "Lonesome Luke".
Eran naturales y el romance podía ser creíble”.
El domingo 24 de agosto de 1919, mientras posaba para algunas fotografías promocionales en el Witzel Photography Studio de Los Ángeles, recogió lo que pensó que era una bomba de utilería y la encendió con un cigarrillo.
Lloyd estaba en el acto de encender un cigarrillo de la mecha de la bomba cuando explotó, también quemándose gravemente la cara y el pecho e hiriendo su ojo.
Lloyd y Roach se separaron en 1924, y Lloyd se convirtió en el productor independiente de sus propias películas.
Todas estas películas eran enormemente exitosas y provechosas, y Lloyd finalmente se haría el ejecutante de la película mejor pagado de los años 1920.
Sin embargo, su personaje en la pantalla estaba fuera de contacto con las audiencias de películas de la Gran Depresión de la década de 1930.
El 23 de marzo de 1937, Lloyd vendió la tierra de su estudio, Harold Lloyd Motion Picture Company, a La iglesia de Jesucristo de Santos Actuales.
Regresó para una aparición estelar adicional en The Sin of Harold Diddlebock, un desafortunado homenaje a la carrera de Lloyd, dirigida por Preston Sturges y financiada por Howard Hughes.
Lloyd y Sturges tenían concepciones diferentes del material y lucharon con frecuencia durante el rodaje; Lloyd en particular se preocupó que mientras Sturges hubiera gastado tres a cuatro meses en el guión del primer tercio de la película, "los dos tercios últimos de ello escribió en una semana o menos".
Algunos vieron The Old Gold Comedy Theater como una versión más ligera de Lux Radio Theater, y contó con algunas de las personalidades de cine y radio más conocidas de la época, incluyendo Fred Allen, June Allyson, Lucille Ball, Ralph Bellamy, Linda Darnell, Susan Hayward, Herbert Marshall, Dick Powell, Edward G. Robinson, Jane Wyman y Alan Young.
Muchos años más tarde, los discos de acetato de 29 de los espectáculos fueron descubiertos en la casa de Lloyd, y ahora circulan entre los coleccionistas de radio de antaño.
Fue un Potentado Pasado del Santuario de Al-Malaikah en Los Ángeles, y finalmente fue seleccionado como Potentado Imperial de los Shriners de América del Norte para el año 1949-50.
Lloyd fue investido con el rango y la decoración de la Corte de Honor del Comandante Caballero en 1955 y coronó a un Inspector General Honorario, 33 °, en 1965.
Como primer paso, Lloyd escribirá la historia de su vida para Simon y Schuster.
Se hizo conocido por sus fotografías de desnudos de modelos, como Bettie Page y la stripper Dixie Evans, para varias revistas masculinas.
Nunca quisimos que se tocaran con pianos”.
Se han acercado a eso, pero no han llegado hasta el final”.
A principios de los años 1960, Lloyd produjo dos películas de la compilación, presentando escenas de sus viejas comedias, el mundo de Harold Lloyd de la Comedia y El Lado Divertido de Vida.
Time-Life lanzó varios de los largometrajes más o menos intactos, también usando algunas de las partituras de Scharf que habían sido encargadas por Lloyd.
El documental de Brownlow y Gill se mostró como parte de la serie de PBS American Masters, y creó un renovado interés en el trabajo de Lloyd en los Estados Unidos, pero las películas no estaban disponibles en gran medida.
También adoptaron a Gloria Freeman (1924-1986) en septiembre de 1930, a quien renombraron Marjorie Elizabeth Lloyd, pero fue conocida como "Peggy" durante la mayor parte de su vida.
Davis murió de un ataque al corazón en 1969, dos años antes de la muerte de Lloyd.
En 1925, en el apogeo de su carrera cinematográfica, Lloyd entró en la masonería en el Alexander Hamilton Lodge No.
En 1926, se convirtió en un masón del Rito Escocés 32 ° en el Valle de Los Ángeles, California.
Una parte del inventario personal de Lloyd de sus películas mudas (que entonces se estimaba en 2 millones de dólares) fue destruida en agosto de 1943 cuando su cámara acorazada se incendió.
El fuego ahorró la casa principal y las dependencias.
Lloyd fue honrado en 1960 por su contribución a las películas con una estrella en el Paseo de la Fama de Hollywood ubicado en 1503 Vine Street.
La segunda citación fue un desaire a Chaplin, quien en ese momento había cometido una falta de macartismo y se le revocó su visa de entrada a los Estados Unidos.
Gladys Marie Smith (8 de abril de 1892 – 29 de mayo de 1979), conocida profesionalmente como Mary Pickford, fue una actriz y productora de cine canadiense-estadounidense con una carrera que abarcó cinco décadas.
Su padre, John Charles Smith, era hijo de inmigrantes metodistas ingleses y trabajaba en una variedad de trabajos extraños.
Para complacer a los familiares de su esposo, la madre de Pickford bautizó a sus hijos como metodistas, la religión de su padre.
Gladys, su madre y dos hermanos menores recorrieron los Estados Unidos por ferrocarril, actuando en compañías de tercera clase y obras de teatro.
Gladys finalmente consiguió un papel secundario en una obra de Broadway de 1907, The Warrens of Virginia.
Después de completar la carrera de Broadway y recorrer la obra, sin embargo, Pickford estaba de nuevo sin trabajo.
Rápidamente comprendió que la actuación cinematográfica era más simple que la actuación escénica estilizada del día.
Como Pickford dijo de su éxito en Biograph: Interpreté a matorraleras y secretarias y mujeres de todas las nacionalidades ... Decidí que si podía entrar en tantas fotos como fuera posible, me daría a conocer, y habría una demanda para mi trabajo.
En enero de 1910, Pickford viajó con un equipo de Biograph a Los Ángeles.
Los actores no figuraban en los créditos de la compañía de Griffith.
Pickford dejó Biograph en diciembre de 1910.
Regresó a Broadway en la producción de David Belasco de A Good Little Devil (1912).
En 1913, decidió trabajar exclusivamente en cine.
Pickford dejó el escenario para unirse a la lista de estrellas de Zukor.
Dramas cómicos, como En el carro del obispo (1913), Caprice (1913), y especialmente Hearts Adrift (1914), la hicieron irresistible para los espectadores.
Tess of the Storm Country fue liberado cinco semanas después.
Solo Charlie Chaplin, que superó ligeramente la popularidad de Pickford en 1916, tuvo un atractivo igualmente fascinante con los críticos y el público.
También se convirtió en vicepresidenta de Pickford Film Corporation.
Debido a su falta de una infancia normal, disfrutó haciendo estas imágenes.
En el agosto de 1918, el contrato de Pickford expiró y, rechazando los términos de Zukor para una renovación, le ofrecieron $250,000 dejar el negocio de la película.
A través de United Artists, Pickford continuó produciendo y actuando en sus propias películas; también podía distribuirlas como quisiera.
Durante este período, también hizo la Pequeña Annie Rooney (1925), otra película en la cual Pickford jugó a un niño, Gorriones (1926), que mezcló Dickensian con el estilo expresionista alemán recién acuñado, y Mi Mejor Muchacha (1927), una comedia romántica que presenta a su futuro marido Charles "Buddy" Rogers.
Interpretó a una socialité imprudente en Coquette (1929), su primera película hablada, un papel por el cual sus famosos rizos fueron cortados en un bob de 1920.
El público no pudo responder a ella en los roles más sofisticados.
Los actores establecidos de Hollywood entraron en pánico por la inminente llegada de los talkies.
Se retiró de la actuación cinematográfica en 1933 después de tres costosos fracasos con su última aparición en la película siendo Secrets.
Durante la Primera Guerra Mundial promovió la venta de Liberty Bonds, haciendo una serie intensiva de discursos de recaudación de fondos, comenzando en Washington, DC, donde vendió bonos junto a Charlie Chaplin, Douglas Fairbanks, Theda Bara y Marie Dressler.
En un solo discurso en Chicago, vendió un estimado de cinco millones de dólares en bonos.
Al final de la Primera Guerra Mundial, Pickford concibió el Motion Picture Relief Fund, una organización para ayudar a los actores financieramente necesitados.
Como resultado, en 1940, el Fondo pudo comprar tierras y construir el Motion Picture Country House and Hospital, en Woodland Hills, California.
Ella exigió (y recibió) estos poderes en 1916, cuando estaba bajo contrato con Zukor's Famous Players in Famous Plays (más tarde Paramount).
Mary Pickford Corporation era brevemente la compañía de producción de la película de Pickford.
Los distribuidores (también la parte de los estudios) arreglaron para producciones de la compañía para mostrarse en locales de la película de la compañía.
Era únicamente una compañía de distribución, que ofrecía a los productores de películas independientes acceso a sus propias pantallas, así como el alquiler de cines temporalmente no reservados propiedad de otras compañías.
Como cofundadora, así como productora y estrella de sus propias películas, Pickford se convirtió en la mujer más poderosa que haya trabajado en Hollywood.
Ella y Chaplin siguieron siendo socios de la compañía durante décadas.
Se rumorea que quedó embarazada de Moore a principios de la década de 1910 y tuvo un aborto espontáneo o un aborto.
La pareja vivió juntos de vez en cuando durante varios años.
Alrededor de este tiempo, Pickford también sufrió de la gripe durante la pandemia de la gripe de 1918.
Fueron a Europa para su luna de miel; los fanáticos en Londres y en París causaron disturbios tratando de llegar a la famosa pareja.
Pickford continuó personificando a la virtuosa pero ardiente chica de al lado.
Los jefes de estado extranjeros y los dignatarios que visitaron la Casa Blanca a menudo preguntaron si también podían visitar Pickfair, la mansión de la pareja en Beverly Hills.
Otros invitados fueron George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, HG Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noel Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder y Meher Baba, entre otros.
También estaban constantemente en exhibición como embajadores no oficiales de Estados Unidos en el mundo, liderando desfiles, cortando cintas y pronunciando discursos.
Se divorciaron el 10 de enero de 1936.
Criticó sus imperfecciones físicas, incluyendo la pequeña estatura de Ronnie y los dientes torcidos de Roxanne.
Sus hermanos, Lottie y Jack, murieron por causas relacionadas con el alcohol en 1936 y 1933, respectivamente.
Pickford se retiró y gradualmente se convirtió en un recluso, permaneciendo casi en su totalidad en Pickfair y permitiendo visitas solo de Lillian Gish, su hijastro Douglas Fairbanks Jr. y pocas otras personas.
Ella apareció en la corte en 1959, en un asunto relacionado con su copropiedad de la estación de televisión de Carolina del Norte WSJS-TV.
Charles "Buddy" Rogers a menudo daba visitas a los huéspedes de Pickfair, incluyendo vistas de un genuino bar occidental que Pickford había comprado para Douglas Fairbanks, y un retrato de Pickford en el salón.
También tenía una casa en Toronto, Ontario, Canadá.
Sus huellas de manos y pies se exhiben en el Teatro Chino de Grauman en Hollywood, California.
El Teatro Mary Pickford en el Edificio Conmemorativo James Madison de la Biblioteca del Congreso se nombra en su honor.
Una sala de cine de primera ejecución en Cathedral City, California se llama The Mary Pickford Theatre, que se estableció el 25 de mayo de 2001.
Entre ellos se encuentran un vestido de cuentas raro y espectacular que llevaba en la película Dorothy Vernon de Haddon Hall (1924) diseñado por Mitchell Leisen, su Oscar especial y una caja de joyas.
La casa de la familia había sido demolida en 1943, y muchos de los ladrillos entregados a Pickford en California.
En 1993, una Estrella de Palma de Oro en el Paseo de las Estrellas de Palm Springs fue dedicada a ella.
Desde enero de 2011 hasta julio de 2011, el Festival Internacional de Cine de Toronto exhibió una colección de recuerdos de Mary Pickford en la Galería de Cine Canadiense del edificio TIFF Bell LightBox.
Fue donado a Keene State College y actualmente está siendo restaurado por la Biblioteca del Congreso para su exposición.
El Google Doodle del 8 de abril de 2017 conmemoró el 125 cumpleaños de Mary Pickford.
Gloria Josephine May Swanson (27 de marzo de 1899 – 4 de abril de 1983) fue una actriz, productora y empresaria estadounidense.
Su amor de la colegiala en el actor de Estudios Essanay Francis X. Bushman llevó a su tía la toma para recorrer el estudio de Chicago del actor.
Su debut en el cine sonoro en 1929, The Trespasser, le valió una segunda nominación al Premio de la Academia.
Su padre era un sueco-americano y su madre era de ascendencia alemana, francesa y polaca.
En cualquiera de las dos versiones, pronto fue contratada como extra.
Su primer papel fue un breve paseo con la actriz Gerda Holmes, que pagó un enorme (en esos días) $ 3.25.
En 1915, coprotagonizó Sweedie Goes to College con su futuro primer marido, Wallace Beery.
Vernon y Swanson proyectaron una gran química de pantalla que resultó popular entre el público.
El tejón fue bastante impresionado por Swanson para recomendarla al director Jack Conway para Su Decisión y Usted No Puede Creer Todo en 1918.
(1920), Algo en que pensar (1920), y Los asuntos de Anatol (1921) pronto siguieron.
Se había convertido en una estrella en 1921 por su aparición en The Four Horsemen of the Apocalypse, pero Swanson lo había conocido desde sus días como aspirante a actor, sin aparente esperanza para su futuro profesional.
El rodaje se permitió por primera vez en muchos de los sitios históricos relacionados con Napoleón.
En ese momento, Swanson era considerada la estrella más rentable de su época.
La producción fue un desastre, con Parker siendo indecisa y los actores no tenían la experiencia suficiente para ofrecer las actuaciones que ella quería.
Los miembros dieron más pasos al registrar su descontento con Will H. Hays, Presidente de los Productores y Distribuidores de Películas de América.
Hays estaba entusiasmado con la historia básica, pero tenía problemas específicos que se trataron antes del lanzamiento de la película.
Propuso financiar personalmente su siguiente foto y llevó a cabo un examen exhaustivo de sus registros financieros.
Kennedy, sin embargo, le aconsejó que contratara a Erich von Stroheim para dirigir otra película muda, The Swamp, posteriormente retitulada Queen Kelly.
Stroheim trabajó durante varios meses en la escritura del guión básico.
El rodaje se cerró en enero, y Stroheim disparó, después de las quejas de Swanson sobre él y sobre la dirección general que estaba tomando la película.
El Trespasser en 1929 fue una producción de sonido, y obtuvo Swanson su segunda nominación al Oscar.
El estreno mundial se llevó a cabo en Londres, la primera producción de sonido estadounidense en hacerlo.
Perfect Understanding, una comedia de producción sonora de 1933, fue la única película producida por esta compañía.
Comenzó a aparecer en producciones teatrales y protagonizó The Gloria Swanson Hour en WPIX-TV en 1948.
La historia de la película sigue a una descolorida actriz de cine mudo Norma Desmond (Swanson), enamorada de un guionista fracasado Joe Gillis (William Holden).
Norma juega un juego de cartas de bridge con un grupo de actores también conocidos como "los Ceraworks".
Los sueños de Norma de un regreso son subvertidos, y cuando Gillis intenta romper con ella, amenaza con suicidarse, pero en su lugar lo mata.
Aunque Swanson se había opuesto a soportar una prueba de pantalla para la película, se había alegrado de ganar mucho más dinero de lo que había estado en la televisión y en el escenario.
Swanson más tarde recibió el Teatro de la Corona con Gloria Swanson, una serie de la antología de televisión en la cual de vez en cuando actuaba.
Ella era la "invitada misteriosa" en What's My Line.
Hizo una aparición notable en un episodio de 1966 de The Beverly Hillbillies, en el que se interpreta a sí misma.
El actor y dramaturgo Harold J. Kennedy, que había aprendido las cuerdas en Yale y con el Teatro Mercury de Orson Wells, sugirió que Swanson hiciera un recorrido por carretera de "Reflected Glory", una comedia que había corrido en el escenario de Broadway con Tallulah Bankhead como su estrella.
Después de su éxito con Sunset Boulevard, protagonizó en Broadway un renacimiento de Twentieth Century con José Ferrer, y en Nina con David Niven.
Como republicana apoyó las campañas de 1940 y 1944 para presidente de Wendell Willkie y la campaña presidencial de 1964 de Barry Goldwater.
Tomando medicamentos que le dio Beery, supuestamente para las náuseas matutinas, abortó el feto y fue llevada inconsciente al hospital.
En 1923, adoptó a Sonny Smith de 1 año, a quien renombró Joseph Patrick Swanson después de su padre.
Ella había concebido un hijo con él antes de que su divorcio de Somborn fuera definitivo, una situación que habría llevado a un escándalo público y posible final de su carrera cinematográfica.
Después de una recuperación de cuatro meses de su aborto, regresaron a los Estados Unidos como nobleza europea.
Se convirtió en un ejecutivo de cine representando a Pathé (EE.UU.) en Francia.
Swanson se describió a sí misma como una "vampiro mental", alguien con una curiosidad de búsqueda sobre cómo funcionaban las cosas, y que persiguió las posibilidades de convertir esas ideas en realidad.
Se conocieron por casualidad en París cuando Swanson estaba siendo equipado por Coco Chanel para su película de 1931 Tonight or Never.
Sus amigos, algunos de los cuales abiertamente no le gustaban, pensaban que estaba cometiendo un error.
Swanson había pensado inicialmente que iba a poder retirarse de la actuación, pero el matrimonio se vio afectado por el alcoholismo de Davey desde el principio.
Fue coautor (escritor fantasma) de la autobiografía de Billie Holiday, Lady Sings the Blues, el autor de Sugar Blues, un libro de salud de 1975 que aún se imprime, y el autor de la versión en inglés de Georges Ohsawa de You Are All Sanpaku.
Swanson y su esposo conocieron a John Lennon y Yoko Ono porque eran fanáticos del trabajo de Dufty.
Fue incinerada y sus cenizas enterradas en la Iglesia Episcopal del Descanso Celestial en la Quinta Avenida en la ciudad de Nueva York, a la que asistió solo un pequeño círculo familiar.
En 1974, Swanson fue uno de los galardonados con el primer Festival de Cine de Telluride.
Debido a la naturaleza erótica de sus actuaciones, las películas de Nielsen fueron censuradas en los Estados Unidos, y su trabajo permaneció relativamente oscuro para el público estadounidense.
La familia de Nielsen se mudó varias veces durante su infancia mientras su padre buscaba empleo.
El padre de Nielsen murió cuando ella tenía catorce años.
En 1901, Nielsen, de 21 años, quedó embarazada y dio a luz a una hija, Jesta.
Nielsen se graduó de la escuela de teatro en 1902.
El estilo de actuación minimalista de Nielsen se evidenció en su exitosa interpretación de una joven ingenua atraída a una vida trágica.
Nielsen y Gad se casaron, y luego hicieron cuatro películas más juntos.
Me di cuenta de que la era del cortometraje había pasado.
Fueron las ventas internacionales de películas las que le proporcionaron a Union ocho películas de Nielsen por año.
Utilicé todos los medios disponibles, e ideé muchos nuevos, para llevar las películas de Asta Nielsen al mundo”.
En una encuesta de popularidad rusa de 1911, Nielsen fue votada como la estrella de cine femenina más importante del mundo, detrás de Linder y por delante de su compatriota danés Valdemar Psilander.
En 1921, Nielsen, a través de su propia compañía de distribución de películas de Asta Films, apareció en el Svend Gade y Heinz Schall dirigió Hamlet.
Sin embargo, trabajos académicos como la filmografía autorizada publicada por Filmarchiv Austria en 2010 no hacen mención de tal película.
Trabajó en películas alemanas hasta el inicio de las películas sonoras.
A partir de entonces, Nielsen actuó solo en el escenario.
Entendiendo las implicaciones, Nielsen declinó y abandonó Alemania en 1936.
Se divorciaron en 1919 cuando Nielsen se casó con el constructor naval sueco Freddy Windgardh.
Comenzaron un matrimonio del derecho consuetudinario a largo plazo que duró a partir de 1923 hasta finales de los años 1930.
Fred Astaire (nacido Frederick Austerlitz; 10 de mayo de 1899 - 22 de junio de 1987) fue un bailarín, actor, cantante, coreógrafo y presentador de televisión estadounidense.
Protagonizó más de 10 musicales de Broadway y West End, hizo 31 películas musicales, cuatro especiales de televisión y numerosas grabaciones.
La madre de Astaire nació en los Estados Unidos de inmigrantes alemanes luteranos de Prusia Oriental y Alsacia.
Fritz estaba buscando trabajo en el comercio de la elaboración de cerveza y se mudó a Omaha, Nebraska, donde fue empleado por la Storz Brewing Company.
Johanna planeó un "acto del hermano y la hermana", común en el vodevil entonces, para sus dos hijos.
Comenzaron a formarse en la Escuela de Maestros Alviene del Teatro y la Academia de Artes Culturales.
Se les enseñó a bailar, hablar y cantar en preparación para el desarrollo de un acto.
En una entrevista, la hija de Astaire, Ava Astaire McKenzie, observó que a menudo ponen a Fred en un sombrero de copa para que se vea más alto.
Como resultado de la venta de su padre, Fred y Adele consiguieron un contrato importante y jugaron en el Circuito de Orpheum en el Medio Oeste, el Oeste y algunas ciudades del Sur en los Estados Unidos.
En 1912, Fred se convirtió en episcopal.
De la bailarina de vodevil Aurelio Coccia, aprendieron el tango, el vals y otros bailes de salón popularizados por Vernon e Irene Castle.
Conoció por primera vez a George Gershwin, que estaba trabajando como taponador de canciones para la compañía editorial de música de Jerome H. Remick, en 1916.
De su trabajo en The Passing Show de 1918, Heywood Broun escribió: "En una noche en la que había una gran cantidad de buenos bailes, Fred Astaire se destacó ... Él y su compañera, Adele Astaire, hicieron que el espectáculo se detuviera temprano en la noche con un hermoso baile de extremidades sueltas".
Pero en ese momento, la habilidad de baile de Astaire estaba empezando a eclipsar a la de su hermana.
El claqué de Astaire fue reconocido por entonces como uno de los mejores.
Después del cierre de Funny Face, los Astaire fueron a Hollywood para una prueba de pantalla (ahora perdida) en Paramount Pictures, pero Paramount los consideró inadecuados para las películas.
El final de la asociación fue traumático para Astaire, pero lo estimuló a expandir su alcance.
Le prestaron durante unos días a MGM en 1933 para su debut de Hollywood significativo en la Dama de Baile de la película musical exitosa.
Escribió a su agente: "No me importa hacer otra película con ella, pero en cuanto a esta idea de 'equipo', ¡está 'fuera'!
La asociación, y la coreografía de Astaire y Hermes Pan, ayudaron a hacer del baile un elemento importante del musical cinematográfico de Hollywood.
Seis de los nueve musicales de Astaire-Rogers se convirtieron en los mayores generadores de dinero para RKO; todas las películas trajeron un cierto prestigio y arte que todos los estudios codiciaban en ese momento.
Esto dio la ilusión de una cámara casi estacionaria filmando un baile entero en una sola toma.
El estilo de Astaire de secuencias de baile permitió al espectador seguir a los bailarines y la coreografía en su totalidad.
La segunda innovación de Astaire implicó el contexto de la danza; era inflexible que todas las canciones y rutinas de baile sean integrales a las tramas de la película.
Una sería una actuación en solitario de Astaire, a la que denominó su "solista de calcetines".
Creo que Ginger Rogers lo fue.
Ella fingió mucho.
En 1976, el presentador de programas de entrevistas británico Sir Michael Parkinson le preguntó a Astaire quién era su compañero de baile favorito.
A pesar de su éxito, Astaire no estaba dispuesto a tener su carrera ligada exclusivamente a cualquier asociación.
A lo largo de este período, Astaire continuó valorando la contribución de los colaboradores coreográficos.
Protagonizaron Broadway Melody de 1940, en la que realizaron una célebre rutina de baile extendido de Cole Porter "Comienza la Beguine".
Jugó junto a Bing Crosby en Holiday Inn (1942) y más tarde Blue Skies (1946).
La última película presentó "Puttin' On the Ritz", una innovadora rutina de canto y baile asociada indeleblemente con él.
La primera película, Nunca te harás rico (1941), catapultó a Hayworth al estrellato.
Presentó un dúo con "I'm Old Fashioned" de Kern, que se convirtió en la pieza central del tributo de 1983 del Ballet de la Ciudad de Nueva York de Jerome Robbins a Astaire.
Astaire coreografió esta película sola y logró un modesto éxito de taquilla.
La fantasía Yolanda y el ladrón (1945) contó con un ballet surrealista de vanguardia.
Siempre inseguro y creyendo que su carrera comenzaba a tambalearse, Astaire sorprendió a su público al anunciar su retiro durante la producción de su próxima película Blue Skies (1946).
Ambas películas revivieron la popularidad de Astaire y en 1950 protagonizó dos musicales.
Mientras que Three Little Words lo hizo bastante bien en la taquilla, Let's Dance fue una decepción financiera.
Pero debido a su alto costo, no pudo obtener ganancias en su primer lanzamiento.
Entonces, su esposa Phyllis se enfermó y de repente murió de cáncer de pulmón.
Papá piernas largas sólo lo hizo moderadamente bien en la taquilla.
Del mismo modo, el siguiente proyecto de Astaire - su último musical en MGM, Silk Stockings (1957), en el que co-protagonizó con Cyd Charisse, también perdió dinero en la taquilla.
El primero de estos programas, An Evening with Fred Astaire (1958), ganó nueve premios Emmy, incluyendo "Mejor actuación individual de un actor" y "Programa individual más destacado del año".
La elección tuvo una reacción controvertida porque muchos creían que su baile en el especial no era el tipo de "actuación" para el que se diseñó el premio.
Restauraron la cinta de vídeo original, transfiriendo su contenido a un formato moderno y llenando huecos donde la cinta se había deteriorado con imágenes de cinescopio.
Astaire apareció en papeles no bailables en otras tres películas y varias series de televisión desde 1957 hasta 1969.
La compañera de baile de Astaire fue Petula Clark, quien interpretó a la hija escéptica de su personaje.
Astaire continuó actuando en la década de 1970.
En la segunda compilación, a los setenta y seis años, realizó breves secuencias de enlace de baile con Kelly, sus últimas actuaciones de baile en una película musical.
En 1978, co-protagonizó con Helen Hayes en una bien recibida película de televisión A Family Upside Down en la que interpretaban a una pareja de ancianos que lidiaban con problemas de salud.
Astaire le pidió a su agente que le obtuviera un papel en Galactica debido al interés de sus nietos en la serie y los productores estaban encantados con la oportunidad de crear un episodio completo para presentarlo.
Mucho después de que se completara la fotografía para el número de baile en solitario "I Want to Be a Dancin' Man" para la película de 1952 The Belle of New York, se decidió que el humilde traje de Astaire y el escenografía raída eran inadecuados y toda la secuencia fue refilmada.
Marco por marco, las dos actuaciones son idénticas, hasta el gesto más sutil.
El suyo era un estilo de baile único y reconocible que influyó en gran medida en el estilo American Smooth de baile de salón y estableció estándares contra los cuales se juzgarían los musicales de baile de cine posteriores.
Señala que el estilo de baile de Astaire fue consistente en películas posteriores hechas con o sin la ayuda de Pan.
Sin embargo, esto casi siempre se limitaba al área de secuencias de fantasía extendidas, o "balletes de ensueño".
Más tarde en la vida, admitió: "Yo mismo tuve que hacer la mayor parte de eso".
Muchas rutinas de baile se construyeron alrededor de un "truco", como bailar en las paredes en Royal Wedding o bailar con sus sombras en Swing Time.
Trabajarían con un pianista de ensayo (a menudo el compositor Hal Borne) que a su vez comunicaría modificaciones a los orquestadores musicales.
Con toda la preparación completada, el tiroteo real iría rápidamente, ahorrando costos.
Ni siquiera irá a ver sus acometidas... Siempre piensa que no es bueno”.
Michael Kidd, co-coreógrafo de Astaire en la película de 1953 The Band Wagon, descubrió que su propia preocupación por la motivación emocional detrás del baile no era compartida por Astaire.
Vamos a añadir las miradas más tarde.'"
Irving Berlin consideraba a Astaire el equivalente a cualquier intérprete masculino de sus canciones: «tan bueno como Jolson, Crosby o Sinatra, no necesariamente por su voz, sino por su concepción de proyectar una canción».
En su apogeo, Astaire fue referenciado en las letras de los compositores Cole Porter, Lorenz Hart y Eric Maschwitz y continúa inspirando a los compositores modernos.
Durante 1952, Astaire grabó The Astaire Story, un álbum de cuatro volúmenes con un quinteto dirigido por Oscar Peterson.
Bogart comenzó a actuar en espectáculos de Broadway, comenzando su carrera en películas con Up the River (1930) para Fox y apareció en papeles secundarios para la próxima década, a veces retratando gángsteres.
Los detectives privados de Bogart, Sam Spade (en El Halcón Maltés) y Phillip Marlowe (en 1946 El Sueño Grande), se hicieron los modelos para detectives en otras películas negras.
Poco después de la fotografía principal de The Big Sleep (1946, su segunda película juntos), solicitó el divorcio de su tercera esposa y se casó con Bacall.
Volvió a interpretar a esos personajes inestables e inestables como comandante de una nave naval de la Segunda Guerra Mundial en The Caine Mutiny (1954), que fue un éxito crítico y comercial y le valió otra nominación al Mejor Actor.
El nombre "Bogart" deriva del apellido holandés, "Bogaert".
Maud era un episcopal de herencia inglesa y descendiente del pasajero del Mayflower John Howland.
Clifford McCarty escribió que el departamento de publicidad de Warner Bros. lo había alterado al 23 de enero de 1900 "para fomentar la opinión de que un hombre nacido el día de Navidad no podía ser tan villano como parecía estar en la pantalla".
Lauren Bacall escribió en su autobiografía que el cumpleaños de Bogart siempre se celebraba el día de Navidad, diciendo que bromeaba sobre ser engañado de un regalo cada año.
Maud fue una ilustradora comercial que recibió su formación artística en Nueva York y Francia, incluido el estudio con James Abbott McNeill Whistler.
Ganó más de $ 50,000 al año en el apogeo de su carrera, una suma muy grande de dinero en ese momento, y considerablemente más que los $ 20,000 de su esposo.
Tuvo dos hermanas menores: Frances ("Pat") y Catherine Elizabeth ("Kay").
Un beso, en nuestra familia, fue un acontecimiento.
Heredó una tendencia a la aguja, una afición por la pesca, un amor de toda la vida por la navegación y una atracción por las mujeres de voluntad fuerte de su padre.
Bogart más tarde asistió a la Academia de Phillips, un internado al cual se admitió basado en conexiones familiares.
Se han dado varias razones; según uno, fue expulsado por arrojar al director (o a un jardinero) a Rabbit Pond en el campus.
Luego se ofreció como voluntario para la Reserva Temporal de la Guardia Costera en 1944, patrullando la costa de California en su yate, el Santana.
En uno, su labio fue cortado por la metralla cuando su barco (el) fue bombardeado.
Mientras cambiaba de tren en Boston, el prisionero esposado supuestamente le pidió a Bogart un cigarrillo.
Cuando Bogart fue tratado por un médico, se había formado una cicatriz.
En lugar de coserlo, lo arruinó”.
Su carácter y valores se desarrollaron por separado de su familia durante sus días de marina, y comenzó a rebelarse.
Bogart reanudó su amistad con Bill Brady Jr. (cuyo padre tenía conexiones con el mundo del espectáculo), y obtuvo un trabajo de oficina con la nueva compañía World Films de William A. Brady.
Hizo su debut en el escenario unos meses más tarde como mayordomo japonés en la obra de 1921 de Alice Drifting (nerviosamente entregando una línea de diálogo), y apareció en varias de sus obras posteriores.
Una pelea en el bar en este momento también fue una supuesta causa del daño labial de Bogart, coincidiendo con el relato de Louise Brooks.
A Bogart le disgustaron sus partes triviales, afeminadas de la carrera temprana, llamándolos "pantalones blancos Willie" papeles.
Menken dijo en su solicitud de divorcio que Bogart valoró su carrera más que el matrimonio, citando negligencia y abuso.
Allí conoció a Spencer Tracy, un actor de Broadway a quien Bogart amaba y admiraba, y se convirtieron en amigos cercanos y compañeros de bebida.
Tracy recibió la facturación superior, pero Bogart apareció en los carteles de la película.
Un cuarto de siglo después, los dos hombres planearon hacer Las Horas Desesperadas juntos.
Bogart viajó de ida y vuelta entre Hollywood y el escenario de Nueva York de 1930 a 1935, sin trabajo durante largos períodos.
Aunque Leslie Howard fuera la estrella, el crítico de New York Times Brooks Atkinson dijo que el juego era "un durazno... un melodrama Occidental rugiente... Humphrey Bogart hace el mejor trabajo de su carrera como un actor".
Warner Bros. compró los derechos de la pantalla de El bosque petrificado en 1935.
Howard, que tenía los derechos de producción, dejó en claro que quería que Bogart protagonizara con él.
Cuando Warner Bros. vio que Howard no se movería, cedieron y contrataron a Bogart.
Según Variety, "la amenaza de Bogart no deja nada que desear".
Debe haber algo en mi tono de voz, o esta cara arrogante, algo que antagoniza a todos.
A pesar de su éxito, Warner Bros. no tenía interés en elevar el perfil de Bogart.
Bogart utilizó estos años para comenzar a desarrollar su personaje cinematográfico: un herido, estoico, cínico, encantador, vulnerable, solitario y burlón con un código de honor.
Sus disputas con Warner Bros. sobre papeles y dinero fueron similares a las libradas por el estudio con estrellas más establecidas y menos maleables como Bette Davis y James Cagney.
Su único papel principal durante este período fue en Dead End (1937, prestado a Samuel Goldwyn), como un gángster modelado después de Baby Face Nelson.
En Black Legion (1937), una película que Graham Greene describió como "inteligente y emocionante, si bien seria", interpretó a un buen hombre que fue atrapado (y destruido) por una organización racista.
El problema era que estaban bebiendo la mía y yo estaba haciendo esta película apestosa”.
El 21 de agosto de 1938, Bogart entró en un tercer matrimonio turbulento con la actriz Mayo Methot, una mujer animada, amistosa cuando sobrio pero paranoico y agresivo cuando bebido.
Ella prendió fuego a su casa, lo apuñaló con un cuchillo y le cortó las muñecas varias veces.
Según su amigo, Julius Epstein, "El matrimonio Bogart-Methot fue la secuela de la Guerra Civil".
Sin embargo, la influencia de Methot fue cada vez más destructiva, y Bogart también continuó bebiendo.
Cuando pensó que un actor, director o estudio había hecho algo de mala calidad, habló públicamente al respecto.
Paul Muni, George Raft, Cagney y Robinson rechazaron el papel principal, dando a Bogart la oportunidad de interpretar a un personaje con cierta profundidad.
Trabajó bien con Ida Lupino, provocando celos de Mayo Methot.
Podía citar a Platón, al Papa, a Ralph Waldo Emerson y más de mil líneas de Shakespeare, y se suscribió a la Harvard Law Review.
Basada en la novela de Dashiell Hammett, fue serializada por primera vez en la revista pulp Black Mask en 1929 y fue la base de dos versiones cinematográficas anteriores; la segunda fue Satan Met a Lady (1936), protagonizada por Bette Davis.
Huston luego aceptó ansiosamente a Bogart como su Sam Spade.
La película, dirigida por Michael Curtiz y producida por Hal Wallis, contó con Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre y Dooley Wilson.
Se dice que Bogart fue responsable de la noción de que Rick Blaine debería ser retratado como un jugador de ajedrez, una metáfora de las relaciones que mantuvo con amigos, enemigos y aliados.
Bogart se denominó por el Mejor Actor en un Papel Principal, pero perdió a Paul Lukas para su rendimiento en la Vigilancia en el Rin.
Bogart fue en United Service Organizations y giras de bonos de guerra con Methot en 1943 y 1944, haciendo arduos viajes a Italia y el norte de África (incluida Casablanca).
Cuando se conocieron, Bacall tenía 19 años y Bogart 44; la apodó "Bebé".
Nos divertiremos mucho juntos".
Por mí mismo y luego algunos, HarperCollins, Nueva York, 2005.
Se consideraba a sí mismo el protector y mentor de Bacall, y Bogart estaba usurpando ese papel.
Además, tiene un sentido del humor que contiene ese tono de desprecio”.
El diálogo, especialmente en las escenas añadidas proporcionadas por Hawks, estaba lleno de insinuaciones sexuales, y Bogart es convincente como detective privado Philip Marlowe.
El matrimonio fue feliz, con tensiones debido a sus diferencias.
Según el biógrafo de Bogart, Stefan Kanfer, era "una línea de producción de cine negro sin distinción particular".
Al carecer de un interés amoroso o un final feliz, se consideraba un proyecto arriesgado.
James Agee escribió: “Bogart hace un trabajo maravilloso con este personaje ... millas por delante del muy buen trabajo que ha hecho antes”.
Bogart apareció en sus películas finales para Warners, Relámpago de cadena (1950) y El Ejecutor (1951).
Santana también hizo dos películas sin él: Y el bebé hace tres (1949) y El secreto de la familia (1951).
Varios biógrafos de Bogart, y la actriz y escritora Louise Brooks, han sentido que este papel es el más cercano al verdadero Bogart.
Una parodia de El halcón maltés, Beat the Devil fue la última película de Bogart y John Huston.
El amor de Huston por la aventura, su profunda y duradera amistad (y éxito) con Bogart, y la oportunidad de trabajar con Hepburn convencieron al actor de abandonar Hollywood para un rodaje difícil en el Congo belga.
Bacall llegó por más de cuatro meses, dejando a su pequeño hijo en Los Ángeles.
Ella adornó mis ropa interior en el África más oscura”.
A Hepburn (un abstemio) le fue peor en las condiciones difíciles, perdiendo peso y en un momento se puso muy enfermo.
A pesar de la incomodidad de saltar del barco a pantanos, ríos y marismas, la reina africana aparentemente reavivó el temprano amor de Bogart por los barcos; cuando regresó a California, compró una clásica carrera de caoba Hacker-Craft que mantuvo hasta su muerte.
Cuando Bogart ganó, sin embargo, dijo: "Es un largo camino desde el Congo belga a la etapa de este teatro.
Al igual que en el tenis, necesitas un buen oponente o compañero para sacar lo mejor de ti.
Aunque conservó parte de su antigua amargura por tener que hacerlo, tuvo una fuerte actuación en el liderazgo; recibió su nominación final al Oscar y fue objeto de una historia de portada de la revista Time del 7 de junio de 1954.
Es el tipo de director con el que no me gusta trabajar... la película es una mierda.
A pesar de la acritud, la película tuvo éxito; según una revisión en The New York Times, Bogart era "increíblemente hábil... la habilidad con la cual este viejo actor acanalado por la roca mezcla las mordazas y tales duplicidades con una manera varonil de derretirse es una de las alegrías incalculables del espectáculo".
Estaba inquieto con Ava Gardner en la protagonista femenina; ella acababa de romper con su compañero de Rat Pack Frank Sinatra, y Bogart estaba molesto por su actuación inexperta.
Cuando Bacall los encontró juntos, extrajo una costosa juerga de compras de su esposo; los tres viajaron juntos después del tiroteo.
También apareció en The Jack Benny Show, donde un kinescope sobreviviente de la transmisión en vivo lo captura en su única actuación de comedia de sketch de televisión (25 de octubre de 1953).
Stephen se convirtió en autor y biógrafo y presentó un especial de televisión sobre su padre en Turner Classic Movies.
A raíz de Santana, Bogart había formado una nueva compañía y tenía planes para una película (Melville Goodwin, EE.UU.) en la que interpretaría a un general y Bacall un magnate de la prensa.
No habló de su salud y visitó a un doctor en el enero de 1956 después de la persuasión considerable de Bacall.
Se sometió a una cirugía adicional en noviembre de 1956, cuando el cáncer había hecho metástasis.
En él se inscribía: "Si quieres algo, simplemente silba".
Después de haber estudiado con Stella Adler en la década de 1940, se le atribuye ser uno de los primeros actores en llevar el sistema Stanislavski de actuación y método de actuación, derivado del sistema Stanislavski, a las audiencias principales.
Dirigió y protagonizó el culto occidental One-Eyed Jacks, un fracaso crítico y comercial, después de lo cual entregó una serie de notables fracasos de taquilla, comenzando con Mutiny on the Bounty (1962).
Rechazó el premio debido al supuesto maltrato y mala interpretación de los nativos americanos por parte de Hollywood.
Según el Libro Guinness de los Récords, Brando recibió un récord de $ 3.7 millones ($ millones en dólares ajustados a la inflación) y el 11.75% de las ganancias brutas por el trabajo de 13 días en Superman.
Su ascendencia era en su mayoría alemana, holandesa, inglesa e irlandesa.
Brando fue criado como científico cristiano.
Sin embargo, ella era una alcohólica y a menudo tenía que ser traída a casa desde bares en Chicago por su marido.
Brando albergaba mucha más enemistad por su padre, declarando: "Yo era su homónimo, pero nada de lo que hice lo complació ni siquiera le interesó.
Alrededor de 1930, los padres de Brando se mudaron a Evanston, Illinois, cuando el trabajo de su padre lo llevó a Chicago, pero se separaron en 1935 cuando Brando tenía 11 años.
Brando, cuyo apodo de la infancia era "Bud", era un imitador de su juventud.
En 2007 TCM biopic Brando: El Documental, el amigo de la infancia George Englund recuerda la interpretación más temprana de Brando como imitando las vacas y los caballos en la granja de la familia como una manera de distraer a su madre de la bebida.
La hermana de Brando, Frances, dejó la universidad en California para estudiar arte en Nueva York.
Brando sobresalió en el teatro y lo hizo bien en la escuela.
La facultad votó para expulsarlo, aunque fue apoyado por los estudiantes, que pensaban que la expulsión era demasiado dura.
En un documental de 1988, Marlon Brando: The Wild One, la hermana de Brando, Jocelyn recordó: "Estaba en una obra de teatro escolar y la disfrutó ... Así que decidió que iría a Nueva York y estudiaría actuación porque eso era lo único que había disfrutado.
Durante un tiempo vivió con Roy Somlyo, quien más tarde se convirtió en un productor de Broadway cuatro veces ganador del Emmy.
La notable perspicacia y sentido del realismo de Brando fueron evidentes desde el principio.
Según Dustin Hoffman en su Masterclass en línea, Brando a menudo hablaba con camarógrafos y otros actores sobre su fin de semana, incluso después de que el director llamara a la acción.
Su comportamiento lo hizo expulsar del elenco de la producción de la New School en Sayville, pero poco después fue descubierto en una obra de teatro producida localmente allí.
Cornell también lo eligió como el Mensajero en su producción de Antígona de Jean Anouilh ese mismo año.
Bankhead había rechazado el papel de Blanche Dubois en Un tranvía llamado deseo, que Williams había escrito para ella, para recorrer la obra para la temporada 1946-47.
Wilson era en gran parte tolerante con el comportamiento de Brando, pero alcanzó su límite cuando Brando murmuró durante un ensayo general poco antes de la apertura del 28 de noviembre de 1946.
Fue maravilloso", recordó un miembro del elenco. "
Sin embargo, los críticos no fueron tan amables.
Recibió mejores críticas en las siguientes paradas de la gira, pero lo que sus colegas recordaron fueron solo indicaciones ocasionales del talento que demostraría más tarde.
Brando mostró su apatía por la producción al demostrar algunos modales impactantes en el escenario.
Después de varias semanas en el camino, llegaron a Boston, momento en el cual Bankhead estaba listo para despedirlo.
Pierpont escribe que John Garfield fue la primera opción para el papel, pero "hizo demandas imposibles".
Humaniza el carácter de Stanley en el sentido de que se convierte en la brutalidad y la insensibilidad de la juventud en lugar de un anciano vicioso ... Un nuevo valor surgió de la lectura de Brando que fue, con mucho, la mejor lectura que he escuchado ".
Él dijo: "El telón subió y en el escenario está ese hijo de puta del gimnasio, y él me está interpretando".
El primer papel de la pantalla de Brando era un veterano parapléjico amargo en Los Hombres (1950).
Por propia cuenta de Brando, puede haber sido debido a esta película que su estado de borrador fue cambiado de 4-F a 1-A. Había tenido una cirugía en su rodilla trick, y ya no era lo suficientemente debilitante físicamente como para incurrir en la exclusión del borrador.
Casualmente, el psiquiatra conocía a un médico amigo de Brando.
El papel es considerado como uno de los más grandes de Brando.
La película fue dirigida por Elia Kazan y coprotagonizada por Anthony Quinn.
Durante nuestras escenas juntos, percibí una amargura hacia mí, y si sugería un trago después del trabajo, él me rechazaba o bien era hosco y decía poco.
Después de lograr el efecto deseado, Kazan nunca le dijo a Quinn que lo había engañado.
Gielgud quedó tan impresionado que le ofreció a Brando una temporada completa en el Hammersmith Theatre, una oferta que rechazó.
Era como una puerta de horno que se abría: el calor salía de la pantalla.
Según todos los informes, Brando estaba molesto por la decisión de su mentor, pero trabajó con él de nuevo en On The Waterfront.
Los importadores de Triumph eran ambivalentes en la exposición, ya que el tema era bandas de motociclistas ruidosas que se apoderaban de una pequeña ciudad.
Cuando inicialmente se le ofreció el papel, Brando, todavía picado por el testimonio de Kazan a HUAC, se mostró reacio y la parte de Terry Malloy casi fue a Frank Sinatra.
Brando ganó el Oscar por su papel como el estibador irlandés-estadounidense Terry Malloy en On the Waterfront.
En su crítica del 29 de julio de 1954, el crítico de The New York Times A. H. Weiler elogió la película, calificándola de "un uso inusualmente poderoso, emocionante e imaginativo de la pantalla por parte de profesionales talentosos".
Interpretó a Napoleón en la película Désirée de 1954.
Brando fue especialmente despectivo con el director Henry Koster.
Las relaciones entre Brando y el coprotagonista Frank Sinatra también fueron frías, con Stefan Kanfer observando: "Los dos hombres eran opuestos diametrales: Marlon requería múltiples tomas; Frank detestaba repetirse a sí mismo".
Frank Sinatra llamó a Brando "el actor más sobrevalorado del mundo", y se refirió a él como "mumbles".
Pauline Kael no estaba particularmente impresionada por la película, pero señaló que "Marlon Brando se moría de hambre para interpretar al intérprete de duende Sakini, y parece que está disfrutando del truco, hablando con un acento loco, sonriendo infantilmente, inclinándose hacia adelante y haciendo movimientos complicados con sus piernas.
Newsweek encontró la película un "cuento aburrido de la reunión de los dos", pero fue sin embargo un éxito de taquilla.
La película ganó cuatro premios de la Academia.
Según todos los relatos, Brando estaba devastado por su muerte, y el biógrafo Peter Manso le dijo a A&E Biography: "Ella fue la que pudo darle la aprobación como nadie más pudo y, después de que su madre murió, parece que Marlon deja de preocuparse".
Los Leones Jóvenes también presentan la única aparición de Brando en una película con el amigo y el rival Montgomery Clift (aunque no compartieran ningunas escenas juntos).
Brando interpreta al personaje principal Rio, y Karl Malden interpreta a su compañero "Papá" Longworth.
La inexperiencia de Brando como editor también retrasó la postproducción y Paramount finalmente tomó el control de la película.
Para entonces, estaba aburrido con todo el proyecto y me alejé de él”.
Según los informes, la repulsión de Brando con la industria cinematográfica se extendió al set de su próxima película, el remake de Metro-Goldwyn-Mayer de Mutiny on the Bounty, que se filmó en Tahití.
El director del motín, Lewis Milestone, afirmó que los ejecutivos "merecen lo que obtienen cuando le dan a un actor de jamón, un niño petulante, un control completo sobre una imagen costosa".
The Ugly American (1963) fue la primera de estas películas.
Todas las otras películas de Brando Universal durante este período, incluyendo Bedtime Story (1964), The Appaloosa (1966), A Countess from Hong Kong (1967) y The Night of the Following Day (1969), también fueron fracasos críticos y comerciales.
Brando también había aparecido en el thriller de espías Morituri en 1965; eso, también, no logró atraer a una audiencia.
Candy fue especialmente espantosa para muchos; una película de farsa sexual de 1968 dirigida por Christian Marquand y basada en la novela de 1958 de Terry Southern, la película satiriza historias pornográficas a través de las aventuras de su heroína ingenua, Candy, interpretada por Ewa Aulin.
En la edición de marzo de 1966 de The Atlantic, Pauline Kael escribió que en sus días rebeldes, Brando "era antisocial porque sabía que la sociedad era basura; era un héroe para la juventud porque era lo suficientemente fuerte como para no tomar la basura", pero ahora Brando y otros como él se habían convertido en "bufones, descaradamente, burlándose patéticamente de su reputación pública".
Fui muy convincente en mi postura de indiferencia, pero era muy sensible y dolió mucho”.
La película en general recibió críticas mixtas.
Brando dedicó un capítulo completo a la película en sus memorias, afirmando que el director, Gillo Pontecorvo, era el mejor director con el que había trabajado junto a Kazan y Bernardo Bertolucci.
En 1971, Michael Winner lo dirigió en la película de terror británica The Nightcomers con Stephanie Beacham, Thora Hird, Harry Andrews y Anna Palk.
Fue mejor que Brando en los Premios del Círculo de Críticos de Cine de Nueva York de 1972.
Brando también tuvo a One-Eyed Jacks trabajando en su contra, una producción problemática que perdió dinero para Paramount cuando se lanzó en 1961.
Coppola convenció a Brando para una prueba de "maquillaje" grabada en video, en la que Brando hizo su propio maquillaje (usó bolas de algodón para simular las mejillas hinchadas del personaje).
Brando tenía dudas, declarando en su autobiografía, "Nunca había jugado a un italiano antes, y no pensé que podría hacerlo con éxito".
Brando fue firmado por una tarifa baja de $ 50,000, pero en su contrato, se le dio un porcentaje de lo bruto en una escala móvil: 1% de lo bruto por cada $ 10 millones sobre un umbral de $ 10 millones, hasta 5% si la imagen superaba los $ 60 millones.
En una entrevista de 1994 que se puede encontrar en el sitio web de la Academia de Logro, Coppola insistió: "El Padrino era una película muy poco apreciada cuando la estábamos haciendo.
No les gustaba la forma en que lo estaba filmando.
En una entrevista televisiva de 2010 con Larry King, Al Pacino también habló sobre cómo el apoyo de Brando lo ayudó a mantener el papel de Michael Corleone en la película, a pesar del hecho de que Coppola quería despedirlo.
Rompió el hielo brindando al grupo con una copa de vino.
Caan agrega: "El primer día que conocimos a Brando, todos estaban asombrados".
Además, debido a que tenía tanto poder y autoridad incuestionable, pensé que sería un contraste interesante interpretarlo como un hombre gentil, a diferencia de Al Capone, que golpeó a la gente con bates de béisbol.
Realmente no había principio.
Él boicoteó la ceremonia de premiación, en lugar de enviar activista por los derechos de los indígenas americanos Sacheen Littlefeather, que apareció en traje completo Apache, para indicar las razones de Brando, que se basaron en su objeción a la representación de los indígenas americanos por Hollywood y la televisión.
Como con películas anteriores, Brando rechazó memorizar sus líneas para muchas escenas; en cambio, escribió sus líneas en tarjetas de la señal y los fijó alrededor del juego para la referencia fácil, dejando Bertolucci con el problema de guardarlos del marco de cuadro.
Su acuerdo de participación le valió 3 millones de dólares.
Pauline Kael, en la reseña de The New Yorker, escribió: "El avance de la película finalmente ha llegado.
En 1973, Brando fue devastado por la muerte de su mejor amigo de la infancia Wally Cox.
Ausente durante la primera hora de la película, Clayton entra a caballo, colgando boca abajo, encamisado en piel blanca, al estilo Littlefeather.
Penn, que creía en dejar que los actores hicieran lo suyo, consintió a Marlon todo el tiempo.
En 1978, Brando narró la versión en inglés de Raoni, un documental franco-belga dirigido por Jean-Pierre Dutilleux y Luiz Carlos Saldanha que se centró en la vida de Raoni Metuktire y los problemas que rodean la supervivencia de las tribus indígenas del centro norte de Brasil.
En 1979, hizo una rara aparición en televisión en la miniserie Roots: The Next Generations, interpretando a George Lincoln Rockwell; ganó un Premio Primetime Emmy al Mejor Actor de Reparto en una Miniserie o Película por su actuación.
A Brando le pagaron un millón de dólares a la semana por tres semanas de trabajo.
En el documental, Coppola habla de lo asombrado que estaba cuando un Brando con sobrepeso apareció en sus escenas y, sintiéndose desesperado, decidió retratar a Kurtz, quien parece demacrado en la historia original, como un hombre que había complacido todos los aspectos de sí mismo.
Sin embargo, regresó en 1989 en A Dry White Season, basada en la novela antiapartheid de André Brink de 1979.
Brando recibió elogios por su actuación, ganando una nominación al Premio de la Academia al Mejor Actor de Reparto y ganando el Premio al Mejor Actor en el Festival de Cine de Tokio.
Variety también elogió la actuación de Brando como Sabatini y señaló: "La sublime actuación cómica de Marlon Brando eleva a The Freshman de la comedia screwball a un nicho peculiar en la historia del cine".
El guionista de The Island of Dr. Moreau, Ron Hutchinson, diría más tarde en sus memorias, Aferrarse al iceberg: escribir para vivir en el escenario y en Hollywood (2017), que Brando saboteó la producción de la película al pelearse y negarse a cooperar con sus colegas y el equipo de filmación.
Este fue su último papel y su único papel como personaje femenino.
El hijo del actor, Miko, fue guardaespaldas y asistente de Jackson durante varios años, y fue amigo del cantante.
Papá tuvo dificultades para respirar en sus últimos días, y estaba con oxígeno la mayor parte del tiempo.
Así que Michael le dio a papá un carrito de golf con un tanque de oxígeno portátil para que pudiera ir y disfrutar de Neverland.
También sufría de diabetes y cáncer de hígado.
Su única línea grabada fue incluida en el juego final como un tributo al actor.
Un angustiado Brando le dijo a Malden que seguía cayendo.
Poco antes de su muerte, aparentemente había rechazado el permiso para que se insertaran tubos que transportaban oxígeno en sus pulmones, lo que, según se le dijo, era la única forma de prolongar su vida.
En 1976, le dijo a un periodista francés: "La homosexualidad está tan de moda que ya no es noticia.
También afirmó numerosos otros romances, aunque no hablara de sus matrimonios, sus esposas o sus hijos en su autobiografía.
Brando conoció a la actriz Rita Moreno en 1954, y comenzaron una historia de amor.
Años después de que se separaron, Moreno interpretó su interés amoroso en la película La noche del día siguiente.
Se dice que era la hija de un trabajador de acero galés de la ascendencia irlandesa, Guillermo O'Callaghan, que había sido el superintendente en los ferrocarriles estatales indios.
Brando y Kashfi tuvieron un hijo, Christian Brando, el 11 de mayo de 1958; se divorciaron en 1959.
Tuvieron dos hijos: Miko Castaneda Brando (nacido en 1961) y Rebecca Brando (nacido en 1966).
Debido a que Teriipaia era un hablante nativo de francés, Brando se volvió fluido en el idioma y dio numerosas entrevistas en francés.
Brando y Teriipaia se divorciaron en julio de 1972.
Brando tuvo una relación a largo plazo con su ama de llaves María Cristina Ruiz, con quien tuvo tres hijos: Ninna Priscilla Brando (nacida el 13 de mayo de 1989), Myles Jonathan Brando (nacido el 16 de enero de 1992) y Timothy Gahan Brando (nacido el 6 de enero de 1994).
Sus numerosos nietos también incluyen a Prudence Brando y Shane Brando, hijos de Miko C. Brando; los hijos de Rebecca Brando; y los tres hijos de Teihotu Brando entre otros.
Su comportamiento durante el rodaje de Mutiny on the Bounty (1962) pareció reforzar su reputación como una estrella difícil.
Galella había seguido a Brando, quien estaba acompañado por el presentador de talk show Dick Cavett, después de una grabación de The Dick Cavett Show en la ciudad de Nueva York.
El rodaje de Mutiny on the Bounty afectó la vida de Brando de una manera profunda, ya que se enamoró de Tahití y su gente.
El huracán de 1983 destruyó muchas de las estructuras, incluido su complejo turístico.
Fue incluido en los registros de la Comisión Federal de Comunicaciones (FCC) como Martin Brandeaux para preservar su privacidad.
Asistió a algunos eventos de recaudación de fondos para John F. Kennedy en las elecciones presidenciales de 1960.
En el otoño de 1967, Brando visitó Helsinki, Finlandia, en una fiesta benéfica organizada por UNICEF en el Teatro de la ciudad de Helsinki.
Habló a favor de los derechos del niño y la ayuda al desarrollo en los países en desarrollo.
Sentí que sería mejor ir a averiguar dónde está; qué es ser negro en este país; de qué se trata esta rabia”, dijo Brando en el programa de entrevistas nocturno de ABC-TV Joey Bishop Show.
Fue uno de los actos de coraje más increíbles que he visto, y significó mucho e hizo mucho”.
En 1964 Brando fue arrestado en un "fish-in" celebrado para protestar por un tratado roto que había prometido a los nativos americanos los derechos de pesca en Puget Sound.
Brando terminó su apoyo financiero para el grupo sobre su percepción de su radicalización creciente, expresamente un paso en un folleto de la Pantera puesto por Eldridge Cleaver que aboga por la violencia indiscriminada, "para la Revolución".
Sacheen Littlefeather lo representó en la ceremonia.
El evento llamó la atención de los EE.UU. y los medios de comunicación del mundo.
También fue un activista contra el apartheid.
Está catalogado por el American Film Institute como la cuarta estrella masculina más grande cuyo debut en la pantalla ocurrió antes o durante 1950 (ocurrió en 1950).
Encyclopedia Britannica le describe como "el más célebre de los actores del método, y su entrega arrastrada, murmurando marcó su rechazo de la formación dramática clásica.
Fue un desarrollo del líder gángster y el forajido.
Su interpretación del líder de la pandilla Johnny Strabler en The Wild One se ha convertido en una imagen icónica, utilizada tanto como símbolo de rebeldía como un accesorio de moda que incluye una chaqueta de motocicleta de estilo Perfecto, una gorra inclinada, jeans y gafas de sol.
La escena de "Podría haber sido un contendiente" de On the Waterfront, según el autor de Brooklyn Boomer, Martin H. Levinson, es "una de las escenas más famosas de la historia del cine, y la línea en sí se ha convertido en parte del léxico cultural de Estados Unidos".
Tienes que hacerles creer que te estás muriendo... Trata de pensar en el momento más íntimo que hayas tenido en tu vida.
En 1999, el American Film Institute lo clasificó octavo en su lista de las mejores estrellas masculinas de la Edad de Oro de Hollywood.
Pasó varios años en el vodevil como bailarín y comediante, hasta que obtuvo su primera actuación importante en 1925.
Después de excelentes críticas, Warner Bros. lo firmó por un contrato inicial de $ 400 por semana y tres semanas; cuando los ejecutivos del estudio vieron los primeros diarios para la película, el contrato de Cagney se extendió inmediatamente.
Fue nominado por tercera vez en 1955 por Love Me or Leave Me with Doris Day.
Cagney abandonó Warner Bros. varias veces a lo largo de su carrera, cada vez regresando en términos personales y artísticos mucho mejores.
Trabajó para una compañía de cine independiente durante un año mientras se resolvía la demanda, estableciendo su propia compañía de producción, Cagney Productions, en 1942 antes de regresar a Warner siete años después.
Cagney era el segundo de siete hijos, dos de los cuales murieron pocos meses después de su nacimiento.
La familia se mudó dos veces cuando aún era joven, primero a East 79th Street, y luego a East 96th Street.
Lo siento por el niño que tiene un tiempo demasiado cómodo.
Era un buen luchador callejero, defendiendo a su hermano mayor Harry, un estudiante de medicina, cuando era necesario.
Se involucró en el drama amateur, comenzando como un chico de escenario para una pantomima china en Lenox Hill Neighborhood House (una de las primeras casas de asentamiento en la nación) donde su hermano Harry actuó y Florence James dirigió.
El espectáculo comenzó la asociación de 10 años de Cagney con el vodevil y Broadway.
Finalmente, pidieron prestado algo de dinero y regresaron a Nueva York a través de Chicago y Milwaukee, soportando el fracaso en el camino cuando intentaron ganar dinero en el escenario.
Al igual que con Pitter Patter, Cagney fue a la audición con poca confianza en que obtendría el papel.
Este fue un giro devastador de los acontecimientos para Cagney; aparte de las dificultades logísticas que esto presentaba, el equipaje de la pareja estaba en la bodega del barco y habían renunciado a su apartamento.
Decidió que conseguiría un trabajo haciendo otra cosa”.
Cagney también estableció una escuela de baile para profesionales, y luego consiguió un papel en la obra Women Go On Forever, dirigida por John Cromwell, que duró cuatro meses.
El espectáculo recibió críticas favorables y fue seguido por Grand Street Follies de 1929.
Retitulada Sinners' Holiday, la película fue estrenada en 1930.
Sin embargo, el contrato le permitió a Warners dejarlo al final de cualquier período de 40 semanas, lo que le garantizaba solo 40 semanas de ingresos a la vez.
Debido a las fuertes críticas que había recibido en su carrera de cortometraje, Cagney fue elegido como el buen chico Matt Doyle, junto a Edward Woods como Tom Powers.
El productor Darryl Zanuck afirmó que pensó en ello en una conferencia del guión; Wellman dijo que la idea vino a él cuando vio la toronja en la mesa durante el rodaje; y los escritores Glasmon y Bright afirmaron que estaba basado en la vida real del gángster Hymie Weiss, que lanzó una tortilla en la cara de su novia.
Nunca soñé que se mostraría en la película.
Vio la película repetidamente solo para ver esa escena, y a menudo fue silenciado por clientes enojados cuando su risa encantada se hizo demasiado fuerte.
Warner Bros. se apresuró a unir a sus dos estrellas gángster en ascenso, Edward G. Robinson y Cagney, para la película de 1931 Smart Money.
Cuando terminó de filmar, The Public Enemy estaba llenando los cines con presentaciones durante toda la noche.
Los jefes de estudio también insistieron en que Cagney continuara promocionando sus películas, incluso aquellas en las que no estaba, a las que se opuso.
El éxito de The Public Enemy y Blonde Crazy forzó la mano de Warner Bros.
La película fue seguida rápidamente por The Crowd Roars y Winner Take All.
Los historiadores también debaten la naturaleza de la historia como un fin en sí misma, así como su utilidad para dar perspectiva a los problemas del presente.
Sin embargo, las influencias culturales antiguas han ayudado a generar interpretaciones variantes de la naturaleza de la historia que han evolucionado a lo largo de los siglos y continúan cambiando hoy en día.
Herodotus, un historiador griego del 5to siglo A.C., a menudo se considera el "padre de historia" en la tradición Occidental, aunque también se haya criticado como el "padre de mentiras".
En el inglés medio, el significado de la historia era "historia" en general.
En alemán moderno, francés, y la mayor parte de lenguas germánicas y Romances, que son sólidamente sintéticas y muy flexionadas, la misma palabra todavía se usa para significar tanto "historia" como "historia".
En palabras de Benedetto Croce, “Toda la historia es historia contemporánea”.
Por lo tanto, la constitución del archivo del historiador es un resultado de circunscribir un archivo más general invalidando el uso de ciertos textos y documentos (falsificando sus reclamaciones de representar el "pasado verdadero").
El estudio de la historia a veces se ha clasificado como parte de las humanidades y en otras ocasiones como parte de las ciencias sociales.
En el siglo XX, el historiador francés Fernand Braudel revolucionó el estudio de la historia, utilizando disciplinas externas como la economía, la antropología y la geografía en el estudio de la historia global.
En general, las fuentes del conocimiento histórico se pueden separar en tres categorías: lo que se escribe, lo que se dice y lo que se conserva físicamente, y los historiadores a menudo consultan a los tres.
Los hallazgos arqueológicos rara vez son independientes, con fuentes narrativas que complementan sus descubrimientos.
Por ejemplo, Mark Leone, el excavador e intérprete de la histórica Annapolis, Maryland, EE.UU., ha tratado de entender la contradicción entre los documentos textuales que idealizan la "libertad" y el registro material, lo que demuestra la posesión de esclavos y las desigualdades de riqueza que se hacen evidentes por el estudio del entorno histórico total.
Es posible que los historiadores se ocupen tanto de lo muy específico como de lo muy general, aunque la tendencia moderna ha sido hacia la especialización.
En tercer lugar, puede referirse a por qué se produce la historia: la filosofía de la historia.
¿Por quién fue producido (autoría)?
¿Cuál es el valor probatorio de su contenido (credibilidad)?
El método histórico comprende las técnicas y pautas por las cuales los historiadores usan fuentes primarias y otras pruebas para investigar y luego escribir la historia.
Tucídides, a diferencia de Heródoto, consideraba que la historia era el producto de las elecciones y acciones de los seres humanos, y miraba causa y efecto, en lugar de ser el resultado de la intervención divina (aunque Heródoto no estaba totalmente comprometido con esta idea).
Había tradiciones históricas y uso sofisticado del método histórico en China antigua y medieval.
Los historiadores chinos de períodos dinásticos subsecuentes en China usaron su Shiji como el formato oficial para textos históricos, así como para la literatura biográfica.
Alrededor de 1800, el filósofo e historiador alemán Georg Wilhelm Friedrich Hegel trajo la filosofía y un enfoque más secular en el estudio histórico.
La originalidad de Ibn Khaldun era afirmar que la diferencia cultural de otra época debe regir la evaluación del material histórico relevante, distinguir los principios según los cuales podría ser posible intentar la evaluación y, por último, sentir la necesidad de experiencia, además de principios racionales, para evaluar una cultura del pasado.
Su método histórico también sentó las bases para la observación del papel del estado, la comunicación, la propaganda y el sesgo sistemático en la historia, H. Mowlana (2001).
Dr. S.W. Akhtar (1997).
Para Ranke, los datos históricos deben recopilarse cuidadosamente, examinarse objetivamente y combinarse con rigor crítico.
En el siglo XX, los historiadores académicos se centraron menos en las narrativas nacionalistas épicas, que a menudo tendían a glorificar a la nación o a los grandes hombres, a análisis más objetivos y complejos de las fuerzas sociales e intelectuales.
Muchos de los defensores de la historia como ciencia social fueron o son conocidos por su enfoque multidisciplinario.
Hasta ahora, sólo una teoría de la historia provenía de la pluma de un historiador profesional.
Los historiadores intelectuales como Herbert Butterfield, Ernst Nolte y George Mosse han abogado por la importancia de ideas en la historia.
Académicos como Martin Broszat, Ian Kershaw y Detlev Peukert trataron de examinar cómo era la vida cotidiana para la gente común en la Alemania del siglo XX, especialmente en el período nazi.
Historiadores feministas como Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese y Lynn Hunt han defendido la importancia de estudiar la experiencia de las mujeres en el pasado.
Otra defensa de la historia de la crítica postmodernista fue el libro de 1994 del historiador australiano Keith Windschuttle, The Killing of History.
Las omisiones históricas pueden ocurrir de muchas maneras y pueden tener un profundo efecto en los registros históricos.
Historia antigua: el estudio desde el principio de la historia humana hasta la Edad media Temprana.
Historia comparada: análisis histórico de entidades sociales y culturales no limitadas a las fronteras nacionales.
Historia cultural: el estudio de la cultura en el pasado.
Historia intelectual: el estudio de las ideas en el contexto de las culturas que las produjeron y su desarrollo a lo largo del tiempo.
Historia moderna: el estudio de los tiempos modernos, la era posterior a la Edad Media.
Paleografía: estudio de textos antiguos.
Psicohistoria: estudio de las motivaciones psicológicas de los acontecimientos históricos.
Historia de la mujer: la historia de los seres humanos femeninos.
Siglos y décadas son períodos de uso común y el tiempo que representan depende del sistema de datación utilizado.
Para ello, los historiadores a menudo recurren a la geografía.
Por ejemplo, para explicar por qué los antiguos egipcios desarrollaron una civilización exitosa, estudiar la geografía de Egipto es esencial.
Historia de las Américas es la historia colectiva de América del Norte y del Sur, incluyendo América Central y el Caribe.
La historia del Caribe comienza con la evidencia más antigua donde se han encontrado restos de hace 7.000 años.
La historia de Eurasia es la historia colectiva de varias regiones costeras periféricas distintas: el Oriente Medio, Asia del Sur, Asia Oriental, Sudeste Asiático y Europa, unidas por la masa interior de la estepa euroasiática de Asia Central y Europa Oriental.
La historia de Asia Oriental es el estudio del pasado transmitido de generación en generación en Asia Oriental.
La historia del sudeste asiático se ha caracterizado como la interacción entre los actores regionales y las potencias extranjeras.
La "vieja" historia social antes de la década de 1960 era una mezcolanza de temas sin un tema central, y a menudo incluía movimientos políticos, como el populismo, que eran "sociales" en el sentido de estar fuera del sistema de élite.
Examina los registros y las descripciones narrativas del conocimiento, las costumbres y las artes del pasado de un grupo de personas.
Este tipo de historia política es el estudio de la conducta de las relaciones internacionales entre los estados o a través de las fronteras estatales a lo largo del tiempo.
Ganó popularidad en los Estados Unidos, Japón y otros países después de la década de 1980 con la comprensión de que los estudiantes necesitan una exposición más amplia al mundo a medida que avanza la globalización.
A pesar de ser un campo relativamente nuevo, la historia de género ha tenido un efecto significativo en el estudio general de la historia.
En Oxford y Cambridge, la erudición fue minimizada.
Los tutores dominaron el debate hasta después de la Segunda Guerra Mundial.
En los Estados Unidos después de la Primera guerra mundial, un movimiento fuerte surgió en el nivel universitario para enseñar cursos en la Civilización Occidental, a fin de dar a estudiantes una herencia común con Europa.
Muchos ven el campo desde ambas perspectivas.
En los Estados Unidos, los libros de texto publicados por la misma compañía a menudo difieren en contenido de un estado a otro.
Los historiadores académicos a menudo han luchado contra la politización de los libros de texto, a veces con éxito.
Una civilización (o civilización) es una sociedad compleja que se caracteriza por el desarrollo urbano, la estratificación social, una forma de gobierno y sistemas simbólicos de comunicación (como la escritura).
En este sentido amplio, una civilización contrasta con las sociedades tribales no centralizadas, incluidas las culturas de pastores nómadas, sociedades neolíticas o cazadores-recolectores; sin embargo, a veces también contrasta con las culturas que se encuentran dentro de las propias civilizaciones.
El tratado fundamental es El proceso civilizador (1939) de Norbert Elias, que traza las costumbres sociales desde la sociedad cortesana medieval hasta el período moderno temprano.
Palabras relacionadas como "civilidad" se desarrollaron a mediados del siglo XVI.
A finales de 1700 y principios de 1800, durante la Revolución Francesa, la "civilización" se utilizó en singular, nunca en plural, y significó el progreso de la humanidad en su conjunto.
Sólo en este sentido generalizado es posible hablar de una "civilización medieval", que en el sentido de Elías habría sido un oxímoron.
Aquí, la civilización, al ser más racional y socialmente impulsada, no está totalmente de acuerdo con la naturaleza humana, y "la totalidad humana solo se puede lograr a través de la recuperación o aproximación a una unidad natural discursiva o preracional original" (ver noble salvaje).
Las civilizaciones se han distinguido por sus medios de subsistencia, tipos de medios de vida, patrones de asentamiento, formas de gobierno, estratificación social, sistemas económicos, alfabetización y otros rasgos culturales.
Todas las civilizaciones han dependido de la agricultura para la subsistencia, con la posible excepción de algunas civilizaciones tempranas en Perú que pueden haber dependido de recursos marítimos.
Los excedentes de grano han sido especialmente importantes porque el grano se puede almacenar durante mucho tiempo.
Sin embargo, en algunos lugares, los cazadores-recolectores han tenido acceso a excedentes de alimentos, como entre algunos de los pueblos indígenas del noroeste del Pacífico y tal vez durante la cultura mesolítica natufiana.
La palabra "civilización" a veces se define simplemente como "vivir en ciudades".
Las sociedades estatales están más estratificadas que otras sociedades; hay una mayor diferencia entre las clases sociales.
Civilizaciones, con jerarquías sociales complejas y gobiernos institucionales organizados.
Algunas personas también adquieren la propiedad de la tierra, o la propiedad privada de la tierra.
A principios de la Edad del Hierro, las civilizaciones contemporáneas desarrollaron el dinero como medio de intercambio para transacciones cada vez más complejas.
Es posible que estas personas no se conozcan personalmente entre sí y que sus necesidades no ocurran todas al mismo tiempo.
La transición de economías más simples a economías más complejas no significa necesariamente una mejora en el nivel de vida de la población.
La estatura promedio de una población es una buena medida de la adecuación de su acceso a las necesidades, especialmente los alimentos.
Al igual que el dinero, la escritura era necesaria por el tamaño de la población de una ciudad y la complejidad de su comercio entre personas que no están todas personalmente familiarizadas entre sí.
Estos incluyen la religión organizada, el desarrollo en las artes, y un sinnúmero de nuevos avances en la ciencia y la tecnología.
Estas culturas son llamadas por algún "primitivo", un término que es considerado por otros como peyorativo.
Los antropólogos de hoy usan el término "no alfabetizado" para describir a estos pueblos.
Pero la civilización también se propaga por el dominio técnico, material y social que engendra la civilización.
Las civilizaciones tienden a desarrollar culturas intrincadas, incluyendo un aparato de toma de decisiones basado en el estado, una literatura, arte profesional, arquitectura, religión organizada y costumbres complejas de educación, coerción y control asociadas con el mantenimiento de la élite.
La civilización en la que vive alguien es la identidad cultural más amplia de esa persona.
El objetivo es preservar el patrimonio cultural de la humanidad y también la identidad cultural, especialmente en caso de guerra y conflicto armado.
El filósofo de principios del siglo veinte Oswald Spengler, Spengler, Oswald, Decadencia del Oeste: Perspectivas de la Historia mundial (1919) usa la palabra alemana Kultur, "cultura", para lo que muchos llaman una "civilización".
Spengler afirma que la civilización es el comienzo de la decadencia de una cultura como "los estados más externos y artificiales de los que una especie de humanidad desarrollada es capaz".
Las civilizaciones generalmente declinaron y cayeron, según Toynbee, debido al fracaso de una "minoría creativa", a través de la decadencia moral o religiosa, para encontrar algún desafío importante, más bien que meras causas económicas o ambientales.
Por ejemplo, las redes comerciales eran, hasta el siglo XIX, mucho más grandes que las esferas culturales o las esferas políticas.
Durante el período de Uruk, Guillermo Algaze ha sostenido que las relaciones comerciales conectaron Egipto, Mesopotamia, Irán y Afganistán.
Diferentes civilizaciones y sociedades en todo el mundo son económica, política e incluso culturalmente interdependientes de muchas maneras.
La civilización central más tarde se expandió para incluir todo el Medio Oriente y Europa, y luego se expandió a una escala global con la colonización europea, integrando las Américas, Australia, China y Japón en el siglo XIX.
Esto alentó una revolución de productos secundarios en la que las personas usaban animales domesticados no solo para carne, sino también para leche, lana, estiércol y arados y carros de tracción, un desarrollo que se extendió a través del Oecumeno euroasiático.
Esta área ha sido identificada por haber "inspirado algunos de los desarrollos más importantes en la historia humana, incluida la invención de la rueda, la siembra de los primeros cultivos de cereales y el desarrollo de la escritura cursiva".
Este cambio climático cambió la relación costo-beneficio de la violencia endémica entre las comunidades, que vio el abandono de las comunidades de las aldeas sin muros y la aparición de ciudades amuralladas, asociadas con las primeras civilizaciones.
La revolución urbana civilizada, a su vez, dependía del desarrollo del sedentismo, la domesticación de granos y animales, la permanencia de los asentamientos y el desarrollo de estilos de vida que facilitaban las economías de escala y la acumulación de excedentes de producción por parte de ciertos sectores sociales.
Algunos se centran en ejemplos históricos, y otros en la teoría general.
Para Gibbon, "La decadencia de Roma era el efecto natural e inevitable de la grandeza inmoderada.
Theodor Mommsen en su Historia de Roma sugirió que Roma se derrumbó con el colapso del Imperio Romano de Occidente en 476 CE y también tendió hacia una analogía biológica de "génesis", "crecimiento", "senescencia", "colapso" y "decadencia".
Arnold J. Toynbee en su Un Estudio de la Historia sugirió que había habido un número mucho mayor de civilizaciones, incluso un pequeño número de civilizaciones detenidas, y que todas las civilizaciones tendieron a pasar por el ciclo identificado por Mommsen.
Durante la fase intermedia, el crecimiento creciente de la población conduce a la disminución de la producción per cápita y los niveles de consumo, se hace cada vez más difícil recaudar impuestos, y los ingresos estatales dejan de crecer, mientras que los gastos estatales crecen debido al crecimiento de la población controlada por el estado.
Ciclos seculares y tendencias milenarias.
El hecho de que Roma necesitaba generar ingresos cada vez mayores para equipar y reequipar ejércitos que fueron derrotados por primera vez repetidamente en el campo, llevó al desmembramiento del Imperio.
Argumenta que el colapso de los mayas tiene lecciones para la civilización de hoy.
La energía gastada en la relación de rendimiento de energía es fundamental para limitar la supervivencia de las civilizaciones.
Koneczny afirmó que las civilizaciones no pueden mezclarse en híbridos, una civilización inferior cuando se le otorgan los mismos derechos dentro de una civilización altamente desarrollada lo superará.
El historiador cultural Morris Berman sugiere en Dark Ages America: the End of Empire que en los Estados Unidos consumistas corporativos, los mismos factores que una vez los impulsaron a la grandeza -individualismo extremo, expansión territorial y económica, y la búsqueda de riqueza material- han empujado a los Estados Unidos a través de un umbral crítico donde el colapso es inevitable.
La corrosión de estos pilares, argumenta Jacobs, está vinculada a males sociales como la crisis ambiental, el racismo y el creciente abismo entre ricos y pobres.
Esta necesidad de que las civilizaciones importen cada vez más recursos, argumenta, se deriva de su sobreexplotación y disminución de sus propios recursos locales.
En el gráfico, Ma significa "hace millones de años".)
Gran parte de la Tierra estaba fundida debido a las frecuentes colisiones con otros cuerpos que llevaron al vulcanismo extremo.
Los humanos reconocibles surgieron como máximo hace 2 millones de años, un período fugazmente pequeño en la escala geológica.
Se estima que el 99 por ciento de todas las especies que alguna vez vivieron en la Tierra, más de cinco mil millones, se han extinguido.
La corteza terrestre ha cambiado constantemente desde su formación, al igual que la vida desde su primera aparición.
La Luna se forma alrededor de este tiempo probablemente debido a la colisión de un protoplaneta en la Tierra.
La atmósfera está compuesta de gases volcánicos y de efecto invernadero.
Las bacterias comienzan a producir oxígeno, dando forma a la tercera y corriente de las atmósferas de la Tierra.
Los primeros continentes de Colombia, Rodinia y Pannotia, en ese orden, pueden haber existido en este eón.
Gradualmente, la vida se expande a la tierra y comienzan a aparecer formas familiares de plantas, animales y hongos, incluidos anélidos, insectos y reptiles, de ahí el nombre del eón, que significa "vida visible".
Estaba compuesto de hidrógeno y helio creados poco después del Big Bang 13,8 Ga (hace miles de millones de años) y elementos más pesados expulsados por las supernovas.
A medida que la nube comenzó a acelerarse, su momento angular, gravedad e inercia la aplanaron en un disco protoplanetario perpendicular a su eje de rotación.
Después de más contracción, una estrella T Tauri se encendió y se convirtió en el Sol.
La Tierra se formó de esta manera hace unos 4.540 millones de años (con una incertidumbre del 1%) y se completó en gran medida dentro de 10-20 millones de años.
La proto-Tierra creció por acreción hasta que su interior estuvo lo suficientemente caliente como para derretir los pesados metales siderófilos.
A partir de los recuentos de cráteres en otros cuerpos celestes, se infiere que un período de intensos impactos de meteoritos, llamado el Bombardeo Pesado Tardío, comenzó alrededor de 4.1 Ga, y concluyó alrededor de 3.8 Ga, al final del Hadeano.
Al comienzo del Arcaico, la Tierra se había enfriado significativamente.
La nueva evidencia sugiere que la Luna se formó incluso más tarde, 4,48 x 0,02 Ga, o 70-110 millones de años después del inicio del Sistema Solar.
La colisión liberó aproximadamente 100 millones de veces más energía que el impacto más reciente de Chicxulub que se cree que causó la extinción de los dinosaurios no aviares.
La hipótesis del impacto gigante predice que la Luna estaba agotada de material metálico, lo que explica su composición anormal.
La corteza inicial, formada cuando la superficie de la Tierra solidificó por primera vez, desapareció totalmente de una combinación de esta rápida tectónica de placas hadeanas y los intensos impactos del Bombardeo Pesado Tardío.
Estas piezas de la corteza tardía de Hadean y Archean temprano forman los núcleos alrededor de los cuales los continentes de hoy crecieron.
Los cratones consisten principalmente en dos tipos alternativos de terranes.
Por esta razón, las piedras verdes a veces se ven como evidencia de subducción durante el Arcaico.
Ahora se considera probable que muchos de los volátiles se suministraron durante la acreción mediante un proceso conocido como desgasificación por impacto en el que los cuerpos entrantes se vaporizan en el impacto.
Los planetesimales a una distancia de 1 unidad astronómica (UA), la distancia de la Tierra al Sol, probablemente no contribuyeron con agua a la Tierra porque la nebulosa solar estaba demasiado caliente para que se formara hielo y la hidratación de las rocas por el vapor de agua habría tomado demasiado tiempo.
La evidencia reciente sugiere que los océanos pueden haber comenzado a formarse ya en 4.4 Ga. Para el comienzo del eón Arcaico, ya cubrían gran parte de la Tierra.
Por lo tanto, el Sol se ha vuelto un 30% más brillante en los últimos 4.500 millones de años.
Hay muchos modelos, pero poco consenso, sobre cómo surgió la vida de los productos químicos no vivos; los sistemas químicos creados en el laboratorio están muy por debajo de la complejidad mínima para un organismo vivo.
Aunque la composición atmosférica era probablemente diferente de la utilizada por Miller y Urey, experimentos posteriores con composiciones más realistas también lograron sintetizar moléculas orgánicas.
El ARN más tarde habría sido reemplazado por el ADN, que es más estable y, por lo tanto, puede construir genomas más largos, ampliando el rango de capacidades que puede tener un solo organismo.
Una dificultad con el escenario del metabolismo primero es encontrar una manera para que los organismos evolucionen.
La investigación en 2003 informó que la montmorillonita también podría acelerar la conversión de ácidos grasos en "burbujas", y que las burbujas podrían encapsular el ARN unido a la arcilla.
Esta célula LUA es el ancestro de toda la vida en la Tierra hoy en día.
El cambio a una atmósfera rica en oxígeno fue un desarrollo crucial.
Utilizaron la fermentación, la descomposición de compuestos más complejos en compuestos menos complejos con menos energía, y utilizaron la energía así liberada para crecer y reproducirse.
La mayor parte de la vida que cubre la superficie de la Tierra depende directa o indirectamente de la fotosíntesis.
Para suministrar los electrones en el circuito, el hidrógeno se elimina del agua, dejando oxígeno como producto de desecho.
La forma anoxigénica más simple surgió alrededor de 3,8 Ga, no mucho después de la aparición de la vida.
Al principio, el oxígeno liberado estaba ligado con piedra caliza, hierro y otros minerales.
Aunque cada célula sólo produjo una pequeña cantidad de oxígeno, el metabolismo combinado de muchas células durante un vasto período de tiempo transformó la atmósfera de la Tierra a su estado actual.
La capa de ozono absorbió, y todavía absorbe, una cantidad significativa de la radiación ultravioleta que una vez había pasado a través de la atmósfera.
Como resultado, la Tierra comenzó a recibir más calor del Sol en el eón Proterozoico.
Los depósitos glaciales encontrados en Sudáfrica se remontan a 2.2 Ga, momento en el que, según la evidencia paleomagnética, deben haber sido ubicados cerca del ecuador.
La edad de hielo de Huron podría haber sido causada por el aumento de la concentración de oxígeno en la atmósfera, lo que causó la disminución de metano (CH4) en la atmósfera.
Sin embargo, el término Tierra Bola de Nieve se usa más comúnmente para describir las glaciaciones extremas posteriores durante el período criogénico.
El dióxido de carbono se combina con la lluvia para capear las rocas para formar ácido carbónico, que luego se lava al mar, extrayendo así el gas de efecto invernadero de la atmósfera.
El dominio de las bacterias probablemente se separó por primera vez de las otras formas de vida (a veces llamado Neomura), pero esta suposición es controvertida.
Los primeros fósiles que poseen características típicas de hongos datan de la era Paleoproterozoica, hace unos 2,4 años; estos organismos bentónicos multicelulares tenían estructuras filamentosas capaces de anastomosis.
Tal vez la célula grande intentó digerir la más pequeña pero fracasó (posiblemente debido a la evolución de las defensas de las presas).
Usando oxígeno, metabolizaba los productos de desecho de las células más grandes y obtenía más energía.
Pronto, se desarrolló una simbiosis estable entre la célula grande y las células más pequeñas dentro de ella.
Un evento similar ocurrió con las cianobacterias fotosintéticas entrando en grandes células heterótrofas y convirtiéndose en cloroplastos.
Además de la teoría endosimbiótica bien establecida del origen celular de las mitocondrias y los cloroplastos, hay teorías de que las células condujeron a peroxisomas, las espiroquetas condujeron a cilios y flagelos, y que tal vez un virus de ADN condujo al núcleo celular, aunque ninguno de ellos es ampliamente aceptado.
Alrededor de 1.1 Ga, el supercontinente Rodinia se estaba reuniendo.
Aunque la división entre una colonia con células especializadas y un organismo multicelular no siempre es clara, hace alrededor de mil millones de años, surgieron las primeras plantas multicelulares, probablemente algas verdes.
Los polos paleomagnéticos se complementan con evidencia geológica como los cinturones orogénicos, que marcan los bordes de las placas antiguas, y las distribuciones pasadas de flora y fauna.
Alrededor de 1000 a 830 Ma, la mayor parte de la masa continental se unió en el supercontinente Rodinia.
El supercontinente hipotético a veces se conoce como Pannotia o Vendia.
La intensidad y el mecanismo de ambas glaciaciones aún están bajo investigación y son más difíciles de explicar que la Tierra Proterozoica Bola de Nieve temprana.
Debido a que el CO2 es un importante gas de efecto invernadero, los climas se enfriaron a nivel mundial.
El aumento de la actividad volcánica resultó de la desintegración de Rodinia al mismo tiempo.
Las nuevas formas de vida, llamadas biota de Ediacara, eran más grandes y más diversas que nunca.
Consiste en tres eras: el Paleozoico, el Mesozoico y el Cenozoico, y es el momento en que la vida multicelular se diversificó en gran medida en casi todos los organismos conocidos hoy en día.
Esto hace que el nivel del mar aumente.
Las huellas de la glaciación de este período sólo se encuentran en la antigua Gondwana.
Los continentes Laurentia y Baltica colisionaron entre 450 y 400 Ma, durante la orogenia de Caledonia, para formar Laurussia (también conocida como Euramerica).
La colisión de Siberia con Laurussia causó la Orogenia Uralian, la colisión de Gondwana con Laurussia se llama la Orogenia Variscan o Hercynian en Europa o la Orogenia Alleghenia en Norteamérica.
Mientras que las formas de vida ediacaranas parecen aún primitivas y no fáciles de poner en ningún grupo moderno, al final de los filos cámbricos más modernos ya estaban presentes.
Algunos de estos grupos cámbricos parecen complejos, pero son aparentemente muy diferentes de la vida moderna; ejemplos son Anomalocaris y Haikouichthys.
Una criatura que podría haber sido el antepasado de los peces, o probablemente estaba estrechamente relacionada con él, era Pikaia.
Los peces, los primeros vertebrados, evolucionaron en los océanos alrededor de 530 Ma.
Los fósiles más antiguos de hongos y plantas terrestres datan de 480-460 Ma, aunque la evidencia molecular sugiere que los hongos pueden haber colonizado la tierra ya en 1000 Ma y las plantas 700 Ma.
Las aletas evolucionaron para convertirse en extremidades que los primeros tetrápodos usaban para levantar la cabeza del agua para respirar aire.
Eventualmente, algunos de ellos se adaptaron tan bien a la vida terrestre que pasaron su vida adulta en la tierra, aunque eclosionaron en el agua y regresaron para poner sus huevos.
Las plantas desarrollaron semillas, que aceleraron dramáticamente su propagación en la tierra, alrededor de este tiempo (aproximadamente 360 Ma).
Otros 30 millones de años (310 Ma) vieron la divergencia de los sinápsidos (incluidos los mamíferos) de los saurópsidos (incluidas las aves y los reptiles).
El evento de extinción Triásico-Jurásico en 200 Ma salvó a muchos de los dinosaurios, y pronto se volvieron dominantes entre los vertebrados.
El 60% de los invertebrados marinos se extinguieron y el 25% de todas las familias.
La tercera extinción masiva fue el evento Pérmico-Triásico, o Gran Mortandad, fue posiblemente causada por alguna combinación del evento volcánico Trampas Siberianas, un impacto de asteroides, gasificación de hidrato de metano, fluctuaciones del nivel del mar y un evento anóxico importante.
Esta fue, con mucho, la extinción más mortífera de la historia, con aproximadamente el 57% de todas las familias y el 83% de todos los géneros asesinados.
A principios del Paleoceno la tierra se recuperó de la extinción, y la diversidad de mamíferos aumentó.
La sabana sin pasto comenzó a predominar en gran parte del paisaje, y los mamíferos como Andrewsarchus se elevaron para convertirse en el mamífero depredador terrestre más grande conocido, y las primeras ballenas como Basilosaurus tomaron el control de los mares.
Ungulados gigantes como Paraceratherium y Deinotherium evolucionaron para gobernar las praderas.
El mar de Tetis fue cerrado por la colisión de África y Europa.
El puente terrestre permitió que las criaturas aisladas de América del Sur migraran a América del Norte, y viceversa.
Las edades de hielo llevaron a la evolución del hombre moderno en el África subsahariana y la expansión.
Muchos creen que tuvo lugar una gran migración a lo largo de Beringia, por lo que, hoy en día, hay camellos (que evolucionaron y se extinguieron en América del Norte), caballos (que evolucionaron y se extinguieron en América del Norte) y nativos americanos.
El tamaño del cerebro aumentó rápidamente, y en 2 Ma, los primeros animales clasificados en el género Homo habían aparecido.
La capacidad de controlar el fuego probablemente comenzó en Homo erectus (u Homo ergaster), probablemente hace al menos 790.000 años, pero tal vez ya en 1.5 Ma.
Es más difícil establecer el origen del lenguaje; no está claro si el Homo erectus podía hablar o si esa capacidad no había comenzado hasta el Homo sapiens.
Las habilidades sociales se volvieron más complejas, el lenguaje más sofisticado y las herramientas más elaboradas.
Los primeros humanos en mostrar signos de espiritualidad son los neandertales (generalmente clasificados como una especie separada sin descendientes sobrevivientes); enterraron a sus muertos, a menudo sin signos de alimentos o herramientas.
A medida que el lenguaje se volvió más complejo, la capacidad de recordar y comunicar información resultó, según una teoría propuesta por Richard Dawkins, en un nuevo replicador: el meme.
Entre 8500 y 7000 aC, los seres humanos en el Creciente Fértil en el Medio Oriente comenzaron la cría sistemática de plantas y animales: la agricultura.
Sin embargo, entre las civilizaciones que sí adoptaron la agricultura, la estabilidad relativa y el aumento de la productividad proporcionada por la agricultura permitieron que la población se expandiera.
Esto llevó a la primera civilización de la Tierra en Sumer en el Medio Oriente, entre 4000 y 3000 aC.
Los humanos ya no tenían que pasar todo su tiempo trabajando para sobrevivir, permitiendo las primeras ocupaciones especializadas (por ejemplo, artesanos, comerciantes, sacerdotes, etc.).
Alrededor del 500 aC, hubo civilizaciones avanzadas en el Medio Oriente, Irán, India, Chína y Grecia, a veces expandiéndose, a veces entrando en declive.
Esta civilización se desarrolló en la guerra, las artes, la ciencia, las matemáticas y en el arquitecto.
El Imperio Romano fue cristianizado por el emperador Constantino a principios del siglo IV y declinó a finales del V.
La Casa de la Sabiduría se estableció en la Bagdad de la era abasí, Irak.
En el siglo XIV, el Renacimiento comenzó en Italia con avances en religión, arte y ciencia.
La civilización europea comenzó a cambiar a partir de 1500, dando lugar a las revoluciones científicas e industriales.
De 1914 a 1918 y de 1939 a 1945, las naciones de todo el mundo se vieron envueltas en guerras mundiales.
Después de la guerra, muchos nuevos estados se formaron, declarando o concediéndose la independencia en un período de la descolonización.
Los desarrollos tecnológicos incluyen armas nucleares, computadoras, ingeniería genética y nanotecnología.
Las principales preocupaciones y problemas como la enfermedad, la guerra, la pobreza, el radicalismo violento y, recientemente, el cambio climático causado por el hombre han aumentado a medida que aumenta la población mundial.
La historia humana, o historia registrada, es la narrativa del pasado de la humanidad.
El Neolítico vio comenzar la Revolución Agrícola, entre 10.000 y 5000 aC, en el Creciente Fértil del Cercano Oriente.
A medida que se desarrolló la agricultura, la agricultura de granos se hizo más sofisticada y provocó una división del trabajo para almacenar alimentos entre temporadas de cultivo.
El hinduismo se desarrolló a finales de la Edad del Bronce en el subcontinente indio.
La historia postclásica (la "Edad Media", c. 500-1500 dC) fue testigo del surgimiento del cristianismo, la Edad de Oro islámica (c. 750 dC - c. 1258 dC) y los renacimientos timúrida e italiana (desde alrededor de 1300 dC).
En el siglo XVIII, la acumulación de conocimiento y tecnología había alcanzado una masa crítica que provocó la Revolución Industrial y comenzó el período moderno tardío, que comenzó alrededor de 1800 y ha continuado hasta el presente.
Los humanos anatómicamente modernos surgieron en África hace unos 300.000 años, y lograron la modernidad conductual hace unos 50.000 años.
Tal vez ya hace 1,8 millones de años, pero ciertamente hace 500.000 años, los humanos comenzaron a usar el fuego para calentar y cocinar.
Los humanos paleolíticos vivían como cazadores-recolectores, y eran generalmente nómadas.
La rápida expansión de la humanidad a América del Norte y Oceanía tuvo lugar en el clímax de la edad de hielo más reciente.
El valle del río Amarillo en Chína cultivó mijo y otros cultivos de cereales alrededor de 7000 aC; el valle de Yangtze domesticó el arroz antes, por lo menos 8000 aC.
La metalurgia, se utilizó por primera vez en la creación de herramientas y adornos de cobre alrededor de 6000 aC.
Las ciudades eran centros de comercio, manufactura y poder político.
El desarrollo de las ciudades era sinónimo del surgimiento de la civilización.
Estas culturas inventaron diversamente la rueda, las matemáticas, el bronce, los barcos de vela, la rueda del alfarero, la tela tejida, la construcción de edificios monumentales y la escritura.
Típico del Neolítico era una tendencia a adorar a las deidades antropomórficas.
Estos asentamientos se concentraron en fértiles valles fluviales: el Tigris y el Éufrates en Mesopotamia, el Nilo en Egipto, el Indo en el subcontinente indio y los ríos Yangtze y Amarillo en Chína.
La escritura cuneiforme comenzó como un sistema de pictografías, cuyas representaciones pictóricas finalmente se simplificaron y fueron más abstractas.
El transporte fue facilitado por vías fluviales, por ríos y mares.
Estos desarrollos llevaron al surgimiento de estados e imperios territoriales.
En Creta, la civilización minoica había entrado en la Edad del Bronce en 2700 aC y es considerada como la primera civilización en Europa.
Durante los siguientes milenios, las civilizaciones se desarrollaron en todo el mundo.
En la India, esta era fue el período védico (1750-600 aC), que sentó las bases del hinduismo y otros aspectos culturales de la sociedad india temprana, y terminó en el siglo VI aC.
Durante la etapa formativa en Mesoamérica (aproximadamente 1500 BCE a 500 CE), las civilizaciones más complejas y centralizadas comenzaron a desarrollarse, generalmente en lo que es ahora México, Centroamérica y Perú.
La teoría de la edad axial de Karl Jaspers también incluye el zoroastrismo persa, pero otros estudiosos disputan su línea de tiempo para el zoroastrismo.)
Estos fueron el taoísmo, el legalismo y el confucianismo.
Los grandes imperios dependían de la anexión militar del territorio y de la formación de asentamientos defendidos para convertirse en centros agrícolas.
Hubo una serie de imperios regionales durante este período.
El Imperio Mediano dio paso a sucesivos imperios iraníes, incluido el Imperio aqueménida (550-330 aC), el Imperio parto (247 aC-224 dC) y el Imperio sasánida (224-651 dC).
Más tarde, Alejandro Magno (356-323 aC), de Macedonia, fundó un imperio de conquista, que se extiende desde la actual Grecia hasta la actual India.
Desde el siglo III dC, la dinastía Gupta supervisó el período conocido como la Edad de Oro de la antigua India.
La estabilidad consiguiente contribuyó a anunciar en la edad de oro de la cultura hindú en los 4tos y 5tos siglos.
En la época de Augusto (63 aC - 14 dC), el primer emperador romano, Roma ya había establecido el dominio sobre la mayor parte del Mediterráneo.
El imperio occidental caería, en 476 CE, a la influencia alemana bajo Odoacro.
La dinastía Han era comparable en poder e influencia al Imperio Romano que se encontraba en el otro extremo de la Ruta de la Seda.
Al igual que con otros imperios durante el Período Clásico, Han Chína avanzó significativamente en las áreas de gobierno, educación, matemáticas, astronomía, tecnología y muchos otros.
Los imperios regionales exitosos también se establecieron en las Américas, surgiendo de culturas establecidas ya en 2500 aC.
Las grandes ciudades-estado mayas aumentaron lentamente en número y prominencia, y la cultura maya se extendió por todo Yucatán y sus alrededores.
Sin embargo, en algunas regiones hubo períodos de rápido progreso tecnológico.
La dinastía Han de Chína cayó en guerra civil en 220 CE, comenzando el período de los Tres Reinos, mientras que su contraparte romana se descentralizó y se dividió cada vez más al mismo tiempo en lo que se conoce como la Crisis del Tercer Siglo.
El desarrollo del estribo y la cría de caballos lo suficientemente fuertes como para llevar a un arquero completamente armado hicieron de los nómadas una amenaza constante para las civilizaciones más establecidas.
La parte restante del Imperio Romano, en el Mediterráneo del Este, siguió como lo que vino para llamarse el Imperio Bizantino.
La era comúnmente se fecha a partir de la caída del 5to siglo del Imperio Romano Occidental, que se fragmentó en muchos reinos separados, algunos de los cuales se confederarían más tarde bajo el Sacro Imperio Romano.
El sur de Asia vio una serie de reinos medios de la India, seguidos por el establecimiento de imperios islámicos en la India.
Esto permitió que África se afiliara al sistema comercial de Sudeste Asiático, trayéndolo el contacto con Asia; esto, junto con la cultura musulmana, causó la cultura de Swahili.
Esta también fue una batalla cultural, con la cultura helenística y cristiana bizantina compitiendo contra las tradiciones persas iraníes y la religión zoroástrica.
Desde su centro en la Península Arábiga, los musulmanes comenzaron su expansión a principios de la Era Posclásica.
Gran parte de este aprendizaje y desarrollo puede estar vinculado a la geografía.
La influencia de los comerciantes musulmanes sobre las rutas comerciales afro-árabes y árabes-asiáticas fue tremenda.
Motivados por la religión y los sueños de conquista, los líderes europeos lanzaron una serie de Cruzadas para tratar de hacer retroceder el poder musulmán y recuperar Tierra Santa.
La dominación árabe de la región terminó a mediados del 11er siglo con la llegada de los turcos de Seljuq, emigrando el sur de las patrias de Turkic en Asia Central.
La región más tarde se llamará la Costa de Berbería y recibirá a piratas y corsarios que usarán varios puertos africanos del Norte para sus incursiones contra las ciudades costeras de varios países europeos en busca de esclavos para venderse en mercados africanos del Norte como la parte de la trata de esclavos de Berbería.
En el siglo VIII, el Islam comenzó a penetrar en la región y pronto se convirtió en la única fe de la mayoría de la población, aunque el budismo se mantuvo fuerte en el este.
Después de que Genghis Khan murió en 1227, la mayor parte de Asia Central siguió siendo dominada por un estado del sucesor, Chagatai Khanate.
La región luego se dividió en una serie de kanatos más pequeños que fueron creados por los uzbekos.
Los invasores bárbaros formaron sus propios nuevos reinos en los restos del Imperio Romano de Occidente.
El cristianismo se expandió en Europa occidental y se fundaron monasterios.
Manorialism, la organización de campesinos en pueblos que debían alquileres y servicio de trabajo a la nobleza, y feudalism, una estructura política por lo cual caballeros y nobleza del estado inferior debían el servicio militar a sus jefes supremos a cambio del derecho a alquileres de tierras y señoríos, eran dos de las formas de organizar la sociedad medieval que se desarrolló durante la Edad media Alta.
Los comerciantes italianos importaban esclavos para trabajar en los hogares o en el procesamiento del azúcar.
El hambre, la peste y la guerra devastaron a la población de Europa occidental.
Finalmente dieron paso a la dinastía Zagwe que son famosos por su arquitectura de corte de roca en Lalibela.
Ellos controlaban el comercio transahariano de oro, marfil, sal y esclavos.
África Central vio el nacimiento de varios estados, incluido el Reino de Kongo.
Construyeron estructuras de piedra defensivas grandes sin el mortero como Gran Zimbabwe, capital del Reino de Zimbabve, Khami, capital del Reino de Butua y Danangombe (Dhlo-Dhlo), capital del Imperio de Rozvi.
El siglo IX vio una lucha tripartita por el control del norte de la India, entre el Imperio Pratihara, el Imperio Pala y el Imperio Rashtrakuta.
Sin embargo, la dinastía Tang finalmente se dividió, y después de medio siglo de agitación, la dinastía Song reunificó a Chína, cuando era, según William McNeill, el "país más rico, más hábil y más poblado de la tierra".
Después de aproximadamente un siglo del gobierno de la dinastía Yuan mongol, los chinos étnicos reafirmaron el control con la fundación de la dinastía Ming (1368).
El período de Nara del 8vo siglo marcó la aparición de un estado japonés fuerte y a menudo se retrata como una edad de oro.
El período feudal de la historia japonesa, dominado por poderosos señores regionales (daimyos) y el gobierno militar de señores de la guerra (shogunes) como el shogunato Ashikaga y el shogunato Tokugawa, se extendió desde 1185 hasta 1868.
Silla conquistó Baekje en 660, y Goguryeo en 668, marcando el principio del período de estados del Norte y del sur, con Silla Unificada en el sur y Balhae, un estado del sucesor a Goguryeo, en el norte.
Comenzando en el 9no siglo, el Reino de Bagan se levantó a la prominencia en Myanmar moderno.
Los pueblos ancestrales y sus predecesores (siglos IX-XIII) construyeron extensos asentamientos permanentes, incluyendo estructuras de piedra que seguirían siendo los edificios más grandes de América del Norte hasta el siglo XIX.
En América del Sur, los siglos XIV y XV vieron el surgimiento del Inca.
La Revolución Científica recibió el ímpetu de la introducción de Johannes Gutenberg a Europa de la imprenta, usando el tipo movible, y de la invención del telescopio y microscopio.
El período moderno tardío continúa ya sea hasta el final de la Segunda Guerra Mundial, en 1945, o hasta el presente.
El período moderno temprano se caracterizó por el surgimiento de la ciencia y por el progreso tecnológico cada vez más rápido, la política cívica secularizada y el estado nación.
Durante el período moderno temprano, Europa era capaz de recobrar su dominio; los historiadores todavía debaten las causas.
Se había desarrollado una economía monetaria avanzada por 1000 CE.
Gozaba de una ventaja tecnológica y tenía el monopolio en la producción de hierro fundido, fuelles de pistón, construcción de puentes colgantes, impresión y la brújula.
Una teoría del ascenso de Europa sostiene que la geografía de Europa jugó un papel importante en su éxito.
Esto le dio a Europa cierto grado de protección contra el peligro de los invasores de Asia Central.
La Edad de Oro del Islam fue terminada por el saco mongol de Bagdad en 1258.
La geografía contribuyó a importantes diferencias geopolíticas.
Por el contrario, Europa casi siempre estaba dividida en una serie de estados en guerra.
Casi todas las civilizaciones agrícolas se han visto fuertemente limitadas por sus entornos.
El avance tecnológico y la riqueza generada por el comercio fueron ampliando gradualmente las posibilidades.
La expansión marítima de Europa, como era de esperar, dada la geografía del continente, fue en gran parte obra de sus estados atlánticos: Portugal, España, Inglaterra, Francia y los Países Bajos.
En África del Norte, el Sultanato de Saadi permaneció como un estado bereber independiente hasta 1659.
La costa swahili disminuyó después de venir bajo el Imperio portugués y más tarde el Imperio omaní.
El reino sudafricano de Zimbabwe dio paso a reinos más pequeños como Mutapa, Butua y Rozvi.
Otras civilizaciones en África avanzaron durante este período.
Japón experimentó su período Azuchi-Momoyama (1568-1603), seguido por el período Edo (1603-1868).
El Sultanato de Johor, centrado en el extremo sur de la península malaya, se convirtió en la potencia comercial dominante en la región.
Rusia hizo incursiones en la costa noroeste de América del Norte, con una primera colonia en la actual Alaska en 1784, y el puesto avanzado de Fort Ross en la actual California en 1812.
La Revolución Industrial comenzó en Gran Bretaña y utilizó nuevos modos de producción, la fábrica, la producción en masa y la mecanización, para fabricar una amplia gama de bienes más rápido y utilizando menos mano de obra de lo que se requería anteriormente.
Después de que los europeos habían logrado influencia y control sobre las Américas, las actividades imperiales se dirigieron a las tierras de Asia y Oceanía.
Los británicos también colonizaron Australia, Nueva Zelanda y Sudáfrica con un gran número de colonos británicos que emigraron a estas colonias.
Dentro de Europa, los desafíos económicos y militares crearon un sistema de estados nacionales, y las agrupaciones etnolinguísticas comenzaron a identificarse como naciones distintivas con aspiraciones de autonomía cultural y política.
Mientras tanto, la contaminación industrial y los daños ambientales, presentes desde el descubrimiento del fuego y el comienzo de la civilización, se aceleraron drásticamente.
Gran parte del resto del mundo fue influenciado por naciones fuertemente europeizadas: Estados Unidos y Japón.
La Primera guerra mundial llevó al colapso de cuatro Imperios – la Austria-Hungría, el Imperio alemán, el Imperio Otomano y el Imperio ruso – y debilitó el Reino Unido y Francia.
Las rivalidades nacionales en curso, exacerbadas por la agitación económica de la Gran Depresión, ayudaron a precipitar la Segunda Guerra Mundial.
La Guerra Fría terminó pacíficamente en 1991 después del Picnic paneuropeo, la posterior caída del Telón de Acero y el Muro de Berlín, y el colapso del Bloque Oriental y el Pacto de Varsovia.
En las primeras décadas de la posguerra, las colonias en Asia y África de los imperios belgas, británicos, holandeses, franceses y otros imperios de Europa occidental ganaron su independencia formal.
La eficacia de la Unión Europea se vio obstaculizada por la inmadurez de sus instituciones económicas y políticas comunes, algo comparable a la insuficiencia de las instituciones de los Estados Unidos en virtud de los Artículos de la Confederación antes de la adopción de la Constitución de los Estados Unidos que entró en vigor en 1789.
En las décadas posteriores a la Segunda Guerra Mundial, estos avances llevaron a los viajes en jet, satélites artificiales con innumerables aplicaciones, incluido el Sistema de Posicionamiento Global (GPS) e Internet.
La competencia mundial por los recursos naturales ha aumentado debido al crecimiento de las poblaciones y la industrialización, especialmente en la India, Chína y Brasil.
Un archivo es una acumulación de registros históricos, en cualquier medio, o la instalación física en la que se encuentran.
Se han definido metafóricamente como "las secreciones de un organismo", y se distinguen de los documentos que se han escrito conscientemente o creado para comunicar un mensaje particular a la posteridad.
Esto significa que los archivos son bastante distintos de las bibliotecas en lo que respecta a sus funciones y organización, aunque las colecciones de archivos a menudo se pueden encontrar dentro de los edificios de la biblioteca.
Los arqueólogos han descubierto archivos de cientos (y a veces miles) de tablillas de arcilla que se remontan al tercer y segundo milenio antes de Cristo en sitios como Ebla, Mari, Amarna, Hattusas, Ugarit y Pylos.
Sin embargo, se han perdido, ya que los documentos escritos en materiales como el papiro y el papel se deterioraron a un ritmo más rápido, a diferencia de sus contrapartes de tabletas de piedra.
Inglaterra después de 1066 desarrolló archivos y métodos de investigación de archivo.
Si bien hay muchos tipos de archivos, el censo más reciente de archiveros en los Estados Unidos identifica cinco tipos principales: académicos, comerciales (con fines de lucro), gubernamentales, sin fines de lucro y otros.
El acceso a las colecciones en estos archivos es generalmente por cita previa solamente; algunos han publicado horas para hacer consultas.
Los ejemplos de archivos comerciales prominentes en los Estados Unidos incluyen Coca-Cola (que también posee el mundo del museo separado de Coca-Cola), Procter and Gamble, Motorola Heritage Services y Archivos y Levi Strauss & Co. Estos archivos corporativos mantienen documentos históricos y artículos relacionados con la historia y la administración de sus compañías.
Los trabajadores en este tipo de archivos pueden tener cualquier combinación de capacitación y títulos, ya sea de antecedentes históricos o bibliotecarios.
En los Estados Unidos, la Administración Nacional de Archivos y Registros (NARA) mantiene instalaciones de archivo centrales en el Distrito de Columbia y College Park, Maryland, con instalaciones regionales distribuidas en todo Estados Unidos.
En el Reino Unido, los Archivos Nacionales (antes conocido como la Oficina del Registro Público) son el archivo del gobierno para Inglaterra y País de Gales.
En conjunto, el volumen total de archivos bajo la supervisión de la Administración de Archivos de Francia es el más grande del mundo.
Arquidiócesis, diócesis y parroquias también tienen archivos en las Iglesias Católica y Anglicana.
A menudo, estas instituciones dependen de la financiación de subvenciones del gobierno, así como de los fondos privados.
Muchos museos guardan archivos para probar la procedencia de sus piezas.
Esta fue una cifra separada del 1,3% que se identificó como autónomo.
La misión del archivo es reunir historias de mujeres que quieren expresarse y quieren que sus historias sean escuchadas.
Los archivos de una organización (como una corporación o gobierno) tienden a contener otros tipos de registros, como archivos administrativos, registros comerciales, memorandos, correspondencia oficial y actas de reuniones.
Muchas de estas donaciones aún no se han catalogado, pero actualmente están en proceso de ser preservadas digitalmente y puestas a disposición del público en línea.
Los socios internacionales para los archivos son la UNESCO y Blue Shield International de conformidad con la Convención de La Haya para la Protección de los Bienes Culturales de 1954 y su 2o Protocolo de 1999.
Page, Morgan M. "Uno de las Bóvedas: Chisme, Acceso, y Trans Historia-Contar."
Un ejemplo de esto es la descripción de Morgan M. Page de difundir la historia transgénero directamente a las personas trans a través de varias redes sociales y plataformas de redes como tumblr, Twitter e Instagram, así como a través de podcast.
Con las opciones disponibles a través del contraarchivo, existe el potencial de "desafiar las concepciones tradicionales de la historia" tal como se perciben dentro de los archivos contemporáneos, lo que crea espacio para narrativas que a menudo no están presentes en muchos materiales de archivo.
Una biografía, o simplemente bio, es una descripción detallada de la vida de una persona.
Las obras biográficas suelen ser de no ficción, pero la ficción también se puede utilizar para retratar la vida de una persona.
Otra colección bien conocida de biografías antiguas es De vita Caesarum ("Sobre las Vidas de Caesars") por Suetonio, escrito sobre d.
Ermitaños, monjes y sacerdotes utilizaron este período histórico para escribir biografías.
Un ejemplo secular significativo de una biografía de este período es la vida de Carlomagno por su cortesano Einhard.
Contenían más datos sociales para un gran segmento de la población que otras obras de ese período.
Antes de finales de la Edad media, las biografías se hicieron menos orientadas por la iglesia en Europa ya que las biografías de reyes, caballeros y tiranos comenzaron a aparecer.
Después de Malory, el nuevo énfasis en el humanismo durante el Renacimiento promovió un enfoque en temas seculares, como artistas y poetas, y alentó a escribir en lengua vernácula.
Otros dos desarrollos son dignos de mención: el desarrollo de la imprenta en el siglo XV y el aumento gradual de la alfabetización.
Influyente en la configuración de las concepciones populares de los piratas, A General History of the Pyrates (1724), de Charles Johnson, es la fuente principal de las biografías de muchos piratas conocidos.
Carlyle afirmó que la vida de los grandes seres humanos era esencial para entender la sociedad y sus instituciones.
El trabajo de Boswell era único en su nivel de investigación, que implicó el estudio de archivo, cuentas del testigo ocular y entrevistas, su narrativa robusta y atractiva y su representación honesta de todos los aspectos de la vida de Johnson y carácter – una fórmula que sirve de la base de la literatura biográfica hasta este día.
Sin embargo, el número de biografías impresas experimentó un rápido crecimiento, gracias a la expansión del público lector.
Las publicaciones periódicas comenzaron a publicar una secuencia de bocetos biográficos.
Las biografías sociológicas concebían las acciones de sus sujetos como resultado del entorno, y tendían a restar importancia a la individualidad.
El concepto convencional de héroes y las narrativas del éxito desaparecieron en la obsesión por las exploraciones psicológicas de la personalidad.
Hasta este punto, como Strachey comentó en el prefacio, las biografías victorianas habían sido "tan familiares como el cortejo del enterrador", y llevaban el mismo aire de "barbarie lenta y fúnebre".
El libro alcanzó fama mundial debido a su estilo irreverente e ingenioso, su naturaleza concisa y objetivamente precisa, y su prosa artística.
Robert Graves (I, Claudio, 1934) se destacó entre los que siguieron el modelo de Strachey de "biografías de desacreditación".
En la Primera Guerra Mundial, las reimpresiones baratas de tapa dura se habían vuelto populares.
Junto con las películas biográficas documentales, Hollywood produjo numerosas películas comerciales basadas en las vidas de personas famosas.
A diferencia de los libros y las películas, a menudo no cuentan una narrativa cronológica: en cambio, son archivos de muchos elementos de medios discretos relacionados con una persona individual, incluidos videoclips, fotografías y artículos de texto.
Las técnicas generales de "escritura de vida" son un tema de estudio académico.
La información puede provenir de "historia oral, narrativa personal, biografía y autobiografía" o "diarios, cartas, memorandos y otros materiales".
Los castillos de estilo europeo se originaron en los siglos IX y X, después de la caída del Imperio carolingio, lo que provocó que su territorio se dividiera entre señores y príncipes individuales.
Los castillos urbanos se utilizaron para controlar a la población local y las rutas de viaje importantes, y los castillos rurales a menudo se situaban cerca de características que eran parte integral de la vida en la comunidad, como molinos, tierra fértil o una fuente de agua.
A finales del siglo XII y principios del XIII, surgió un enfoque científico para la defensa del castillo.
Estos cambios en la defensa se han atribuido a una mezcla de la tecnología del castillo de las Cruzadas, como la fortificación concéntrica y la inspiración de defensas más tempranas, como fortalezas romanas.
Aunque la pólvora se introdujo en Europa en el siglo XIV, no afectó significativamente la construcción del castillo hasta el siglo XV, cuando la artillería se volvió lo suficientemente poderosa como para romper los muros de piedra.
El feudalismo era el vínculo entre un señor y su vasallo donde, a cambio del servicio militar y la expectativa de lealtad, el señor concedería la tierra del vasallo.
Los castillos servían para una variedad de propósitos, los más importantes de los cuales eran militares, administrativos y domésticos.
Cuando Guillermo el Conquistador avanzó a través de Inglaterra, fortificó posiciones clave para asegurar la tierra que había tomado.
Un castillo podía actuar como una fortaleza y prisión, pero también era un lugar donde un caballero o señor podía entretener a sus compañeros.
En diferentes áreas del mundo, las estructuras análogas compartían características de fortificación y otras características definitorias asociadas con el concepto de castillo, aunque se originaron en diferentes períodos y circunstancias y experimentaron diferentes evoluciones e influencias.
Antes del 16to siglo, cuando las culturas japonesas y europeas se encontraron, la fortificación en Europa se había movido más allá de castillos y había confiado en innovaciones como la traza italiana italiana y fortalezas de la estrella.
La excavación de la tierra para hacer el montículo dejó una zanja alrededor de la mota, llamada foso (que podría ser húmedo o seco).
Era una característica común de los castillos, y la mayoría tenía al menos uno.
El agua era suministrada por un pozo o cisterna.
Aunque a menudo se asocia con el tipo de castillo motte-and-bailey, los baileys también se pueden encontrar como estructuras defensivas independientes.
Keep no era un término usado en la época medieval – el término se aplicó desde el siglo XVI en adelante – en su lugar “donjon” se usaba para referirse a grandes torres, o turris en latín.
Aunque a menudo la parte más fuerte de un castillo y un último lugar de refugio si las defensas exteriores cayeran, la fortaleza no se dejó vacía en caso de ataque, pero fue usada como una residencia por el señor que poseyó el castillo, o sus invitados o representantes.
Las pasarelas a lo largo de la parte superior de los muros cortina permitieron a los defensores llover misiles sobre los enemigos de abajo, y las almenas les dieron más protección.
El frente de la puerta era un punto ciego y para superar esto, se agregaron torres sobresalientes a cada lado de la puerta en un estilo similar al desarrollado por los romanos.
El paso a través de la puerta de entrada se alargó para aumentar la cantidad de tiempo que un asaltante tenía que pasar bajo fuego en un espacio confinado e incapaz de tomar represalias.
Lo más probable es que se usaran para dejar caer objetos sobre los atacantes o para permitir que se vertiera agua en los incendios para extinguirlos.
Se podría agregar una abertura horizontal más pequeña para darle a un arquero una mejor vista para apuntar.
Las primeras fortificaciones se originaron en el Creciente Fértil, el Valle del Indo, Egipto y China, donde los asentamientos estaban protegidos por grandes muros.
Muchos movimientos de tierra sobreviven hoy en día, junto con la evidencia de empalizadas para acompañar las zanjas.
Aunque primitivos, a menudo eran efectivos, y solo fueron superados por el uso extensivo de máquinas de asedio y otras técnicas de guerra de asedio, como en la Batalla de Alesia.
Las discusiones han atribuido típicamente la subida del castillo a una reacción a ataques por magiares, musulmanes y Vikingos y una necesidad de la defensa privada.
Algunas altas concentraciones de castillos se producen en lugares seguros, mientras que algunas regiones fronterizas tenían relativamente pocos castillos.
La construcción de la sala en piedra no necesariamente la hacía inmune al fuego, ya que todavía tenía ventanas y una puerta de madera.
Los castillos no solo eran sitios defensivos, sino que también aumentaban el control de un señor sobre sus tierras.
En 864, el rey de Francia Occidental, Carlos el Calvo, prohibió la construcción de Castilla sin su permiso y ordenó que todos fueran destruidos.
Suiza es un caso extremo de que no hay control estatal sobre quién construyó castillos, y como resultado había 4.000 en el país.
En 950 Provenza fue el hogar de 12 castillos, por 1000 esta cifra había aumentado a 30, y en 1030 era más de 100.
A principios del siglo XI, el motte and keep, un montículo artificial con una empalizada y una torre en la parte superior, era la forma más común de castillo en Europa, en todas partes excepto en Escandinavia.
Aunque la construcción de piedra se haría más tarde común en otra parte, a partir del 11er siglo adelante era el material de construcción primario para castillos cristianos en España, mientras al mismo tiempo la madera todavía era el material de construcción dominante en Europa noroeste.
Antes de los castillos del siglo 12 eran tan poco comunes en Dinamarca como lo habían sido en Inglaterra antes de la conquista normanda.
Su decoración emulaba la arquitectura románica, y a veces incorporaba ventanas dobles similares a las que se encuentran en los campanarios de las iglesias.
Aunque fueron reemplazados por sus sucesores de piedra, los castillos de madera y movimiento de tierra no fueron inútiles.
Hasta finales del 12do siglo los castillos generalmente tenían pocas torres; una entrada con pocas características defensivas como arrowslits o un portcullis; un gran guarda o donjon, por lo general cuadrado y sin arrowslits; y la forma habría sido dictada por la disposición de la tierra (el resultado era a menudo estructuras irregulares o curvilíneas).
Las torres habrían sobresalido de las paredes y presentaban luces de flechas en cada nivel para permitir que los arqueros apuntaran a cualquiera que se acercara o al muro cortina.
Donde sí existían, ya no eran cuadrados sino poligonales o cilíndricos.
Probablemente desarrolladas en el siglo XII, las torres proporcionaron fuego flanqueante.
Parecía que los cruzados habían aprendido mucho sobre la fortificación de sus conflictos con los sarracenos y la exposición a la arquitectura bizantina.
Las leyendas fueron desacreditadas, y en el caso de Santiago de San Jorge se demostró que provenía de Saint-Georges-d'Espéranche, en Francia.
Los constructores del castillo de Europa Occidental eran conscientes de e bajo la influencia del diseño romano; los fuertes costeros romanos tardíos en la "Orilla sajona" inglesa se reutilizaron y en España la pared alrededor de la ciudad de Ávila imitó la arquitectura romana cuando se construyó en 1091.
Un ejemplo de este enfoque es Kerak.
Los castillos que fundaron para asegurar sus adquisiciones fueron diseñados principalmente por maestros albañiles sirios.
Mientras que los castillos se utilizaron para mantener un sitio y controlar el movimiento de los ejércitos, en Tierra Santa algunas posiciones estratégicas clave quedaron sin fortificar.
El diseño varió no sólo entre órdenes, pero entre castillos individuales, aunque fuera común para aquellos fundados en este período tener defensas concéntricas.
Si los asaltantes pasaran la primera línea de defensa, quedarían atrapados en el campo de batalla entre las paredes interior y exterior y tendrían que asaltar la segunda pared.
Por ejemplo, era común en los castillos cruzados tener la puerta principal en el lado de una torre y que hubiera dos vueltas en el pasillo, alargando el tiempo que le tomaba a alguien llegar al recinto exterior.
Aunque había cientos de castillos de madera en Prusia y Livonia, el uso de ladrillos y morteros era desconocido en la región antes de los cruzados.
Las hendiduras de flecha no comprometieron la fuerza del muro, pero no fue hasta el programa de construcción del castillo de Eduardo I que fueron ampliamente adoptados en Europa.
Aunque las maquilaciones tenían el mismo propósito que las galerías de madera, probablemente fueron una invención oriental en lugar de una evolución de la forma de madera.
El conflicto y la interacción entre los dos grupos llevaron a un intercambio de ideas arquitectónicas, y los cristianos españoles adoptaron el uso de torres separadas.
El historiador francés Francois Gebelin escribió: "El gran renacimiento en la arquitectura militar fue conducido, como uno naturalmente esperaría, por los reyes poderosos y príncipes del tiempo; por los hijos de Guillermo el Conquistador y sus descendientes, los Plantagenets, cuando se hicieron duques de Normandía.
Los nuevos castillos eran generalmente de una construcción más ligera que estructuras más tempranas y presentaron pocas innovaciones, aunque los sitios fuertes todavía se crearan como ese de Raglan en País de Gales.
Estas armas eran demasiado pesadas para que un hombre las llevara y disparara, pero si apoyaba el extremo del trasero y apoyaba el hocico en el borde del puerto de armas, podía disparar el arma.
Esta adaptación se encuentra en toda Europa, y aunque la madera rara vez sobrevive, hay un ejemplo intacto en Castle Doornenburg en los Países Bajos.
Otros tipos de puerto, aunque menos comunes, eran las ranuras horizontales, que permitían solo el movimiento lateral, y las grandes aberturas cuadradas, que permitían un mayor movimiento.
El jamón es un ejemplo de la tendencia de los nuevos castillos a prescindir de características anteriores, como las machicolaciones, las torres altas y las almenas.
En un esfuerzo por hacerlos más efectivos, las armas se hicieron cada vez más grandes, aunque esto obstaculizó su capacidad para llegar a castillos remotos.
Si bien esto era suficiente para nuevos castillos, las estructuras preexistentes tenían que encontrar una manera de hacer frente a ser golpeadas por cañones.
Una solución a esto fue tirar hacia abajo la parte superior de una torre y llenar la parte inferior con los escombros para proporcionar una superficie desde la que disparar las armas.
A partir de esta estrella evolucionada fuertes, también conocidos como traza italiana.
La segunda opción resultó ser más popular, ya que se hizo evidente que no tenía mucho sentido tratar de hacer que el sitio fuera genuinamente defendible frente al cañón.
Algunos castillos verdaderos fueron construidos en las Américas por las colonias españolas y francesas.
Entre otras estructuras defensivas (incluso fortalezas y ciudadelas), los castillos también se construyeron en Nueva Francia hacia el final del 17mo siglo.
La casa solariega y los establos estaban dentro de un baile fortificado, con una torreta redonda alta en cada esquina.
Aunque la construcción del castillo se desvaneció hacia finales del siglo XVI, los castillos no necesariamente quedaron fuera de uso.
En otros casos todavía tenían un papel en la defensa.
En conflictos posteriores, como la Guerra Civil Inglesa (1641-1651), muchos castillos fueron refortificados, aunque posteriormente fueron menospreciados para evitar que se volvieran a utilizar.
El renacimiento o los castillos simulados se hicieron populares como una manifestación de un interés romántico en la Edad Media y la caballería, y como parte del renacimiento gótico más amplio en la arquitectura.
Esto se debía a que ser fiel al diseño medieval habría dejado las casas frías y oscuras para los estándares contemporáneos.
Las locuras eran similares, aunque diferían de las ruinas artificiales en que no formaban parte de un paisaje planificado, sino que parecían no tener ninguna razón para ser construidas.
Un castillo con murallas de tierra, una mota, defensas de madera y edificios podrían haber sido construidos por una mano de obra no calificada.
El costo de construir un castillo varió según factores como su complejidad y los costos de transporte del material.
En el medio había castillos como Orford, que fue construido a finales del siglo XII para el Reino Unido - 1.400, y en el extremo superior estaban los de Dover, que costaron alrededor del Reino Unido -7.000 entre 1181 y 1191.
El costo de un gran castillo construido a lo largo de este tiempo (desde el Reino Unido -1,000 hasta el Reino Unido -10.000) tomaría los ingresos de varias casas señoriales, afectando severamente las finanzas de un señor.
Máquinas e invenciones medievales, como la grúa de rueda de rodadura, se hicieron indispensables durante la construcción, y las técnicas de construcción de andamios de madera fueron mejoradas desde la Antig.
Muchos países tenían castillos de madera y piedra, sin embargo, Dinamarca tenía pocas canteras y, como resultado, la mayoría de sus castillos son de tierra y madera, o más tarde fueron construidos con ladrillos.
Por ejemplo, cuando el castillo de Tattershall fue construido entre 1430 y 1450, había un montón de piedra disponible cerca, pero el propietario, Lord Cromwell, optó por usar ladrillo.
Confiaba en el apoyo de los que estaban debajo de él, ya que sin el apoyo de sus inquilinos más poderosos, un señor podía esperar que su poder se viera socavado.
Esto se aplicaba especialmente a la realeza, que a veces poseía tierras en diferentes países.
Los hogares reales tomaron esencialmente la misma forma que los hogares baroniales, aunque en una escala mucho más grande y las posiciones eran más prestigiosas.
Como centros sociales, los castillos eran lugares importantes para la exhibición.
Los castillos se han comparado con catedrales como objetos del orgullo arquitectónico, y algunos castillos incorporaron jardines como rasgos ornamentales.
El amor cortés era la erotización del amor entre la nobleza.
La leyenda de Tristán e Isolda es un ejemplo de historias de amor cortesano contadas en la Edad Media.
El propósito del matrimonio entre las élites medievales era asegurar la tierra.
Esto se deriva de la imagen del castillo como una institución marcial, pero la mayoría de los castillos en Inglaterra, Francia, Irlanda y Escocia nunca estuvieron involucrados en conflictos o asedios, por lo que la vida doméstica es una faceta descuidada.
Por ejemplo muchos castillos se localizan cerca de caminos romanos, que permanecieron rutas de transporte importantes en la Edad media, o podrían llevar a la alteración o creación de nuevos sistemas del camino en el área.
Los castillos urbanos fueron particularmente importantes para controlar los centros de población y producción, especialmente con una fuerza invasora, por ejemplo, después de la conquista normanda de Inglaterra en el siglo XI, la mayoría de los castillos reales se construyeron en o cerca de las ciudades.
Los castillos rurales a menudo se asociaban con molinos y sistemas de campo debido a su papel en la gestión de la finca del señor, que les dio una mayor influencia sobre los recursos.
No solo eran prácticos en el sentido de que aseguraban un suministro de agua y pescado fresco, sino que eran un símbolo de estatus ya que eran caros de construir y mantener.
Los beneficios de la construcción de castillos en los asentamientos no se limitaban a Europa.
Los asentamientos también podrían crecer naturalmente alrededor de un castillo, en lugar de ser planeados, debido a los beneficios de la proximidad a un centro económico en un paisaje rural y la seguridad dada por las defensas.
Por lo general, se ubicaban cerca de las defensas de la ciudad existentes, como las murallas romanas, aunque esto a veces resultaba en la demolición de estructuras que ocupaban el sitio deseado.
Cuando los normandos invadieron Irlanda, Escocia y País de Gales en los 11ros y 12dos siglos, el establecimiento en aquellos países era predominantemente no urbano, y la fundación de ciudades a menudo se unió con la creación de un castillo.
Esto significaba una estrecha relación entre los señores feudales y la Iglesia, una de las instituciones más importantes de la sociedad medieval.
Otro ejemplo es el del castillo Bodiam del siglo XIV, también en Inglaterra; aunque parece ser un castillo avanzado, está en un sitio de poca importancia estratégica, y el foso era poco profundo y más probable que tuviera la intención de hacer que el sitio parezca impresionante que como una defensa contra la minería.
Las guarniciones eran caras y, como resultado, a menudo pequeñas, a menos que el castillo fuera importante.
En 1403, una fuerza de 37 arqueros defendió con éxito el castillo de Caernarfon contra dos asaltos de los aliados de Owain Glynd?r durante un largo asedio, lo que demuestra que una pequeña fuerza podría ser efectiva.
Bajo él habría habido caballeros que, en beneficio de su entrenamiento militar, habrían actuado como un tipo de clase de oficial.
Era más eficiente matar de hambre a la guarnición que asaltarla, particularmente para los sitios más fuertemente defendidos.
Un largo asedio podría ralentizar al ejército, permitiendo que venga ayuda o que el enemigo prepare una fuerza más grande para más tarde.
Si se ve obligado a asaltar un castillo, había muchas opciones disponibles para los atacantes.
El trebuchet, que probablemente evolucionó a partir de la petraria en el siglo XIII, fue el arma de asedio más efectiva antes del desarrollo de los cañones.
Las ballestas o springalds eran máquinas de asedio que funcionaban según los mismos principios que las ballestas.
Eran más comúnmente utilizados contra la guarnición en lugar de los edificios de un castillo.
Se cavaría una mina que condujera a la pared y una vez que se alcanzara el objetivo, los soportes de madera que evitaban que el túnel se derrumbara se quemarían.
Una contra-mina podría ser excavada hacia el túnel de los sitiadores; suponiendo que los dos convergieran, esto resultaría en un combate cuerpo a cuerpo subterráneo.
Se usaban para forzar la apertura de las puertas del castillo, aunque a veces se usaban contra muros con menos efecto.
Una opción más segura para los que asaltaban un castillo era usar una torre de asedio, a veces llamada campanario.
Las fincas del reino, o tres fincas, eran las amplias órdenes de jerarquía social utilizadas en la cristiandad (Europa cristiana) desde la Edad Media hasta la Europa moderna temprana.
La monarquía incluía al rey y la reina, mientras que el sistema estaba formado por el clero (el Primer Estado), los nobles (Segundo Estado), los campesinos y la burguesía (Tercer Estado).
En Inglaterra, un sistema de dos estados evolucionó que combinó nobleza y clero en una finca señorial con "bienes comunes" como la segunda finca.
En Escocia, los Tres Estados eran el Clero (Primer Estado), la Nobleza (Segundo Estado) y los Comisionados de Comarca, o "burgueses" (Tercer Estado), que representaban a la burguesía, la clase media y la clase baja.
Como el clero no podía casarse, tal movilidad teóricamente se limitó a una generación.
Huizinga El menguante de la Edad Media (1919, 1924:47).
Los plebeyos eran universalmente considerados el orden más bajo.
En muchas regiones y reinos también existían grupos de población nacidos fuera de estas fincas residentes específicamente definidas.
La transformación económica y política del campo en el período fue llenada por un crecimiento grande de población, producción agrícola, innovaciones tecnológicas y centros urbanos; los movimientos de reforma y renovación intentaron afilar la distinción entre estado clerical y laico, y el poder, reconocido por la iglesia también tenía su efecto.
El segundo orden, los que luchan, era el rango de los políticamente poderosos, ambiciosos y peligrosos.
Además, los Estados Primero y Segundo se basaron en el trabajo del Tercer Estado, lo que hizo que el estatus inferior de este último fuera aún más evidente.
La mayoría nacieron dentro de este grupo y también murieron como parte de él.
En el mayo de 1776, el ministro de finanzas Turgot se despidió, después de no poder decretar reformas.
Cuando no pudo persuadirlos para que aprobaran su "programa ideal", Luis XVI trató de disolver los Estados Generales, pero el Tercer Estado resistió por su derecho a la representación.
Como el Parlamento de Escocia era unicameral, todos los miembros se sentaron en la misma cámara, a diferencia de la Cámara de los Lores inglesa separada y Cámara de los Comunes.
Como en Inglaterra, el Parlamento de Irlanda evolucionó del Magnum Concilium "gran consejo" convocado por el gobernador principal de Irlanda, asistida por el consejo (curia regis), magnates (señores feudales), y prelados (obispos y abades).
En 1297, los condados fueron representados primero por caballeros elegidos del condado (los sheriffs los habían representado antes).
Cada uno era hombre libre, y tenía derechos y responsabilidades específicos, y el derecho de enviar representantes al Riksdag de los Estados.
Antes del 18vo siglo, el Rey tenía el derecho de echar un voto decisivo si las Fincas se dividieron por partes iguales.
Sin embargo, después de la Dieta de Porvoo, la Dieta de Finlandia se convocó de nuevo sólo en 1863.
Alrededor de 1400, se introdujeron las cartas de patente, en 1561 se agregaron las filas del Conde y el Barón, y en 1625 la Casa de la Nobleza fue codificada como el Primer Estado de la tierra.
Los jefes de las casas nobles eran miembros hereditarios de la asamblea de nobles.
Esto dio lugar a una gran influencia política para la nobleza superior.
En siglos posteriores, la finca incluyó a profesores de universidades y ciertas escuelas estatales.
El comercio sólo estaba permitido en las ciudades cuando la ideología mercantilista había conseguido la ventaja, y los burgueses tenían el derecho exclusivo de llevar a cabo el comercio en el marco de los gremios.
Para que un establecimiento se hiciera una ciudad, un estatuto real que concede el derecho de mercado se requirió, y el comercio exterior requirió derechos del puerto de grapa diplomados real.
Dado que la mayoría de la población eran familias de agricultores independientes hasta el siglo XIX, no siervos ni villanos, hay una notable diferencia en la tradición en comparación con otros países europeos.
Sus representantes a la Dieta se eligieron indirectamente: cada municipalidad envió a electores para elegir al representante de un distrito electoral.
No tenían derechos políticos y no podían votar.
En Suecia, el Riksdag de los Estados existió hasta que fue reemplazado por un Riksdag bicameral en 1866, que dio derechos políticos a cualquier persona con ciertos ingresos o propiedad.
En Finlandia, esta división legal existió hasta 1906, todavía basándose en la constitución sueca de 1772.
Además, los trabajadores industriales que vivían en la ciudad no estaban representados por el sistema de cuatro estados.
Más tarde, en los siglos XV y XVI, Bruselas se convirtió en el lugar donde se reunían los Estados Generales.
A consecuencia de la Unión de Utrecht en 1579 y los acontecimientos que siguieron después, los estados generales declararon que ya no obedecieron al rey Philip II de España, que también era el señor supremo de los Países Bajos.
Fue el nivel de gobierno donde se trataron todas las cosas que preocuparon a las siete provincias que se convirtieron en parte de la República de los Países Bajos Unidos.
En los Países Bajos del sur, las últimas reuniones de los Estados Generales leales a los Habsburgo tuvieron lugar en los Estados Generales de 1600 y los Estados Generales de 1632.
Ya no consistía en representantes de los Estados, y mucho menos de los Estados: todos los hombres eran considerados iguales en virtud de la Constitución de 1798.
En 1815, cuando los Países Bajos se unieron con Bélgica y Luxemburgo, los Estados Generales se dividieron en dos cámaras: la Primera Cámara y la Segunda Cámara.
A partir de 1848, la Constitución holandesa dispone que los miembros de la Segunda Cámara sean elegidos por el pueblo (al principio sólo por una parte limitada de la población masculina; el sufragio masculino y femenino universal existe desde 1919), mientras que los miembros de la Primera Cámara son elegidos por los miembros de los Estados Provinciales.
El clero estaba representado por los príncipes-obispos independientes, príncipes-arzobispos y príncipes-abades de los muchos monasterios.
Muchos pueblos cuyos territorios dentro del Sacro Imperio Romano habían sido independientes durante siglos no tenían representantes en la Dieta Imperial, y esto incluía a los Caballeros Imperiales y las aldeas independientes.
Las cuatro fincas principales eran: nobleza (dvoryanstvo), clero, moradores rurales y moradores urbanos, con una estratificación más detallada en ello.
La burguesía, en su sentido original, está íntimamente ligada a la existencia de las ciudades, reconocidas como tales por sus estatutos urbanos (por ejemplo, estatutos municipales, privilegios municipales, leyes municipales alemanas), por lo que no había burguesía aparte de la ciudadanía de las ciudades.
Históricamente, la palabra burguesa francesa medieval denotaba a los habitantes de los burgos (ciudades amuralladas), los artesanos, artesanos, comerciantes y otros, que constituían "la burguesía".
Los gremios surgieron cuando los hombres de negocios individuales (como artesanos, artesanos y comerciantes) entraron en conflicto con sus terratenientes feudales que buscaban rentas que exigían mayores rentas de las acordadas previamente.
Tienden a pertenecer a una familia que ha sido burguesa durante tres o más generaciones.
Los nombres de estas familias son generalmente conocidos en la ciudad donde residen, y sus antepasados a menudo han contribuido a la historia de la región.
Sin embargo, estas personas viven lujosamente, disfrutando de la compañía de los grandes artistas de la época.
En la lengua francesa, el término burguesía casi designa una casta por sí misma, aunque la movilidad social en este grupo socioeconómico es posible.
Hitler desconfiaba del capitalismo por ser poco confiable debido a su egoísmo, y prefería una economía dirigida por el Estado que estuviera subordinada a los intereses del pueblo.
Hitler también dijo que la burguesía empresarial "no sabe nada más que sus ganancias".
La utilidad de estas cosas era inherente a sus funciones prácticas.
Belle de Jour (Belleza del día, 1967) cuenta la historia de una esposa burguesa que se aburre de su matrimonio y decide prostituirse.
En Europa, el título de Emperador se ha usado desde la Edad media, considerada en aquellos tiempos igual o casi igual en la dignidad a ese del Papa debido a la posición de éste como la cabeza visible de la iglesia y el líder espiritual de la parte católica de Europa Occidental.
En la medida en que hay una definición estricta de emperador, es que un emperador no tiene relaciones que impliquen la superioridad de cualquier otro gobernante y típicamente gobierna sobre más de una nación.
Su estado fue oficialmente reconocido por el Emperador romano Santo en 1514, aunque no oficialmente usado por los monarcas rusos hasta 1547.
Tales títulos prerromanos como Gran Rey o Rey de Reyes, usados por los Reyes de Persia y otros, a menudo se consideran como el equivalente.
El Imperio se identificó en cambio con posesiones territoriales enormes, más bien que el título de su jefe a mediados del 18vo siglo.
Los antiguos romanos aborrecían el nombre de Rex ("rey"), y era fundamental para el orden político mantener las formas y pretensiones del gobierno republicano.
Augusto, considerado el primer emperador romano, estableció su hegemonía recogiendo sobre sí cargos, títulos y honores de la Roma republicana que tradicionalmente se habían distribuido a diferentes personas, concentrando lo que se había distribuido el poder en un solo hombre.
Sin embargo, fue la descripción informal de Imperator ("comandante") que se convirtió en el título cada vez más favorecido por sus sucesores.
Este es uno de los títulos más duraderos: César y sus transliteraciones aparecieron en cada año desde la época de César Augusto hasta la eliminación del zar Simeón II de Bulgaria del trono en 1946.
Las excepciones incluyen el título de la Historia de Augustan, una colección semihistórica de las biografías de los Emperadores del 2do y 3er siglo.
Sin embargo, a pocos se les concedió el título, y ciertamente no era una regla que todas las esposas de los emperadores reinantes lo recibieran.
En la república tardía, como en los primeros años de la nueva monarquía, Imperator era un título concedido a generales romanos por sus tropas y el Senado romano después de una gran victoria, aproximadamente comparable al mariscal de campo (jefe o comandante del ejército entero).
La dinastía Nervan-Antoniana, que gobernó durante la mayor parte del siglo II, estabilizó el Imperio.
Tres tentativas secesionistas efímeras tenían sus propios emperadores: el Imperio galo, el Imperio británico y el Imperio de Palmyrene aunque éste usara rex más con regularidad.
En un momento dado, había hasta cinco partícipes del imperio (ver: Tetrarquía).
La ciudad es más comúnmente llamada Constantinopla y hoy en día se llama Estambul).
Estos emperadores romanos "bizantinos" posteriores completaron la transición de la idea del emperador como un funcionario semi-republicano al emperador como un monarca absoluto.
Los emperadores del período Bizantinos también usaron la palabra griega "autokrator", significando "uno que se gobierna", o "monarch", que fue usado tradicionalmente por escritores griegos para traducir al dictador latino.
De hecho, ninguno de estos (y otros) epítetos y títulos adicionales habían sido descartados por completo.
Después de tragedia del saqueo horrible de la ciudad, los conquistadores declararon un nuevo "Imperio de Rumania", conocido por historiadores como el Imperio latino de Constantinople, instalando a Balduino IX, el Conde de Flandes, como el Emperador.
Desde la época de Otón el Grande en adelante, gran parte del antiguo reino carolingio de Francia Oriental se convirtió en el Sacro Imperio Romano.
Este rey menor llevaba entonces el título de rey romano (Rey de los romanos).
El emperador del Sacro Imperio Romano Germánico fue considerado el primero entre los que estaban en el poder.
La geografía a menudo se define en términos de dos ramas: geografía humana y geografía física.
Tradicionalmente, la geografía se ha asociado con la cartografía y los topónimos.
Debido a que el espacio y el lugar afectan a una variedad de temas, como la economía, la salud, el clima, las plantas y los animales, la geografía es altamente interdisciplinaria.
El primero se centra en gran medida en el entorno construido y en cómo los humanos crean, ven, gestionan e influyen en el espacio.
Requiere una comprensión de los aspectos tradicionales de la geografía física y humana, como las formas en que las sociedades humanas conceptualizan el medio ambiente.
El estudio de sistemas más grandes que la Tierra misma por lo general forma la parte de Astronomía o Cosmología.
En la década de 1950, el movimiento científico regional dirigido por Walter Isard surgió para proporcionar una base más cuantitativa y analítica a las preguntas geográficas, en contraste con las tendencias descriptivas de los programas de geografía tradicionales.
La cartografía ha pasado de una colección de técnicas de redacción a una ciencia real.
Además de todas las otras subdisciplinas de la geografía, los especialistas en SIG deben comprender la informática y los sistemas de bases de datos.
La geoestadística se utiliza ampliamente en una variedad de campos, incluyendo hidrología, geología, exploración de petróleo, análisis del clima, planificación urbana, logística y epidemiología.
El mapa reconstruido por Eckhard Unger muestra Babilonia en el Éufrates, rodeada por una masa de tierra circular que muestra Asiria, Urartu y varias ciudades, a su vez rodeadas por un "río amargo" (Oceanus), con siete islas dispuestas a su alrededor para formar una estrella de siete puntas.
En contraste con Imago Mundi, un mapa del mundo babilonio más temprano que se remonta al 9no siglo A.C. representó Babylon como estando más al norte del centro del mundo, aunque no sea seguro lo que ese centro se supusiera representar.
A Thales también se le atribuye la predicción de eclipses.
Existe cierto debate sobre quién fue la primera persona en afirmar que la Tierra es de forma esférica, y el crédito va a Parménides o Pitágoras.
Una de las primeras estimaciones del radio de la Tierra fue hecha por Eratóstenes.
Los meridianos se subdividieron en 360 °, con cada grado subdividido en 60 (minutos).
Extendió el trabajo de Hipparchus, usando un sistema de la rejilla en sus mapas y adoptando una longitud de 56,5 millas para un grado.
Durante la Edad Media, la caída del Imperio Romano llevó a un cambio en la evolución de la geografía de Europa al mundo islámico.
Además, los eruditos islámicos tradujeron e interpretaron las obras anteriores de los romanos y los griegos y establecieron la Casa de la Sabiduría en Bagdad para este propósito.
Abu Rayhan Biruni (976-1048) describió por primera vez una proyección equidistante equi-azimutal polar de la esfera celeste.
También desarrolló técnicas similares cuando se trataba de medir las alturas de las montañas, las profundidades de los valles y la extensión del horizonte.
El problema que enfrentan tanto los exploradores como los geógrafos fue encontrar la latitud y longitud de una ubicación geográfica.
El 18vo y los 19nos siglos eran los tiempos cuando la geografía se hizo reconocida como una disciplina académica discreta y se hizo la parte de un plan de estudios universitario típico en Europa (especialmente París y Berlín).
En los últimos dos siglos, los avances en la tecnología con computadoras han llevado al desarrollo de la geomática y las nuevas prácticas, como la observación participante y la geoestadística, a incorporarse a la cartera de herramientas de la geografía.
Arnold Henry Guyot (1807-1884) – observó la estructura de los glaciares y la comprensión avanzada en el movimiento de los glaciares, especialmente en el flujo rápido de hielo.
William Morris Davis (1850-1934): padre de la geografía estadounidense y desarrollador del ciclo de erosión.
Ellen Churchill Semple (1863-1932) – primera mujer presidenta de la Asociación de Geógrafos Americanos.
Walter Christaller (1893-1969) - geógrafo humano e inventor de la teoría del lugar central.
David Harvey (nacido en 1935) - geógrafo marxista y autor de teorías sobre geografía espacial y urbana, ganador del Premio Vautrin Lud.
En algunos casos, se hace una distinción entre el capital oficial (constitucional) y la sede del gobierno, que está en otro lugar.
Ejemplos son la antigua Babilonia, Abbasid Bagdad, la antigua Atenas, Roma, Bratislava, Budapest, Constantinopla, Chang'an, el antiguo Cusco, Kiev, Madrid, París, Podgorica, Londres, Pekín, Praga, Tallin, Tokio, Lisboa, Riga, Vilnius y Varsovia.
En algunos países, la capital se ha cambiado por razones geopolíticas; la primera ciudad de Finlandia, Turku, que había servido de la capital del país desde la Edad media bajo el gobierno sueco, perdió su derecho durante el Magnífico Ducado de Finlandia en 1812, cuando Helsinki se hizo la capital corriente de Finlandia por el Imperio ruso.
En Canadá, hay una capital federal, mientras que las diez provincias y tres territorios tienen cada uno capitales.
En Australia, el término "ciudades capitales" se usa regularmente para referirse a esas seis capitales estatales más la capital federal Canberra, y Darwin, la capital del Territorio del Norte.
A diferencia de las federaciones, generalmente no hay una capital nacional separada, sino que la capital de una nación constituyente también será la capital del estado en general, como Londres, que es la capital de Inglaterra y del Reino Unido.
Las capitales nacionales de Alemania y Rusia (el Stadtstaat de Berlín y la ciudad federal de Moscú) también son estados constituyentes de ambos países por derecho propio.
Frankfort, Kentucky, a medio camino entre Louisville y Lexington.
Tallahassee, Florida, elegido como el punto medio entre Pensacola y St. Augustine, Florida - entonces las dos ciudades más grandes de Florida.
Los cambios en el régimen político de una nación a veces resultan en la designación de una nueva capital.
Cuando las Islas Canarias se convirtieron en una comunidad autónoma en 1982, Santa Cruz de Tenerife y Las Palmas de Gran Canaria recibieron el estatus de capital.
Estonia: el Tribunal Supremo y el Ministerio de Educación e Investigación se encuentran en Tartu.
En caso de emergencia, la sede de los poderes constitucionales puede transferirse a otra ciudad, para que las Cámaras del Parlamento se sienten en el mismo lugar que el Presidente y el Gabinete.
Toda la maquinaria estatal cambia de una ciudad a otra cada seis meses.
Dharamshala, que también es la sede de la Administración Central Tibetana, es la segunda capital de invierno del estado.
La ciudad en sí se administra como un territorio de la Unión.
Uttarakhand: Dehradun es la capital administrativa y legislativa, mientras que el tribunal superior se encuentra en Nainital.
Su construcción comenzó en 1960 y se completó en 1966.
El palacio presidencial (Palacio Malacanang) y la Corte Suprema se localizan dentro de la capital pero las dos casas del Congreso se localizan en barrios residenciales separados.
Sri Lanka: Sri Jayawardenepura Kotte se designa la capital administrativa y la ubicación del parlamento, mientras la antigua capital, Colombo, se designa ahora como la "capital comercial".
Sudáfrica: La capital administrativa es Pretoria, la capital legislativa es Ciudad del Cabo y la capital judicial es Bloemfontein.
Suiza: Berna es la ciudad federal de Suiza y funciona como capital de facto.
También similar a Illinois y estado de Nueva York, la mayor parte de funcionarios electos a nivel estatal y oficiales que están basados en Pensilvania del Sudeste (Ciudad de Filadelfia, condado de Bucks, condado de Montgomery, condado de Delaware y condado de Chester) prefieren trabajar generalmente en Filadelfia.
Israel y Palestina: Tanto el Gobierno de Israel como la Autoridad Palestina reclaman Jerusalén como su capital.
Una reubicación simbólica de una ciudad capital a una ubicación geográfica o demográficamente periférica puede ser por razones económicas o estratégicas (a veces conocida como capital de avanzada o capital de punta de lanza).
Los emperadores Ming trasladaron su capital a Pekín desde el centro de Nanjing para ayudar a supervisar la frontera con los mongoles.
Delhi finalmente se hizo la capital colonial después de la Coronación Durbar del rey-emperador George V en 1911, siguiendo como la capital de India independiente a partir de 1947.
A veces, la ubicación de una nueva ciudad capital fue elegida para terminar las disputas reales o potenciales entre varias entidades, como en los casos de Canberra, Ottawa, Washington, Wellington y Managua.
En el período de los Tres Reinos, tanto Shu como Wu cayeron cuando sus respectivas capitales de Chengdu y Jianye cayeron.
Después del colapso de la dinastía Qing, la descentralización de la autoridad y la mejora de las tecnologías de transporte y comunicación permitieron tanto a los nacionalistas chinos como a los comunistas chinos reubicar rápidamente las capitales y mantener intactas sus estructuras de liderazgo durante la gran crisis de la invasión japonesa.
Se puede definir como un lugar permanente y densamente establecido con límites definidos administrativamente cuyos miembros trabajan principalmente en tareas no agrícolas.
Históricamente, los habitantes de las ciudades han sido una pequeña proporción de la humanidad en general, pero después de dos siglos de urbanización rápida y sin precedentes, más de la mitad de la población mundial ahora vive en ciudades, lo que ha tenido profundas consecuencias para la sostenibilidad global.
Esta mayor influencia significa que las ciudades también tienen influencias significativas en los problemas globales, como el desarrollo sostenible, el calentamiento global y la salud global.
Por lo tanto, las ciudades compactas a menudo se consideran un elemento crucial para combatir el cambio climático.
Por ejemplo, capitales de países como Beijing, Londres, Ciudad de México, Moscú, Nairobi, Nueva Delhi, París, Roma, Atenas, Seúl, Tokio y Washington, DC reflejan la identidad y el ápice de sus respectivas naciones.
La ciudad puede verse como una historia, un patrón de relaciones entre grupos humanos, un espacio de producción y distribución, un campo de fuerza física, un conjunto de decisiones vinculadas o una arena de conflicto.
Los censos nacionales utilizan una variedad de definiciones, que invocan factores como la población, la densidad de población, el número de viviendas, la función económica y la infraestructura, para clasificar las poblaciones como urbanas.
La interdependencia mutua de la ciudad y el país tiene una consecuencia tan obvia que se pasa por alto fácilmente: a escala mundial, las ciudades generalmente se limitan a áreas capaces de mantener una población agrícola permanente.
A medida que las ciudades crecían en complejidad, las principales instituciones cívicas, desde los asientos del gobierno hasta los edificios religiosos, también llegarían a dominar estos puntos de convergencia.
El entorno físico generalmente limita la forma en que se construye una ciudad.
Y puede configurarse para una defensa óptima dado el paisaje circundante.
Esta forma podría evolucionar a partir de un crecimiento sucesivo durante mucho tiempo, con rastros concéntricos de murallas y ciudadelas que marcan los límites de la ciudad más antiguos.
En ciudades como Moscú, este patrón sigue siendo claramente visible.
Las excavaciones en estas áreas han encontrado las ruinas de ciudades orientadas diversamente hacia comercio, política o religión.
Las ciudades planificadas de China se construyeron de acuerdo con principios sagrados para actuar como microcosmos celestiales.
Estos sitios aparecen planificados de una manera altamente reglamentada y estratificada, con una cuadrícula minimalista de habitaciones para los trabajadores y viviendas cada vez más elaboradas disponibles para las clases más altas.
En los siglos siguientes, las ciudades-estado independientes de Grecia, especialmente Atenas, desarrollaron la polis, una asociación de ciudadanos terratenientes masculinos que colectivamente constituían la ciudad.
Bajo la autoridad de su imperio, Roma transformó y fundó muchas ciudades (colonias), y con ellas trajo sus principios de arquitectura urbana, diseño y sociedad.
La civilización de Norte Chico incluyó hasta 30 centros demográficos principales en lo que es ahora la región de Norte Chico de Perú costero del norte y central.
El lugar del poder en Occidente se trasladó a Constantinopla y a la civilización islámica ascendente con sus principales ciudades de Bagdad, El Cairo y Córdoba.
Antes de los trece y catorce siglos, algunas ciudades se hacen estados poderosos, tomando áreas circundantes bajo su control o estableciendo Imperios marítimos extensos.
Las capitales más grandes de Europa Occidental (Londres y París) se beneficiaron del crecimiento del comercio después de aparición de un comercio Atlántico.
Inglaterra lideró el camino cuando Londres se convirtió en la capital de un imperio mundial y las ciudades de todo el país crecieron en ubicaciones estratégicas para la fabricación.
El liderazgo empresarial se manifestó a través de coaliciones de crecimiento formadas por constructores, agentes inmobiliarios, desarrolladores, medios de comunicación, actores gubernamentales como alcaldes y corporaciones dominantes.
Los resultados fueron esfuerzos en la revitalización del centro de la ciudad; gentrificación del centro de la ciudad; la transformación del CBD en empleo de servicios avanzados; entretenimiento, museos y lugares culturales; la construcción de estadios deportivos y complejos deportivos; y desarrollo frente al mar.
Hasta el 18vo siglo, un equilibrio existió entre la población agrícola rural y ciudades que presentan mercados y fabricación a pequeña escala.
El atractivo cultural de las ciudades también juega un papel en la atracción de los residentes.
Batam, Indonesia, Mogadiscio, Somalia, Xiamen, China y Niamey, Níger, se consideran entre las ciudades de más rápido crecimiento del mundo, con tasas de crecimiento anual del 5-8%.
La ONU predice un adicional de 2.500 millones de habitantes de las ciudades (y 300 millones menos de habitantes de los países) en todo el mundo para 2050, con el 90% de la expansión de la población urbana en Asia y África.
Un abismo profundo divide a ricos y pobres en estas ciudades, y por lo general contienen una élite súper rica que vive en comunidades cerradas y grandes masas de personas que viven en viviendas de calidad inferior con infraestructura inadecuada y condiciones deficientes.
Sin embargo, los municipios promulgan rutinariamente estatutos amplios dirigidos a delitos abiertos (y mal definidos), como merodear y obstruir, requerir permisos para protestas o exigir a los residentes y propietarios de viviendas que retiren la nieve de las aceras de la ciudad.
Estos se proporcionan más o menos rutinariamente, de una manera más o menos igual.
Estos criterios orientados a la producción a menudo dan lugar a "reglas de prestación de servicios", procedimientos regularizados para la prestación de servicios, que son intentos de codificar los objetivos de productividad de las burocracias de servicios urbanos.
Robert L. Lineberry, "Mandando Igualdad Urbana: La Distribución de Servicios públicos Municipales"; en Hahn & Levine (1980).
Sin embargo, la financiación de los servicios municipales, así como la renovación urbana y otros proyectos de desarrollo, es un problema perenne, que las ciudades abordan a través de llamamientos a los gobiernos superiores, acuerdos con el sector privado y técnicas como la privatización (venta de servicios al sector privado), la corporatización (formación de empresas cuasi privadas de propiedad municipal) y la financiarización (empaquetado de activos de la ciudad en instrumentos financieros negociables y derivados).
El impacto de la globalización y el papel de las corporaciones multinacionales en los gobiernos locales de todo el mundo, ha llevado a un cambio de perspectiva sobre la gobernanza urbana, lejos de la "teoría del régimen urbano" en la que una coalición de intereses locales gobierna funcionalmente, hacia una teoría del control económico externo, ampliamente asociada en los académicos con la filosofía del neoliberalismo.
Las herramientas de planificación, más allá del diseño original de la ciudad en sí, incluyen la inversión de capital público en infraestructura y controles de uso de la tierra, como la zonificación.
También están disponibles para las ciudades en su implementación de los objetivos de planificación los poderes municipales de zonificación, control de subdivisiones y la regulación de los principios de construcción, vivienda y saneamiento.
Las personas que viven relativamente juntas pueden vivir, trabajar y jugar, en áreas separadas, y asociarse con diferentes personas, formando enclaves étnicos o de estilo de vida o, en áreas de pobreza concentrada, guetos.
Los suburbios en el oeste y, cada vez más, las comunidades cerradas y otras formas de "privatopía" en todo el mundo, permiten a las élites locales segregarse en vecindarios seguros y exclusivos.
Este proletariado marginado, tal vez 1.500 millones de personas en la actualidad, 2.500 millones en 2030, es la clase social de más rápido crecimiento y más novedosa del planeta.
Es ontológicamente similar y diferente a la agencia histórica descrita en el Manifiesto Comunista.
Como centros de comercio, las ciudades han sido durante mucho tiempo el hogar del comercio minorista y el consumo a través de la interfaz de compras.
Un mercado laboral más grueso permite una mejor correspondencia de habilidades entre empresas e individuos.
Las élites culturales tienden a vivir en ciudades, unidas por el capital cultural compartido, y ellas mismas desempeñan algún papel en la gobernanza.
Greg Kerr y Jessica Oliver, "Rethinking Place Identities", en Kavaratzis, Warnaby y Ashworth (2015).
Los turistas patriotas visitan Agra para ver el Taj Mahal, o la ciudad de Nueva York para visitar el World Trade Center.
¿Por qué las personas anónimas, los pobres, los desfavorecidos, los desconectados, con frecuencia prefieren la vida en condiciones miserables en las viviendas al orden saludable y la tranquilidad de las pequeñas ciudades o las subdivisiones sanitarias de los desarrollos semirurales?
Aquellos que vinieron a vivir en ellos lo hicieron con el fin de participar y competir en cualquier nivel alcanzable.
Los deportes también juegan un papel importante en la marca de la ciudad y la formación de identidad local.
Más importante aún, también existe un enorme potencial a largo plazo tanto para el turismo como para la inversión (Kasimati, 2003).
La guerra trajo la concentración del liderazgo social y el poder político en manos de una minoría portadora de armas, instigada por un sacerdocio que ejercía poderes sagrados y poseía conocimientos científicos y mágicos secretos pero valiosos.
Durante la Segunda Guerra Mundial, los gobiernos nacionales en ocasiones declararon ciertas ciudades abiertas, entregándolas efectivamente a un enemigo que avanzaba para evitar daños y derramamiento de sangre.
Dicha guerra, conocida como contrainsurgencia, implica técnicas de vigilancia y guerra psicológica, así como combate cuerpo a cuerpo, extiende funcionalmente la prevención del crimen urbano moderno, que ya utiliza conceptos como el espacio defendible.
Debido a las mayores barreras de entrada, estas redes han sido clasificadas como monopolios naturales, lo que significa que la lógica económica favorece el control de cada red por una sola organización, pública o privada.
Kath Wellman y Frederik Pretorius, "Infraestructura urbana: productividad, evaluación de proyectos y finanzas"; en Wellman y Spiller (2012).
El saneamiento, necesario para una buena salud en condiciones de hacinamiento, requiere el suministro de agua y la gestión de residuos, así como la higiene individual.
La vida urbana moderna depende en gran medida de la energía transmitida a través de la electricidad para la operación de máquinas eléctricas (desde electrodomésticos hasta máquinas industriales hasta sistemas electrónicos ahora omnipresentes utilizados en comunicaciones, negocios y gobierno) y para semáforos, farolas e iluminación interior.
Tom Hart, "Transporte y la Ciudad"; en Paddison (2001).
Muchas grandes ciudades estadounidenses todavía operan el transporte público convencional por ferrocarril, como lo ejemplifica el siempre popular sistema de metro de la ciudad de Nueva York.
Los edificios y desechos antropogénicos, así como el cultivo en jardines, crean entornos físicos y químicos que no tienen equivalentes en la naturaleza, en algunos casos permitiendo una biodiversidad excepcional.
Desde una perspectiva, las ciudades no son ecológicamente sostenibles debido a sus necesidades de recursos.
Las ciudades modernas son conocidas por crear sus propios microclimas, debido al concreto, el asfalto y otras superficies artificiales, que se calientan a la luz del sol y canalizan el agua de lluvia hacia los conductos subterráneos.
Las partículas aéreas aumentan las precipitaciones en un 5-10%.
Por ejemplo, dentro del microclima urbano, los barrios pobres con menos vegetación soportan más calor (pero tienen menos medios para enfrentarlo).
Generalmente se llaman espacios abiertos urbanos (aunque esta palabra no siempre significa espacios verdes), espacios verdes, espacios verdes urbanos.
El estudio utilizó datos de casi 20,000 personas en el Reino Unido.
Las personas que no recibieron al menos dos horas, incluso si superaron una hora por semana, no obtuvieron los beneficios.
El estudio no contó el tiempo pasado en el propio patio o jardín de una persona como tiempo en la naturaleza, pero la mayoría de las visitas a la naturaleza en el estudio se llevaron a cabo a menos de dos millas de casa.
Saskia Sassen usó el término "ciudad global" en su trabajo de 1991, La Ciudad Global: Nueva York, Londres, Tokio para referirse al poder de una ciudad, estado y cosmopolitismo, más bien que a su talla.
3 (1982): 319 Las ciudades globales forman la piedra angular de la jerarquía global, ejerciendo el mando y el control a través de su influencia económica y política.
Los críticos de la noción apuntan a los diferentes reinos del poder y el intercambio.
Las corporaciones multinacionales y los bancos hacen sus sedes en ciudades globales y llevan a cabo gran parte de sus negocios dentro de este contexto.
Nancy Duxbury y Sharon Jeannotte, "Política de Gobernanza Cultural Global"; Capítulo 21 en The Ashgate Research Companion to Planning and Culture; Londres: Ashgate, 2013.
La conferencia Habitat I en 1976 adoptó la "Declaración de Vancouver sobre los Asentamientos Humanos", que identifica la gestión urbana como un aspecto fundamental del desarrollo y establece varios principios para el mantenimiento de los hábitats urbanos.
En enero de 2002, la Comisión de Asentamientos Humanos de las Naciones Unidas se convirtió en una agencia paraguas llamada Programa de las Naciones Unidas para los Asentamientos Humanos o ONU-Hábitat, miembro del Grupo de las Naciones Unidas para el Desarrollo.
Las políticas del Banco han tendido a centrarse en impulsar los mercados inmobiliarios a través del crédito y la asistencia técnica.
Las ciudades ocupan un lugar destacado en la cultura occidental tradicional, apareciendo en la Biblia tanto en formas malvadas como santas, simbolizadas por Babilonia y Jerusalén.
Las ciudades pueden ser percibidas en términos de extremos u opuestos: a la vez liberadoras y opresivas, ricas y pobres, organizadas y caóticas.
Esta y otras ideologías políticas influyen fuertemente en las narrativas y los temas en el discurso sobre las ciudades.
La literatura clásica y medieval incluye un género de descripciones que tratan de rasgos de la ciudad e historia.
Otras representaciones cinematográficas tempranas de ciudades en el siglo veinte generalmente las representaban como espacios tecnológicamente eficientes con sistemas de transporte de automóviles que funcionan sin problemas.
Un país es un organismo territorial o entidad política distinta (es decir, una nación).
No es inherentemente soberano.
El país más grande del mundo por área geográfica es Rusia, mientras que el más poblado es China, seguido de India, Estados Unidos, Indonesia, Pakistán y Brasil.
En muchos países europeos, las palabras se usan para subdivisiones del territorio nacional, como en el Bundeslander alemán, así como un término menos formal para un estado soberano.
No existe un acuerdo universal sobre el número de "países" en el mundo, ya que varios estados han disputado el estatus de soberanía.
El grado de autonomía de los países no soberanos varía ampliamente.
El informe clasifica el desarrollo del país en función del ingreso nacional bruto (RNB) per cápita.
El informe de 2019 reconoce solo a los países desarrollados de América del Norte, Europa y Asia y el Pacífico.
El Banco Mundial define sus regiones como Asia Oriental y el Pacífico, Europa y Asia Central, América Latina y el Caribe, Oriente Medio y África del Norte, América del Norte, Asia Meridional y África Subsahariana.
La exploración es el acto de buscar el propósito de descubrir información o recursos, especialmente en el contexto de la geografía o el espacio, en lugar de la investigación y el desarrollo que generalmente no se centra en las ciencias de la tierra o la astronomía.
Sólo el hecho por el emperador Nerón parecía ser un preparativo para la conquista de Etiopía o Nubia: en 62 dC dos legionarios exploraron las fuentes del río Nilo.
Los romanos también organizaron varias exploraciones en el norte de Europa y exploraron hasta China en Asia.
100 d.C. 166 d.C. Comienzan las relaciones romano-chinas.
El invento clave para su exploración fue la canoa estabilizadora, que proporcionó una plataforma rápida y estable para transportar bienes y personas.
Los estudios de 2011 en Wairau Bar en Nueva Zelanda muestran una alta probabilidad de que uno de los orígenes fuera la isla Ruahine en las Islas de la Sociedad.
Hay similitudes culturales y de idioma entre los isleños de Cook y los maoríes de Nueva Zelanda.
Durante 1328-1333, navegó a lo largo del Mar del Sur de China y visitó muchos lugares en el sudeste asiático y llegó hasta el sur de Asia, aterrizando en Sri Lanka e India, e incluso fue a Australia.
Portugal y España dominaron las primeras etapas de exploración, mientras que otras naciones europeas siguieron, como Inglaterra, Países Bajos y Francia.
Las condiciones extremas en las profundidades del mar requieren métodos y tecnologías elaboradas para soportarlas.
Una subdivisión administrativa, en cambio, se entiende como una división de un estado propio.
Los territorios dependientes que actualmente permanecen en el mundo generalmente mantienen un grado muy alto de autonomía política.
El estatuto de las Islas Cook se considera equivalente a la independencia a efectos del derecho internacional, y el país ejerce plena soberanía sobre sus asuntos internos y externos.
Sin embargo, bajo los términos del acuerdo de libre asociación, Nueva Zelanda conserva cierta responsabilidad por las relaciones exteriores y la defensa de Niue.
Esta lista generalmente se limita a entidades que están sujetas a un tratado internacional sobre su condición, están deshabitadas o tienen un nivel único de autonomía y son en gran medida autónomas en asuntos distintos de los asuntos internacionales.
Son jurisdicciones administradas independientemente, aunque el Gobierno británico es el único responsable de la defensa y la representación internacional y tiene la responsabilidad final de garantizar un buen gobierno.
Ninguna dependencia de la corona tiene representación en el Parlamento del Reino Unido.
Nueva Zelanda y sus dependencias comparten el mismo gobernador general y constituyen un reino monárquico.
El Pacto mutuamente negociado para Establecer una Comunidad de las Islas Marianas del Norte (CNMI) en la Unión Política con los Estados Unidos se aprobó en 1976.
Esta es una fuente constante de ambiguedad y confusión cuando se trata de definir, entender y explicar la relación política de Puerto Rico con los Estados Unidos.
Sin embargo, el estado de sus "países constituyentes" en el Caribe (Aruba, Curazao y Sint Maarten) se puede considerar similar a dependencias o "estados no independientes asociados".
Las fronteras son fronteras geográficas, impuestas por características geográficas como los océanos o por agrupaciones arbitrarias de entidades políticas como gobiernos, estados soberanos, estados federados y otras entidades subnacionales.
La mayoría de las fronteras exteriores están parcial o totalmente controladas, y sólo pueden cruzarse legalmente en los puestos fronterizos designados y las zonas fronterizas pueden ser controladas.
La mayoría de los países tienen algún tipo de control fronterizo para regular o limitar el movimiento de personas, animales y bienes dentro y fuera del país.
Para permanecer o trabajar dentro de las fronteras de un país, los extranjeros (personas extranjeras) pueden necesitar documentos o permisos especiales de inmigración; pero la posesión de dichos documentos no garantiza que se le permita cruzar la frontera.
La mayoría de los países prohíben el transporte de drogas ilegales o animales en peligro de extinción a través de sus fronteras.
En lugares donde el contrabando, la migración y la infiltración son un problema, muchos países fortifican las fronteras con vallas y barreras, e instituyen procedimientos formales de control fronterizo.
Esto es común en países dentro del Espacio Schengen europeo y en secciones rurales de la frontera Canadá-Estados Unidos.
Ríos: algunas fronteras políticas se han formalizado a lo largo de fronteras naturales formadas por ríos.
En la Biblia hebrea, Moisés definió el centro del río Arnón como la frontera entre Moab y las tribus israelitas que se establecieron al este del Jordán.
Ejemplos son el lago Tanganyika, con la República Democrática del Congo y Zambia en su costa oeste y Tanzania y Burundi en el este; y los Grandes Lagos, que forman una parte sustancial de la frontera entre Canadá y los Estados Unidos.
Cordilleras: Muchas naciones tienen sus fronteras políticas definidas a lo largo de cadenas montañosas, a menudo a lo largo de una división de drenaje.
Un ejemplo es el bosque defensivo creado por la dinastía Song de China en el siglo XI.
Por ejemplo, la frontera entre Alemania Oriental y Occidental ya no es una frontera internacional, pero aún se puede ver debido a los marcadores históricos en el paisaje, y sigue siendo una división cultural y económica en Alemania.
Las fronteras marítimas existen en el contexto de aguas territoriales, zonas contiguas y zonas económicas exclusivas; sin embargo, la terminología no abarca límites del lago o del río, que se consideran dentro del contexto de límites de la tierra.
El espacio aéreo se extiende a 12 millas náuticas de la costa de un país y tiene la responsabilidad de proteger su propio espacio aéreo a menos que esté bajo la protección de la OTAN en tiempos de paz.
Sin embargo, hay un acuerdo general de espacio aéreo vertical que termina en el punto de la línea de Kármán.
Las regulaciones fronterizas generales son colocadas por los gobiernos nacionales y locales y pueden variar dependiendo de la nación y las condiciones políticas o económicas actuales.
Trabajar a través de las fronteras: aprovechar el potencial de las actividades transfronterizas para mejorar la seguridad de los medios de vida en las tierras áridas del Cuerno de África.
El tráfico económico humano a través de las fronteras (aparte del secuestro) puede implicar desplazamientos masivos entre lugares de trabajo y asentamientos residenciales.
Puede permitir y detener el movimiento, tanto a través como a lo largo de las fronteras.
Muchas regiones transfronterizas también participan activamente en el fomento de la comunicación y el diálogo interculturales, así como en las estrategias de desarrollo económico transfronterizo.
Desde su concepción a mediados de los 80, esta práctica artística ha ayudado en el desarrollo de cuestiones relacionadas con la patria, las fronteras, la vigilancia, la identidad, la raza, el origen étnico y el origen nacional.
Las fronteras pueden incluir, pero no se limitan a, el idioma, la cultura, la clase social y económica, la religión y la identidad nacional.
Estos artistas son a menudo "cruceros fronterizos".
En general, una zona rural o un campo es un área geográfica que se encuentra fuera de los pueblos y ciudades.
Las zonas rurales típicas tienen una baja densidad de población y pequeños asentamientos.
Las regiones predominantemente urbanas tienen menos del 15 por ciento de su población viviendo en una comunidad rural.
Las regiones del norte rurales son divisiones del censo predominantemente rurales que se encuentran completamente o generalmente encima de las líneas siguientes del paralelo en cada provincia: Terranova y Labrador, 50mo; Quebec 54to; Ontario, 54to; Manitoba, 53ro; Saskatchewan, Alberta y Columbia Británica, 54to.
La Oficina del Censo de los Estados Unidos, el Servicio de Investigación Económica del USDA y la Oficina de Administración y Presupuesto (OMB) se han unido para ayudar a definir las áreas rurales.
El proyecto de ley agrícola de 2002 (P.L. 107-171, Sec.
De acuerdo con el manual, Definitions of Rural: A Handbook for Health Policy Makers and Researchers, "Los residentes de los condados metropolitanos generalmente se cree que tienen fácil acceso a los servicios de salud relativamente concentrados de las áreas centrales del condado.
Esto se convirtió en la definición de modificación de Goldsmith rural.
El gobierno del presidente Emmanuel Macron lanzó un plan de acción en 2019 a favor de las áreas rurales llamado "Agenda Rural".
En Escocia se utiliza una definición diferente de rural.
RBI define las áreas rurales como aquellas áreas con una población de menos de 49,000 (ciudades de nivel -3 a nivel-6).
Las áreas rurales en Paquistán que están cerca de ciudades se consideran como áreas suburbanas o barrios residenciales.
Los suburbios pueden tener su propia jurisdicción política o legal, especialmente en los Estados Unidos, pero este no es siempre el caso, especialmente en el Reino Unido, donde la mayoría de los suburbios se encuentran dentro de los límites administrativos de las ciudades.
En otros, como Marruecos, Francia y la mayor parte de los Estados Unidos, muchos barrios residenciales permanecen municipalidades separadas o se gobiernan en la localidad como la parte de un área metropolitana más grande como un condado, distrito o barrio.
Los términos suburbio interior y suburbio exterior se utilizan para diferenciar entre las zonas de mayor densidad en las proximidades del centro de la ciudad (que no se conoce como "suburbios" en la mayoría de los otros países), y los suburbios de menor densidad en las afueras de la zona urbana.
En Nueva Zelanda, la mayoría de los suburbios no están legalmente definidos, lo que puede llevar a confusión sobre dónde pueden comenzar y terminar.
La palabra suburbano fue empleada por primera vez por el estadista romano Cicerón en referencia a las grandes villas y fincas construidas por los ricos patricios de Roma en las afueras de la ciudad.
A mediados del 19no siglo, las primeras áreas suburbanas principales surgían alrededor de Londres ya que la ciudad (entonces el más grande en el mundo) se hizo más hacinada e insalubre.
La línea llegó a Harrow en 1880.
El departamento de marketing del Met acuñó el término "Metro-land" en 1915 cuando la Guía de la Línea de Extensión se convirtió en la guía de Metro-land, con un precio de 1d.
En parte, esto era una respuesta a la carencia chocante de la aptitud entre muchos reclutas durante la Primera Guerra Mundial, atribuido a condiciones de vida pobres; una creencia resumida en un cartel de la vivienda del período "no se puede esperar conseguir a una población A1 de casas C3" – refiriéndose a clasificaciones de la aptitud militares del período.
El Informe también legisló sobre los estándares mínimos requeridos para una mayor construcción suburbana; esto incluyó la regulación sobre la densidad máxima de viviendas y su disposición e incluso hizo recomendaciones sobre el número ideal de dormitorios y otras habitaciones por casa.
En solo una década, los suburbios aumentaron dramáticamente de tamaño.
Levittown se desarrolló como un prototipo principal de la vivienda producida en masa.
Comprar diferentes bienes y servicios en una ubicación central sin tener que viajar a múltiples ubicaciones, ayudó a mantener los centros comerciales como un componente de estos suburbios de nuevo diseño que estaban en auge en la población.
La Ley de Carreteras de 1956 ayudó a financiar la construcción de 64,000 kilómetros en todo el país al tener $ 26 mil millones para usar, lo que ayudó a vincular muchos más a estos centros comerciales con facilidad.
Algunos suburbios se habían desarrollado alrededor de grandes ciudades donde había transporte ferroviario a los puestos de trabajo en el centro.
El producto fue un gran boom de la vivienda.
Con 16 millones de veteranos elegibles, la oportunidad de comprar una casa estaba repentinamente a la mano.
Los desarrolladores compraron terrenos vacíos a las afueras de la ciudad, instalaron casas de extensión basadas en un puñado de diseños y proporcionaron calles y servicios públicos, o los funcionarios públicos locales se apresuraron a construir escuelas.
Los veteranos podrían obtener uno con un pago inicial mucho más bajo.
El crecimiento de los suburbios fue facilitado por el desarrollo de leyes de zonificación, redlining y numerosas innovaciones en el transporte.
Los afroamericanos y otras personas de color permanecieron concentrados en gran medida dentro de los núcleos en descomposición de la pobreza urbana.
Después de la Segunda Guerra Mundial, la disponibilidad de préstamos de la FHA estimuló un auge de la vivienda en los suburbios estadounidenses.
El crecimiento económico en los Estados Unidos animó la suburbanización de ciudades americanas que requirieron inversiones masivas para la nueva infraestructura y casas.
Una estrategia alternativa es el diseño deliberado de "nuevas ciudades" y la protección de los cinturones verdes alrededor de las ciudades.
Los subsidios federales para el desarrollo suburbano aceleraron este proceso, al igual que la práctica de redlining por parte de los bancos y otras instituciones crediticias.
Virginia Beach es ahora la ciudad más grande de toda Virginia, después de haber excedido hace mucho tiempo la población de su ciudad primaria vecina, Norfolk.
Un mayor porcentaje de blancos (tanto no hispanos como, en algunas áreas, hispanos) y un porcentaje menor de ciudadanos de otros grupos étnicos que en áreas urbanas.
En comparación con las zonas rurales, los suburbios suelen tener una mayor densidad de población, niveles de vida más altos, sistemas de carreteras más complejos, más tiendas y restaurantes franquiciados, y menos tierras de cultivo y vida silvestre.
Sin embargo, de esta población metropolitana, en 2001 casi la mitad vivía en barrios de baja densidad, con sólo uno de cada cinco que viven en un típico barrio "urbano".
En todo Canadá, existen planes integrales para frenar la expansión.
La mayoría del crecimiento demográfico reciente en las tres áreas metropolitanas más grandes de Canadá (Toronto Mayor, Montreal Mayor y Vancouver Mayor) ha ocurrido en municipalidades no principales.
Esto es debido a la anexión y la huella geográfica grande dentro de las fronteras de la ciudad.
En el censo de 2016, la ciudad de Calgary tenía una población de 1.239.220, mientras que el área metropolitana de Calgary tenía una población de 1.392.609, lo que indica que la gran mayoría de las personas en el Calgary CMA vivían dentro de los límites de la ciudad.
En el Reino Unido, el gobierno está tratando de imponer densidades mínimas a los planes de vivienda recientemente aprobados en partes del sudeste de Inglaterra.
Los suburbios se pueden encontrar en Guadalajara, Ciudad de México, Monterrey y la mayoría de las ciudades principales.
A medida que aumentaba el crecimiento de los suburbios de clase media y alta, aumentaron las áreas ocupadas de clase baja, especialmente las "ciudades perdidas" en México, los campamentos en Chile, las barriadas en Perú, las miserias de las villas en Argentina, los asentamientos en Guatemala y las favelas de Brasil.
En un caso ilustrativo de Sudáfrica, se han construido viviendas RDP.
En ciertas áreas como Klang, Subang Jaya y Petaling Jaya, los suburbios forman el núcleo de estos lugares.
En el sistema suburbano, la mayoría de los viajes de un componente a otro requieren que los automóviles ingresen a una carretera de recolección, sin importar cuán corta o larga sea la distancia.
Si se produce un accidente de tráfico en una carretera colectora, o si la construcción de la carretera inhibe el flujo, entonces todo el sistema de carreteras puede quedar inutilizado hasta que se elimine el bloqueo.
Esto fomenta los viajes en automóvil incluso para distancias tan bajas como varios cientos de yardas o metros (que pueden haberse convertido en hasta varias millas o kilómetros debido a la red de carreteras).
En conjunto, estos dos grupos de contribuyentes representan una fuente de ingresos potenciales en gran medida sin explotar que las ciudades pueden comenzar a apuntar de manera más agresiva, particularmente si están luchando.
Canciones francesas como La Zone de Fréhel (1933), Aux quatre coins de la banlieue de Damia (1936), Ma banlieue de Reda Caire (1937), o Banlieue de Robert Lamoureux (1953), evocan explícitamente los suburbios de París desde la década de 1930.
Aunque el cine francés pronto se interesó por los cambios urbanos en los suburbios, con películas como Mon oncle de Jacques Tati (1958), L'Amour existe de Maurice Pialat (1961) o Dos o tres cosas que sé de ella de Jean-Luc Godard (1967).
La canción de 1962 "Little Boxes" de Malvina Reynolds satiriza el desarrollo de los suburbios y sus valores burgueses y conformistas percibidos, mientras que la canción de 1982 Subdivisiones de la banda canadiense Rush también discute los suburbios, al igual que Rockin 'the Suburbs de Ben Folds.
Over the Hedge es una tira cómica escrita y dibujada por Michael Fry y T. Lewis.
Las series de televisión británicas como The Good Life, Butterflies y The Fall and Rise of Reginald Perrin han representado los suburbios como bien cuidados pero implacablemente aburridos, y sus residentes como demasiado conformes o propensos a volverse locos.
Una aldea es un asentamiento o comunidad humana agrupada, más grande que una aldea pero más pequeña que una ciudad (aunque la palabra se usa a menudo para describir aldeas y ciudades más pequeñas), con una población que suele oscilar entre unos pocos cientos y unos pocos miles.
Esto también permitió la especialización de la mano de obra y la artesanía, y el desarrollo de muchos oficios.
El tamaño de estos pueblos varía considerablemente.
Los desa se encuentran generalmente en áreas rurales, mientras que los kelurahan son generalmente subdivisiones urbanas.
Una desa o kelurahan es la subdivisión de un kecamatan (subdistrito), a su vez la subdivisión de un kabupaten (distrito) o kota (ciudad).
En Malasia, un kampung se determina como una localidad con 10.000 o menos personas.
Todos los musulmanes en el pueblo malayo o indonesio quieren que se ore por ellos, y recibir las bendiciones de Alá en el más allá.
Singapur continental solía tener muchos pueblos kampung, pero los desarrollos modernos y las obras de urbanización rápida los han visto arrasados; Kampong Lorong Buangkok es el último pueblo sobreviviente en el continente del país.
El pueblo de Vietnam es el símbolo típico de la producción agrícola asiática.
En Eslovenia, la palabra selo se usa para pueblos muy pequeños (menos de 100 personas) y en dialectos; la palabra eslovena vas se usa en todas partes de Eslovenia.
Podría ser relativo a un sánscrito como la palabra afgana deh y la palabra indonesia desa.
Aproximadamente el 46% de todas las personas migradas han cambiado su residencia de una ciudad a otra.
La unidad administrativa más baja del Imperio ruso, un volost, o su sucesor soviético o ruso moderno, un selsoviet, tenía típicamente la sede en un selo y abrazaba algunas aldeas vecinas.
Mientras que los campesinos del centro de Rusia vivían en una aldea alrededor de la mansión del señor, una familia cosaca a menudo vivía en su propia granja, llamada khutor.
Hay, sin embargo, otro tipo más pequeño de establecimiento que se designa en ucraniano como selysche.
Representan un tipo de pequeña localidad rural que una vez pudo haber sido un khutir, un asentamiento de pescadores o una dacha.
Sin embargo, a menudo se evita la ambiguedad en relación con los asentamientos urbanizados al referirse a ellos utilizando la abreviatura de tres letras smt en su lugar.
Se hicieron muy populares durante la reforma de Stolypin a principios del 20mo siglo.
Los pueblos más grandes también se pueden denominar Flecken o Markt, dependiendo de la región.
Por ejemplo, en áreas como los Wolds de Lincolnshire, los pueblos a menudo se encuentran a lo largo de la línea de la primavera a mitad de camino abajo las laderas y se originan como establecimientos de la línea de la primavera, con los sistemas del campo abiertos originales alrededor del pueblo.
Algunos pueblos han desaparecido (por ejemplo, pueblos medievales desiertos), a veces dejando atrás una iglesia o casa solariega y a veces nada más que golpes en los campos.
Otros pueblos han crecido y se han fusionado y a menudo forman centros dentro de la masa general de suburbios, como Hampstead, Londres y Didsbury en Manchester.
Visto como lejos del bullicio de la vida moderna, se representa como tranquilo y armonioso, aunque un poco introspectivo.
Éstos (como Murton, condado Durham) crecieron de aldeas cuando el hundimiento de una mina de carbón a principios del 20mo siglo causó un crecimiento rápido en su población y los dueños de la mina de carbón construyeron la nueva vivienda, tiendas, bares e iglesias.
Maltby fue construido bajo los auspicios de la Sheepbridge Coal and Iron Company e incluyó amplios espacios abiertos y provisión de jardines.
El pueblo típico tenía un pub o posada, tiendas y un herrero.
Sin embargo, algunas parroquias civiles no tienen parroquia, ciudad o ayuntamiento en funcionamiento ni una reunión parroquial en funcionamiento.
En Escocia, el equivalente es también un consejo de la comunidad, sin embargo, a pesar de ser cuerpos estatutarios no tienen poderes ejecutivos.
El distrito de Danniyeh consta de treinta y seis pequeñas aldeas, que incluyen Almrah, Kfirchlan, Kfirhbab, Hakel al Azimah, Siir, Bakhoun, Miryata, Assoun, Sfiiri, Kharnoub, Katteen, Kfirhabou, Zghartegrein, Ein Qibil.
Dinniyeh tiene un excelente ambiente ecológico lleno de bosques, huertos y arboledas.
Las aldeas en el sur de Siria (Hauran, Jabal al-Druze), el noreste (la isla siria) y la cuenca del río Orontes dependen principalmente de la agricultura, principalmente granos, verduras y frutas.
Las ciudades mediterráneas en Siria, como Tartus y Latakia tienen tipos similares de pueblos.
Cada urbanización es un "pueblo" a menos que se eleve por decreto a la siguiente categoría.
Sin embargo, esta es una generalidad; en muchos estados, hay aldeas que son un orden de magnitud más grande que las ciudades más pequeñas del estado.
En algunos casos, el pueblo puede ser colindante con la ciudad o municipio, en cuyo caso los dos pueden tener un gobierno consolidado.
Hempstead, el pueblo más grande, tiene 55.000 residentes; haciéndolo más populoso que algunas ciudades del estado.
El pueblo de Arlington Heights, Illinois tenía 75.101 residentes a partir del censo de 2010.
Las aldeas pueden incorporar tierras en múltiples municipios e incluso en múltiples condados.
El pueblo más grande es Menomonee Falls, que tiene más de 32,000 residentes.
En Maryland, una localidad designada "Pueblo de ..." puede ser una ciudad incorporada o un distrito fiscal especial.
En ese momento los gobernantes tradicionales solían tener poder absoluto en sus regiones administrativas.
Cada pueblo hausa estaba gobernado por Magaji (cabeza del pueblo) que era responsable ante su Hakimi (alcalde) a nivel de la ciudad.
Sin embargo, tienen casas de barro con techos de paja, como en la mayoría de las aldeas del norte, los techos de zinc se están convirtiendo en una vista común.
Otros tienen la suerte de tener pozos a poca distancia.
Un atlas es una colección de mapas; es típicamente un conjunto de mapas de la Tierra o una región de la Tierra.
Este título proporciona la definición de Mercator de la palabra como una descripción de la creación y la forma de todo el universo, no simplemente como una colección de mapas.
Un atlas de escritorio se hace similar a un libro de referencia.
En cartografía, una línea de contorno (a menudo llamada simplemente "contorno") une puntos de igual elevación (altura) por encima de un nivel dado, como el nivel medio del mar.
El gradiente de la función es siempre perpendicular a las líneas de contorno.
Las líneas de contorno son curvas, rectas o una mezcla de ambas líneas en un mapa que describe la intersección de una superficie real o hipotética con uno o más planos horizontales.
En 1701, Edmond Halley usó tales líneas (isogonos) en un gráfico de variación magnética.
En 1791, un mapa de Francia de J. L. Dupain-Triel utilizó líneas de contorno a intervalos de 20 metros, hachures, alturas puntuales y una sección vertical.
Los isobaths no se usaron rutinariamente en cartas náuticas hasta aquellos de Rusia a partir de 1834 y aquellos de Gran Bretaña a partir de 1838.
Todavía en 1944, John K. Wright todavía prefería el isograma, pero nunca alcanzó un amplio uso.
A pesar de los intentos de seleccionar un solo estándar, todas estas alternativas han sobrevivido hasta el presente.
Las estaciones meteorológicas rara vez se colocan exactamente en una línea de contorno (cuando lo están, esto indica una medida exactamente igual al valor del contorno).
En meteorología, las presiones barométricas mostradas se reducen al nivel del mar, no las presiones superficiales en las ubicaciones del mapa.
Los isallóbaros son líneas que unen puntos de igual cambio de presión durante un intervalo de tiempo específico.
Los gradientes isallobáricos son componentes importantes del viento, ya que aumentan o disminuyen el viento geostrófico.
Una isoterma a 0 °C se llama nivel de congelación.
A partir de estos contornos, se puede determinar un sentido del terreno general.
En cartografía, el intervalo de contorno es la diferencia de elevación entre líneas de contorno adyacentes.
Dos o más líneas de contorno que se fusionan indican un acantilado.
Por lo general, los intervalos de contorno son consistentes en todo el mapa, pero hay excepciones.
Si cruzar una línea equipotencial representa ascender o descender el potencial se infiere de las etiquetas en las cargas.
La precipitación ácida se indica en mapas con isoplats.
Las líneas de contorno también se utilizan para mostrar información no geográfica en economía.
Tales isolinas son útiles para representar más de dos dimensiones (o cantidades) en gráficos bidimensionales.
Al interpretar imágenes de radar, un isodop es una línea de igual velocidad Doppler, y un isoeco es una línea de igual reflectividad de radar.
El color de la línea es la elección de cualquier número de pigmentos que se adapten a la pantalla.
El tipo de línea se refiere a si la línea de contorno básica es sólida, discontinua, punteada o rota en algún otro patrón para crear el efecto deseado.
El marcado numérico es la manera de denotar los valores aritméticos de las líneas de contorno.
Si las líneas de contorno no están etiquetadas numéricamente y las líneas adyacentes tienen el mismo estilo (con el mismo peso, color y tipo), entonces la dirección del gradiente no se puede determinar a partir de las líneas de contorno solas.
Un mapa de contorno correctamente etiquetado ayuda al lector a interpretar rápidamente la forma del terreno.
A continuación, las coordenadas de otros lugares se miden desde el punto de control más cercano a través de la topografía.
Este fenómeno se llama cambio de datum.
Empresas más ambiciosas como el Arco Geodésico Struve en Europa del Este (1816-1855) y el Gran Estudio Trigonométrico de la India (1802-1871) tomaron mucho más tiempo, pero resultaron en estimaciones más precisas de la forma del elipsoide terrestre.
Una definición aproximada del nivel del mar es el dato WGS 84, un elipsoide, mientras que una definición más precisa es el Modelo Gravitacional de la Tierra 2008 (EGM2008), utilizando al menos 2.159 armónicos esféricos.
Cuando se usa sin calificación, el término latitud se refiere a la latitud geodésica.
El cambio de dato entre dos datos particulares puede variar de un lugar a otro dentro de un país o región, y puede ser de cero a cientos de metros (o varios kilómetros para algunas islas remotas).
Por ejemplo, en Sydney hay una diferencia de 200 metros (700 pies) entre las coordenadas GPS configuradas en GDA (basado en el estándar global WGS 84) y AGD (usado para la mayoría de los mapas locales), que es un error inaceptablemente grande para algunas aplicaciones, como la topografía o la ubicación del sitio para el buceo.
Dado que los datos de referencia pueden tener diferentes radios y diferentes puntos centrales, un punto específico en la Tierra puede tener coordenadas sustancialmente diferentes dependiendo del dato utilizado para hacer la medición.
Los datos de referencia más comunes en uso en América del Norte son NAD27, NAD83 y WGS 84.
Este dato, designado como NAD 83 ... se basa en el ajuste de 250,000 puntos, incluidas 600 estaciones Doppler satelitales que limitan el sistema a un origen geocéntrico ".
Es el marco de referencia utilizado por el Departamento de Defensa de los Estados Unidos (DoD) y está definido por la Agencia Nacional de Inteligencia Geoespacial (NGA) (anteriormente la Agencia de Cartografía de Defensa, luego la Agencia Nacional de Cartografía e Imágenes).
Fue utilizado como el marco de referencia para la emisión de efemérides GPS (órbitas) a partir del 23 de enero de 1987.
Se convirtió en el marco de referencia para las órbitas de transmisión el 28 de junio de 1994.
El WGS 84 (G873) fue adoptado como marco de referencia para las órbitas de radiodifusión el 29 de enero de 1997.
WGS 84 es el dato estándar predeterminado para coordenadas almacenadas en unidades GPS recreativas y comerciales.
Por ejemplo, la diferencia longitudinal entre un punto en el ecuador en Uganda, en la placa africana, y un punto en el ecuador en Ecuador, en la placa sudamericana, aumenta en aproximadamente 0,0014 segundos de arco por año.
La mayoría de los mapas, como dentro de un solo país, no abarcan placas.
Ptolomeo le atribuyó la plena adopción de la longitud y la latitud, en lugar de medir la latitud en términos de la duración del día de verano.
La cartografía matemática se reanudó en Europa después de la recuperación de Maximus Planudes del texto de Ptolomeo un poco antes de 1300; el texto fue traducido al latín en Florencia por Jacobus Angelus alrededor de 1407.
Luego eligen el mapeo más apropiado del sistema de coordenadas esféricas en ese elipsoide, llamado sistema de referencia terrestre o dato geodésico.
Un punto en la superficie de la Tierra es el ángulo entre el plano ecuatorial y la línea recta que pasa a través de ese punto y a través (o cerca) del centro de la Tierra.
Todos los meridianos son mitades de grandes elipses (a menudo llamados grandes círculos), que convergen en los polos norte y sur.
El meridiano antípoda de Greenwich es a la vez 180 ° W y 180 ° E. Esto no debe combinarse con la Línea de Fecha Internacional, que diverge de ella en varios lugares por razones políticas y de conveniencia, incluso entre el extremo oriental de Rusia y el extremo occidental de las Islas Aleutianas.
Las coordenadas en un mapa son generalmente en términos de desplazamientos N y E hacia el este en relación con un origen especificado.
En geografía, latitud es una coordenada geográfica que especifica la posición norte-sur de un punto en la superficie de la Tierra.
La latitud se utiliza junto con la longitud para especificar la ubicación precisa de las características en la superficie de la Tierra.
El segundo paso es aproximar el geoide mediante una superficie de referencia matemáticamente más simple.
Las líneas de latitud y longitud constantes juntas constituyen una retícula en la superficie de referencia.
Dado que hay muchos elipsoides de referencia diferentes, la latitud precisa de una característica en la superficie no es única: esto se enfatiza en la norma ISO que establece que "sin la especificación completa del sistema de referencia de coordenadas, las coordenadas (es decir, latitud y longitud) son ambiguas en el mejor de los casos y sin sentido en el peor".
El plano a través del centro de la Tierra y perpendicular al eje de rotación intersecta la superficie en un gran círculo llamado Ecuador.
La variación de tiempo se discute más completamente en el artículo sobre la inclinación axial.
La situación se invierte en el solsticio de junio, cuando el Sol está sobre el Trópico de Cáncer.
Dado que la latitud se define con respecto a un elipsoide, la posición de un punto dado es diferente en cada elipsoide: no se puede especificar exactamente la latitud y longitud de una característica geográfica sin especificar el elipsoide utilizado.
La latitud geográfica debe usarse con cuidado.
La evaluación de la integral de distancia meridiana es fundamental para muchos estudios en geodesia y proyección de mapas.
Hay dos métodos de proceder.
Cuando se convierte de isométrico o conforme a geodésico, dos iteraciones de Newton-Raphson da precisión de doble precisión.
Las diferencias mostradas en la gráfica son en minutos de arco.
La transformación entre coordenadas geodésicas y cartesianas se puede encontrar en la conversión de coordenadas geográficas.
En general, la vertical verdadera en un punto de la superficie no coincide exactamente ni con la normal al elipsoide de referencia ni con la normal al geoide.
La longitud es una coordenada geográfica que especifica la posición este-oeste de un punto en la superficie de la Tierra, o la superficie de un cuerpo celeste.
El meridiano principal, que pasa cerca del Observatorio Real, Greenwich, Inglaterra, se define como 0 ° de longitud por convención.
La hora local (por ejemplo, desde la posición del sol) varía con la longitud, una diferencia de 15 ° de longitud correspondiente a una diferencia de una hora en la hora local.
El principio es sencillo, pero en la práctica, encontrar un método confiable para determinar la longitud tomó siglos y requirió el esfuerzo de algunas de las mentes científicas más grandes.
Su primer meridiano pasó por Alejandría.
Usó un meridiano principal a través de las Islas Canarias, de modo que todos los valores de longitud fueran positivos.
Los astrónomos hindúes y musulmanes continuaron desarrollando estas ideas, agregando muchas nuevas ubicaciones y, a menudo, mejorando los datos de Ptolomeo.
En la Edad media posterior, el interés a la geografía revivió en el Oeste, ya que los viajes aumentaron, y la beca árabe comenzó a conocerse a través del contacto con España y África del Norte.
Cristóbal Colón hizo dos intentos de usar eclipses lunares para descubrir su longitud, el primero en la isla Saona, el 14 de septiembre de 1494 (segundo viaje), y el segundo en Jamaica el 29 de febrero de 1504 (cuarto viaje).
Inicialmente un dispositivo de observación, los desarrollos durante el próximo medio siglo lo transformaron en una herramienta de medición precisa.
En tierra, el período desde el desarrollo de telescopios y relojes de péndulo hasta mediados del siglo XVIII vio un aumento constante en el número de lugares cuya longitud se había determinado con una precisión razonable, a menudo con errores de menos de un grado, y casi siempre dentro de 2-3 °.
Hacer observaciones precisas en un oleaje oceánico es mucho más difícil que en tierra, y los relojes de péndulo no funcionan bien en estas condiciones.
Ofreció dos niveles de recompensas, para soluciones dentro de 1 ° y 0.5 °.
Este trabajo fue apoyado y recompensado con miles de libras de la Junta de Longitud, pero luchó para recibir dinero hasta la recompensa máxima de 20.000 libras esterlinas, finalmente recibiendo un pago adicional en 1773 después de la intervención del parlamento.
Las distancias lunares entraron en uso general después de 1790.
Rápidamente se dio cuenta de que el telégrafo podría usarse para transmitir una señal de tiempo para la determinación de la longitud.
La Encuesta estableció cadenas de ubicaciones mapeadas a través de América Central y del Sur, y las Indias Occidentales, y hasta Japón y Chína en los años 1874-1890.
Esto cambió cuando la telegrafía inalámbrica estuvo disponible a principios del siglo XX.
Los sistemas de radionavegación entraron en uso general después de la Segunda Guerra Mundial.
Con la excepción de la declinación magnética, todos los métodos probados son practicables.
La longitud en un punto puede determinarse calculando la diferencia de tiempo entre esa en su ubicacion y el Tiempo Universal Coordinado (UTC).
La palabra cerca se usa porque el punto podría no estar en el centro de la zona horaria; también las zonas horarias se definen políticamente, por lo que sus centros y límites a menudo no se encuentran en meridianos a múltiplos de 15 °.
La convención estándar internacional (ISO 6709) -que Oriente es positivo- es consistente con un sistema de coordenadas cartesianas diestras, con el Polo Norte hacia arriba.
Desde entonces han cambiado al enfoque estándar.
El geoide es la forma que tomaría la superficie del océano bajo la influencia de la gravedad de la Tierra, incluida la atracción gravitacional y la rotación de la Tierra, si otras influencias como los vientos y las mareas estuvieran ausentes.
Sólo puede ser conocido a través de extensas mediciones gravitacionales y cálculos.
Aunque la Tierra física tiene excursiones de +8,848 m (Monte Everest) y +10,984 (Fosa de Marianas), la desviación del geoide de un elipsoide varía de +85 m (Islandia) a +106 m (sur de la India), menos de 200 m en total.
Si las masas continentales de tierra estuvieran entrecruzadas por una serie de túneles o canales, el nivel del mar en esos canales también casi coincidiría con el geoide.
Eso significa que cuando se viaja en barco, uno no nota las ondulaciones del geoide; la vertical local (línea de plomada) siempre es perpendicular al geoide y el horizonte local tangencial a él.
Esto se debe a que los satélites GPS, que orbitan alrededor del centro de gravedad de la Tierra, pueden medir alturas solo en relación con un elipsoide de referencia geocéntrico.
Los receptores GPS modernos tienen una cuadrícula implementada en su software mediante la cual obtienen, desde la posición actual, la altura del geoide (por ejemplo, el geoide EGM-96) sobre el elipsoide del Sistema Geodésico Mundial (WGS).
Si esa esfera estuviera cubierta de agua, el agua no tendría la misma altura en todas partes.
Esta es la razón por la cual muchos receptores GPS de mano tienen tablas de búsqueda de ondulación incorporadas para determinar la altura sobre el nivel del mar.
Los primeros productos basados en datos de satélite GOCE estuvieron disponibles en línea en junio de 2010, a través de las herramientas de servicios de usuario de observación de la Tierra de la Agencia Espacial Europea (ESA).
El geoide es una superficie equipotencial particular, y está algo involucrado para calcular.
Un globo terráqueo es un modelo esférico de la Tierra, de algún otro cuerpo celeste o de la esfera celeste.
Un globo modelo de la esfera celeste se llama globo celeste.
Podría mostrar naciones y ciudades principales y la red de líneas de latitud y longitud.
Por lo general, también dividirá la esfera celeste en constelaciones.
La primera mención conocida de un globo es de Estrabón, que describe el Globo de las cajas de alrededor de 150 aC.
Muchos globos están hechos con una circunferencia de un metro, por lo que son modelos de la Tierra a una escala de 1:40 millones.
La mayoría de los globos modernos también están impresos con paralelos y meridianos, de modo que uno puede decir las coordenadas aproximadas de una ubicación específica.
Los primeros globos terrestres que representan la totalidad del Viejo Mundo fueron construidos en el mundo islámico.
Behaim era un cartógrafo, navegante y comerciante alemán.
Antes de construir el globo, Behaim había viajado extensamente.
Otro globo temprano, el Hunt-Lenox Globe, ca.
Puede ser el globo terráqueo más antiguo que muestre el Nuevo Mundo.
Un globo facsímil que muestra América fue hecho por Martin Waldseemueller en 1507.
Globus IMP, los dispositivos electromecánicos incluso globos terráqueos de cinco pulgadas se han usado en naves espaciales soviéticas y rusas a partir de 1961 hasta 2002 como instrumentos de navegación.
Este método de la fabricación del globo fue ilustrado en 1802 en un grabado en La Enciclopedia inglesa por George Kearsley.
Esto se coloca en una máquina que moldea el disco en una forma semiesférica.
Estos globos eran "enormes" y muy costosos.
Este último tiene un agujero de bala soviético a través de Alemania.
Un gran círculo, también conocido como un ortódromo, de una esfera es la intersección de la esfera y un plano que pasa a través del punto central de la esfera.
Este caso especial de un círculo de una esfera está en oposición a un pequeño círculo, es decir, la intersección de la esfera y un plano que no pasa por el centro.
La excepción es un par de puntos antípodas, para los cuales hay infinitos círculos grandes.
La longitud del arco menor de un gran círculo se toma como la distancia entre dos puntos en una superficie de una esfera en la geometría de Riemann donde tales grandes círculos se llaman círculos de Riemann.
Otro gran círculo es el que divide los hemisferios tierra y agua.
En cartografía, una proyección de mapa es una forma de aplanar la superficie de un globo terráqueo en un plano para hacer un mapa.
Dependiendo del propósito del mapa, algunas distorsiones son aceptables y otras no; por lo tanto, existen diferentes proyecciones del mapa para preservar algunas propiedades del cuerpo similar a una esfera a expensas de otras propiedades.
Las proyecciones son un tema de varios campos matemáticos puros, incluyendo geometría diferencial, geometría proyectiva y colectores.
Por el contrario, cualquier función matemática que transforma las coordenadas de la superficie curva de manera distinta y suave al plano es una proyección.
La Tierra y otros cuerpos celestes grandes generalmente están mejor modelados como esferoides achatados, mientras que los objetos pequeños como los asteroides a menudo tienen formas irregulares.
Debido a que la superficie curva de la Tierra no es isométrica a un plano, la preservación de las formas conduce inevitablemente a una escala variable y, en consecuencia, a una presentación no proporcional de las áreas.
El propósito del mapa determina qué proyección debe formar la base para el mapa.
Los conjuntos de datos son información geográfica; su recopilación depende del dato (modelo) elegido de la Tierra.
Al igual que la indicatrix de Tissot, la indicatrix de Goldberg-Gott se basa en infinitesimales, y representa distorsiones de flexión y asimetría (flexión y desviación).
A veces se utilizan triángulos esféricos.
Otra forma de visualizar la distorsión local es a través de gradaciones de escala de grises o color cuyo tono representa la magnitud de la deformación angular o inflación de área.
Debido a que la forma real de la Tierra es irregular, la información se pierde en este paso.
Para comparar, uno no puede aplanar una cáscara de naranja sin rasgarla y deformarla.)
Tangente significa que la superficie toca pero no corta a través del globo; secante significa que la superficie corta a través del globo.
Si estas líneas son un paralelo de latitud, como en las proyecciones cónicas, se llama paralelo estándar.
Esto se aplica a cualquier proyección cilíndrica o pseudocilíndrica en un aspecto normal.
La escala es constante a lo largo de todas las líneas rectas que irradian desde una ubicación geográfica particular.
Ya sean esféricos o elipsoidales, los principios discutidos se mantienen sin pérdida de generalidad.
El modelo elipsoidal se usa comúnmente para construir mapas topográficos y para otros mapas a gran y mediana escala que necesitan representar con precisión la superficie terrestre.
En comparación con el elipsoide de mejor ajuste, un modelo geoidal cambiaría la caracterización de propiedades importantes como la distancia, la conformidad y la equivalencia.
Sin embargo, para cuerpos planetarios irregulares como los asteroides, a veces se utilizan modelos análogos al geoide para proyectar mapas desde allí.
Las proyecciones se describen en términos de colocar una superficie gigantesca en contacto con la Tierra, seguida de una operación de escala implícita.
Donde la fuente de luz emana a lo largo de la línea descrita en esta última restricción es lo que produce las diferencias entre las diversas proyecciones cilíndricas "naturales".
Este cilindro se envuelve alrededor de la Tierra, se proyecta y luego se desenrolla.
Distancias norte-sur no estiradas ni comprimidas (1): proyección equirectangular o "plate carrée".
Dado que esta proyección escala las distancias de norte a sur por el recíproco de estiramiento este-oeste, conserva el área a expensas de las formas.
Otros meridianos son más largos que el meridiano central y se inclinan hacia afuera, lejos del meridiano central.
Por lo tanto, los meridianos están igualmente espaciados a lo largo de un paralelo dado.
El mapa cónico resultante tiene una baja distorsión en escala, forma y área cerca de esos paralelos estándar.
Se puede construir desde un punto de vista de una distancia infinita desde el punto tangente; r(d) c pecado .
Proyección en perspectiva casi lateral, que simula la vista desde el espacio a una distancia finita y, por lo tanto, muestra menos de un hemisferio completo, como se usa en The Blue Marble 2012).
El punto o puntos especiales pueden estirarse en una línea o segmento curvo cuando se proyectan.
Azimutal equidistante: Se conservan las distancias desde el centro y el borde.
Por lo tanto, existen muchas proyecciones para servir a los muchos usos de los mapas y su amplia gama de escalas.
Los mapas de referencia del mundo a menudo aparecen en las proyecciones de compromiso.
La proyección Mercator es un mapa cilíndrico presentado por el geógrafo y cartógrafo flamenco Gerardus Mercator en 1569.
Como efecto secundario, la proyección Mercator infla el tamaño de los objetos lejos del ecuador.
Sin embargo, dada la geometría de un reloj de sol, estos mapas pueden haberse basado en la proyección cilíndrica central similar, un caso limitante de la proyección gnomónica, que es la base de un reloj de sol.
Sin embargo, este fue un caso simple y común de identificación errónea.
Mercator tituló el mapa: "Una descripción nueva y aumentada de la Tierra corregida para el uso de marineros".
Se han presentado varias hipótesis a lo largo de los años, pero en cualquier caso la amistad de Mercator con Pedro Nunes y su acceso a las mesas loxodrómicas que Nunes creó probablemente ayudaron a sus esfuerzos.
Sin embargo, las matemáticas involucradas fueron desarrolladas pero nunca publicadas por el matemático Thomas Harriot a partir de 1589.
Dos problemas principales impidieron su aplicación inmediata: la imposibilidad de determinar la longitud en el mar con la precisión adecuada y el hecho de que las direcciones magnéticas, en lugar de las direcciones geográficas, se utilizaron en la navegación.
Sin embargo, no comenzó a dominar los mapas mundiales hasta el siglo XIX, cuando el problema de la determinación de la posición se había resuelto en gran medida.
Debido a estas presiones, los editores redujeron gradualmente su uso de la proyección a lo largo del siglo XX.
Al lograr esto, el inevitable estiramiento este-oeste del mapa, que aumenta a medida que aumenta la distancia desde el ecuador, está acompañado en la proyección de Mercator por un correspondiente estiramiento norte-sur, de modo que en cada punto la escala este-oeste es la misma que la escala norte-sur, por lo que es una proyección de mapa conforme.
En latitudes superiores a 70° norte o sur, la proyección de Mercator es prácticamente inutilizable, porque la escala lineal se vuelve infinitamente grande en los polos.
La isla de Ellesmere en el norte del archipiélago ártico de Canadá se ve aproximadamente del mismo tamaño que Australia, aunque Australia es más de 39 veces más grande.
La superficie real de Groenlandia es comparable a la de la República Democrática del Congo.
Alaska parece ser del mismo tamaño que Australia, aunque Australia es en realidad 4 1/2 veces más grande.
Suecia es mucho más grande que Madagascar.
Un mapa del mundo en un icosaedro regular por proyección gnomónica.
Como resultado de estas críticas, los atlas modernos ya no usan la proyección Mercator para mapas mundiales o para áreas distantes del ecuador, prefiriendo otras proyecciones cilíndricas o formas de proyección de igual área.
Arno Peters provocó controversia a partir de 1972 cuando propuso lo que ahora se suele llamar la proyección Gall-Peters para remediar los problemas del Mercator, alegando que era su propia obra original sin hacer referencia a trabajos anteriores de cartógrafos como el trabajo de Gall de 1855.
El rango para una de las opciones posibles es de aproximadamente 35 km, pero para aplicaciones a pequeña escala (región grande) esta variación puede ignorarse, y pueden tomarse valores medios de 6.371 km y 40.030 km para el radio y la circunferencia respectivamente.
Una proyección del mapa cilíndrica es especificada por fórmulas que unen las coordenadas geográficas de latitud y longitud a coordenadas Cartesianas en el mapa con el origen en el ecuador y eje x a lo largo del ecuador.
Dado que el cilindro es tangencial al globo en el ecuador, el factor de escala entre el globo y el cilindro es la unidad en el ecuador, pero en ningún otro lugar.
La diferencia (o) es en radianes.
Se han utilizado truncamientos aún más extremos: un atlas escolar finlandés se truncó a aproximadamente 76 ° N y 56 ° S, una relación de aspecto de 1,97.
Las tiras más estrechas son mejores: sec 8 ° x 1.01, por lo que una tira de 16 ° de ancho (centrado en el ecuador) es precisa dentro del 1% o 1 parte en 100.
El valor de e2 es de aproximadamente 0,006 para todos los elipsoides de referencia).
Para el modelo anterior 1 cm corresponde a 1.500 km a una latitud de 60 °.
Esta cuerda subtiende un ángulo en el centro igual a 2arcsin (cos +- sen) y la distancia del gran círculo entre A y B es 2a arcsin (cos +- sen).
Para otros cuerpos generalmente se hace referencia a una característica de superficie fija, que para Marte es el meridiano que pasa a través del cráter Airy-0.
Por convención para la Tierra, la Luna y el Sol se expresa en grados que van desde -180 ° a +180 ° Para otros cuerpos se utiliza un rango de 0 ° a 360 °.
La escala de un mapa es la relación de una distancia en el mapa a la distancia correspondiente en el suelo.
La primera forma es la relación entre el tamaño del globo generador y el tamaño de la Tierra.
Muchos mapas indican la escala nominal e incluso pueden mostrar una escala de barras (a veces simplemente llamada "escala") para representarla.
En este caso, "escala" significa el factor de escala (escala de puntos - escala particular).
La proyección del mapa se vuelve crítica para comprender cómo varía la escala a lo largo del mapa.
Este es un estudio de prácticamente todas las proyecciones conocidas desde la antig-edad hasta 1993.
La pequeña escala se refiere a mapas del mundo o mapas de regiones grandes como continentes o naciones grandes.
Los mapas a gran escala muestran áreas más pequeñas con más detalle, como los mapas del condado o los planes de la ciudad.
Sin embargo, como se explicó anteriormente, los cartógrafos usan el término "a gran escala" para referirse a mapas menos extensos, aquellos que muestran un área más pequeña.
Esto se ilustra comúnmente por la imposibilidad de alisar una cáscara de naranja sobre una superficie plana sin rasgarla y deformarla.
A la inversa, los factores de escala isotrópicos en el mapa implican una proyección conforme.
La calificación "pequeño" significa que con una cierta precisión de medición no se puede detectar ningún cambio en el factor de escala sobre el elemento.
Decimos que estas coordenadas definen el mapa de proyección que debe distinguirse lógicamente de los mapas impresos (o vistos) reales.
Dado que la escala de puntos varía con la posición y la dirección, la proyección del círculo en la proyección se distorsionará.
La superposición de estas elipses de distorsión en la proyección del mapa transmite la forma en que la escala de puntos está cambiando sobre el mapa.
La relación entre el eje mayor y el eje menor es .
La escala es verdadera (k-1) en el ecuador, por lo que multiplicar su longitud en un mapa impreso por la inversa de la RF (o escala principal) da la circunferencia real de la Tierra.
La gráfica superior muestra la función isotrópica de la escala de Mercator: la escala en el paralelo es la misma que la escala en el meridiano.
Por lo tanto, la proyección de Mercator tangente es altamente precisa dentro de una franja de 3,24 grados de ancho centrada en el ecuador.
Estas observaciones impulsaron el desarrollo de las proyecciones transversales de Mercator en las que un meridiano se trata "como un ecuador" de la proyección para que obtengamos un mapa preciso dentro de una distancia estrecha de ese meridiano.
Las cuatro direcciones cardinales, o puntos cardinales, son las cuatro direcciones principales de la brújula: norte, este, sur y oeste, comúnmente denotadas por sus iniciales N, E, S y W respectivamente.
Cuando se viaja hacia el este u oeste, es solo en el Ecuador que se puede mantener el este u oeste y seguir recto (sin la necesidad de dirigir).
El polo norte de la aguja magnética apunta hacia el polo norte geográfico de la tierra y viceversa.
En el medio del día, está al sur para los espectadores en el hemisferio norte, que viven al norte del Trópico de Cáncer, y al norte para los del hemisferio sur, que viven al sur del Trópico de Capricornio.
En estos lugares, uno necesita primero determinar si el sol se mueve de este a oeste a través del norte o del sur observando sus movimientos: de izquierda a derecha significa que atraviesa el sur, mientras que de derecha a izquierda significa que atraviesa el norte; o uno puede observar las sombras del sol.
Debido a la inclinación axial de la Tierra, no importa cuál sea la ubicación del espectador, solo hay dos días cada año cuando el sol sale precisamente hacia el este.
Para que este método funcione en el hemisferio sur, el 12 apunta hacia el Sol y el punto a medio camino entre la manecilla de las horas y las 12 en punto indicará el norte.
Este eje se cruza con la Esfera Celestial en los polos Celestiales Norte y Sur, que parecen estar directamente por encima del Norte y del Sur respectivamente en el horizonte.
La fotografía resultante revela una multitud de arcos concéntricos (porciones de círculos perfectos) de los que se puede derivar fácilmente el centro exacto, y que corresponde al polo celeste, que se encuentra directamente sobre la posición del polo verdadero (Norte o Sur) en el horizonte.
La posición exacta del polo cambia durante miles de años debido a la precesión de los equinoccios.
El asterismo "Big Dipper" se puede utilizar para encontrar Polaris.
Dado que encuentra el norte verdadero, en lugar de magnético, es inmune a la interferencia de los campos magnéticos locales o a bordo.
La mayoría de los mapas de la Europa medieval, por ejemplo, se colocan al este (E) en la parte superior.
Los mapas topográficos incluyen la elevación, típicamente a través de líneas de contorno.
El punto norte será entonces el punto en la extremidad que está más cerca del polo norte celeste.
Girando alrededor del disco en el sentido de las agujas del reloj desde el punto norte, uno se encuentra en orden con el punto oeste, el punto sur y luego el punto este.
En la Europa premoderna en general, entre ocho y 32 puntos de la brújula, direcciones cardinales e intercardinales, recibieron nombres.
Los sistemas con cinco puntos cardinales (cuatro direcciones y el centro) incluyen los de Chína premoderna, así como las culturas tradicionales turcas, tibetanas y ainu.
Algunos también pueden incluir "arriba" y "abajo" como direcciones, y por lo tanto se centran en una cosmología de siete direcciones.
El norte está asociado con el Himalaya y el cielo, mientras que el sur está asociado con el inframundo o la tierra de los padres (Pitr loka).
Norte es uno de los cuatro puntos de la brújula o direcciones cardinales.
Septentrionalis es de septentriones, "los siete bueyes arados", un nombre de la Osa Mayor.
Por ejemplo, en Lezgian, Kefer puede significar tanto "incredulidad" como "norte", ya que al norte de la patria de Lezgian musulmana hay áreas antes habitadas por pueblos caucásicos y turcos no musulmanes.
En cualquier objeto astronómico giratorio, el norte a menudo denota el lado que parece girar en sentido contrario a las agujas del reloj cuando se ve desde lejos a lo largo del eje de rotación.
Pero las generalizaciones simples sobre el tema deben ser tratadas como poco sólidas, y como susceptibles de reflejar conceptos erróneos populares sobre el magnetismo terrestre.
Esta convención se ha desarrollado a partir del uso de una brújula, que coloca el norte en la parte superior.
El 95% del Norte Global tiene suficiente comida y refugio, y un sistema educativo que funciona.
El uso del término "Sur" también puede ser relativo al país, particularmente en casos de división económica o cultural notable.
Rara vez el significado se amplía a Bolivia, y en el sentido más restringido solo cubre Chile, Argentina y Uruguay.
Oeste es la dirección opuesta a la de la rotación de la Tierra sobre su eje, y por lo tanto es la dirección general hacia la que el Sol parece progresar constantemente y eventualmente establecerse.
En el antiguo Egipto, se consideraba que Occidente era el portal al inframundo, y es la dirección cardinal considerada en relación con la muerte, aunque no siempre con una connotación negativa.
En el judaísmo, se ve que el oeste está hacia la Shekinah (presencia) de Dios, como en la historia judía, el Tabernáculo y el posterior Templo de Jerusalén se enfrentaron al este, con la Presencia de Dios en el Lugar Santísimo subiendo los escalones hacia el oeste.
El Círculo Polar Ártico es uno de los dos círculos polares y el más al norte de los cinco círculos principales de latitud como se muestra en los mapas de la Tierra.
Un círculo de latitud o línea de latitud en la Tierra es un pequeño círculo abstracto de este a oeste que conecta todas las ubicaciones alrededor de la Tierra (ignorando la elevación) en una línea de coordenadas de latitud dada.
Los círculos de latitud son diferentes a los círculos de longitud, que son todos grandes círculos con el centro de la Tierra en el medio, ya que los círculos de latitud se hacen más pequeños a medida que aumenta la distancia desde el ecuador.
Un círculo de latitud es perpendicular a todos los meridianos.
El ecuador es el círculo de latitud más largo y es el único círculo de latitud que también es un gran círculo.
En un mapa, los círculos de latitud pueden o no ser paralelos, y su espaciamiento puede variar, dependiendo de qué proyección se use para mapear la superficie de la Tierra en un plano.
Por ejemplo, en una proyección de Mercator los círculos de latitud están más espaciados cerca de los polos para preservar las escalas y formas locales, mientras que en una proyección de Gall-Peters los círculos de latitud están espaciados más cerca de los polos para que las comparaciones de área sean precisas.
Hay muchos términos más pequeños, lo que resulta en cambios diarios variables de algunos metros en cualquier dirección.
54°40'N La frontera entre los territorios rusos del siglo XIX al norte y las reclamaciones de tierras estadounidenses y británicas en el oeste de América del Norte.
43°30'N En los Estados Unidos, la frontera entre Minnesota y Iowa.
42°N Originalmente el límite norte de la Nueva España.
41°N En los Estados Unidos, parte de la frontera entre Wyoming y Utah, la frontera entre Wyoming y Colorado, y parte de la frontera entre Nebraska y Colorado.
El límite entre las zonas de ocupación soviética y estadounidense en Corea, y más tarde entre Corea del Norte y Corea del Sur, desde 1945 hasta la Guerra de Corea (1950-1953).
Geográficamente es una extensión hacia el oeste de la frontera entre Virginia y Carolina del Norte y parte de la frontera entre Kentucky y Tennessee.
También, parte de la frontera entre Carolina del Norte y Georgia.
32°N En los Estados Unidos, parte de la frontera entre Nuevo México y Texas.
25°N Parte de la frontera entre Mauritania y Malí.
17°N La división entre la República de Vietnam (Vietnam del Sur) y la República Democrática de Vietnam (Vietnam del Norte) durante la Guerra de Vietnam.
8°N Parte de la frontera entre Somalia y Etiopía.
7°S Una sección corta de la frontera entre la República Democrática del Congo y Angola.
Las artes son una gama muy amplia de prácticas humanas de expresión creativa, narración y participación cultural.
Pueden emplear la habilidad y la imaginación para producir objetos, actuaciones, transmitir ideas y experiencias, y construir nuevos entornos y espacios.
También pueden desarrollar o contribuir a algún aspecto particular de una forma de arte más compleja, como en la cinematografía.
El primer significado de la palabra arte es «modo de hacer».
En su definición abstracta más básica, el arte es una expresión documentada de un ser sensible a través de un medio accesible para que cualquiera pueda verlo, escucharlo o experimentarlo.
Tal calificación pública depende de varios factores subjetivos.
En la antigua Grecia, todo arte y artesanía era referido por la misma palabra, techne.
El arte romano antiguo representaba a los dioses como humanos idealizados, mostrados con rasgos distintivos característicos (por ejemplo, el rayo de Zeus).
Una característica de este estilo es que el color local a menudo se define por un contorno (un equivalente contemporáneo es la caricatura).
En la academia moderna, las artes generalmente se agrupan con o como un subconjunto de las humanidades.
La palabra arquitectura proviene del griego arkhitekton, "maestro constructor, director de obras", de ?- (arkhi) "jefe" + ? (tekton) "constructor, carpintero".
En el uso moderno, la arquitectura es el arte y la disciplina de crear o inferir un plan implícito o aparente de un objeto o sistema complejo.
La arquitectura planificada manipula el espacio, el volumen, la textura, la luz, la sombra o los elementos abstractos para lograr una estética agradable.
Si bien algunos productos cerámicos se consideran obras de arte, algunos se consideran objetos decorativos, industriales o de arte aplicado.
En una fábrica de cerámica o cerámica, un grupo de personas diseña, fabrica y decora la cerámica.
Por lo general, implica hacer marcas en una superficie mediante la aplicación de presión de una herramienta, o mover una herramienta a través de una superficie.
Las principales técnicas utilizadas en el dibujo son el dibujo lineal, el sombreado, el rayado cruzado, el rayado aleatorio, el rayado, el punteado y la mezcla.
Las pinturas pueden ser naturalistas y representativas (como en una naturaleza muerta o pintura de paisajes), fotográficas, abstractas, narrativas, simbolistas (como en el arte simbolista), emotivas (como en el expresionismo) o de naturaleza política (como en el artivismo).
El sustantivo "literatura" proviene de la palabra latina littera que significa "un carácter escrito individual (letra)".
Cada disciplina en las artes escénicas es de naturaleza temporal, lo que significa que el producto se realiza durante un período de tiempo.
La danza también se usa para describir métodos de comunicación no verbal (ver lenguaje corporal) entre humanos o animales (por ejemplo, danza de abejas, danza de apareamiento), movimiento en objetos inanimados (por ejemplo, las hojas bailadas en el viento) y ciertas formas o géneros musicales.
La creación, el rendimiento, el significado e incluso la definición de música varían según la cultura y el contexto social.
El compositor Richard Wagner reconoció la fusión de tantas disciplinas en una sola obra de ópera, ejemplificada por su ciclo Der Ring des Nibelungen ("El Anillo del Nibelungo").
Otras obras de finales de los siglos XIX, XX y XXI han fusionado otras disciplinas de maneras únicas y creativas, como el arte de performance.
John Cage es considerado por muchos como un artista de performance en lugar de un compositor, aunque prefirió este último término.
Las artes aplicadas incluyen campos como el diseño industrial, la ilustración y el arte comercial.
Dentro de las ciencias sociales, los economistas culturales muestran cómo los videojuegos conducen a la participación en formas de arte y prácticas culturales más tradicionales, lo que sugiere la complementariedad entre los videojuegos y las artes.
La arquitectura (del latín architectura, "arquitecto", del griego arkhitekton, "jefe" y "creador") es tanto el proceso como el producto de la planificación, el diseño y la construcción de edificios u otras estructuras.
La práctica, que comenzó en la era prehistórica, se ha utilizado como una forma de expresar la cultura para las civilizaciones en los siete continentes.
En el siglo XIX, Louis Sullivan declaró que "la forma sigue a la función".
La arquitectura comenzó como una arquitectura vernácula oral y rural que se desarrolló desde el ensayo y el error hasta la replicación exitosa.
Durante la Edad media europea, los estilos paneuropeos de catedrales románicas y góticas y abadías surgieron mientras el Renacimiento favoreció formas Clásicas puestas en práctica por arquitectos conocidos por el nombre.
Se hizo hincapié en técnicas modernas, materiales y formas geométricas simplificadas, allanando el camino para superestructuras de gran altura.
Una forma o estructura unificadora o coherente.
El aspecto más importante de la belleza era, por lo tanto, una parte inherente de un objeto, en lugar de algo aplicado superficialmente, y se basaba en verdades universales y reconocibles.
En el siglo XVI, el arquitecto, pintor y teórico manierista italiano Sebastiano Serlio escribió Tutte L'Opere D'Architettura et Prospetiva (Obras completas sobre arquitectura y perspectiva).
La arquitectura gótica, según Pugin, era la única "verdadera forma cristiana de arquitectura".
Entre las filosofías que han influido en los arquitectos modernos y su enfoque del diseño de edificios están el racionalismo, el empirismo, el estructuralismo, el postestructuralismo, la deconstrucción y la fenomenología.
La arquitectura y el urbanismo de las civilizaciones clásicas como la griega y la romana evolucionaron a partir de ideales cívicos en lugar de religiosos o empíricos y surgieron nuevos tipos de edificios.
Los textos sobre arquitectura se han escrito desde tiempos antiguos.
La arquitectura budista, en particular, mostró una gran diversidad regional.
El papel del arquitecto era por lo general uno con ese del maestro albañil o Magister lathomorum ya que a veces se describen en documentos contemporáneos.
Los edificios fueron atribuidos a arquitectos específicos – Brunelleschi, Alberti, Michelangelo, Palladio – y el culto del individuo había comenzado.
La formación arquitectónica formal en el siglo XIX, por ejemplo en la École des Beaux-Arts en Francia, dio mucho énfasis a la producción de hermosos dibujos y poco al contexto y la viabilidad.
Entre ellos destaca el Deutscher Werkbund, formado en 1907 para producir objetos hechos a máquina de mejor calidad.
Cuando la arquitectura moderna se practicó por primera vez, era un movimiento de vanguardia con fundamentos morales, filosóficos y estéticos.
El enfoque de los arquitectos modernistas fue reducir los edificios a formas puras, eliminando referencias históricas y ornamentos en favor de los detalles funcionales.
Arquitectos como Mies van der Rohe, Philip Johnson y Marcel Breuer trabajaron para crear belleza basada en las cualidades inherentes de los materiales de construcción y las técnicas de construcción modernas, intercambiando formas históricas tradicionales por formas geométricas simplificadas, celebrando los nuevos medios y métodos hechos posibles por la Revolución Industrial, incluida la construcción con estructura de acero, que dio origen a superestructuras de gran altura.
Los procesos preparatorios para el diseño de cualquier edificio grande se han vuelto cada vez más complicados, y requieren estudios preliminares de cuestiones tales como la durabilidad, la sostenibilidad, la calidad, el dinero y el cumplimiento de las leyes locales.
La sostenibilidad ambiental se ha convertido en un tema principal, con un profundo efecto en la profesión arquitectónica.
Este cambio importante en la arquitectura también ha cambiado las escuelas de arquitectura para centrarse más en el medio ambiente.
El sistema de calificación LEED (Liderazgo en Energía y Diseño Ambiental) del Consejo de Construcción Verde de los Estados Unidos ha sido fundamental en esto.
También puede ser el diseño inicial y el plan de uso, luego rediseñado para acomodar un propósito cambiado, o un diseño revisado significativamente para la reutilización adaptativa de la carcasa del edificio.
El diseño preliminar del buque, su diseño detallado, construcción, pruebas, operación y mantenimiento, lanzamiento y dique seco son las principales actividades involucradas.
Por el contrario, la arquitectura sagrada como lugar para la meta-intimidad también puede ser no monolítica, efímera e intensamente privada, personal y no pública.
Con el surgimiento del cristianismo y el Islam, los edificios religiosos se convirtieron cada vez más en centros de adoración, oración y meditación.
India fue atravesada por rutas comerciales de comerciantes de lugares tan lejanos como Siraf y China, así como por invasiones de extranjeros, lo que resultó en múltiples influencias de elementos extranjeros en los estilos nativos.
Un ejemplo está en Nalanda (Bihar).
De acuerdo con los cambios en la práctica religiosa, las estupas se incorporaron gradualmente en chaitya-grihas (salas de estupas).
Los templos budistas se desarrollaron más tarde y fuera del sur de Asia, donde el budismo disminuyó gradualmente desde los primeros siglos CE en adelante, aunque un ejemplo temprano es el del Templo Mahabodhi en Bodh Gaya en Bihar.
En la creencia hindú, el templo representa el macrocosmos del universo, así como el microcosmos del espacio interior.
Evolucionó durante un período de más de 2000 años.
Además, el ladrillo reemplazó a la piedra, el orden clásico se observó menos estrictamente, los mosaicos reemplazaron la decoración tallada y se erigieron cúpulas complejas.
Los estilos más tempranos en la arquitectura islámica produjeron 'el plan árabe' o mezquitas hipóstilas durante la Dinastía de Umayyad.
En las mezquitas iwan, uno o más iwans se enfrentan a un patio central que sirve como sala de oración.
La parte superior del minarete es siempre el punto más alto en las mezquitas que tienen uno, y a menudo el punto más alto en el área inmediata.
En consecuencia, los arquitectos de la mezquita tomaron prestada la forma del campanario para sus minaretes, que se utilizaron esencialmente para el mismo propósito: llamar a los fieles a la oración.
Aunque las cúpulas normalmente tomaban la forma de un hemisferio, los mogoles en la India popularizaron las cúpulas en forma de cebolla en el sur de Asia y Persia.
Por lo general, frente a la entrada de la sala de oración se encuentra la pared de la qibla, que es el área visualmente enfatizada dentro de la sala de oración.
En la pared de la qibla, generalmente en su centro, está el mihrab, un nicho o depresión que indica la pared de la qibla.
El mihrab sirve como el lugar donde el imán dirige las cinco oraciones diarias sobre una base regular.
Consiste en una nave, transeptos, y el altar está de pie en el extremo este (ver el diagrama de la catedral).
La mayor parte de historiadores arquitectónicos consideran el diseño de Michelangelo de la Basílica de San Pedro en Roma como un precursor al estilo Barroco; esto puede ser reconocido por espacios interiores más amplios (sustituyendo naves estrechas largas), la atención más juguetona a luz y sombra, ornamentación extensa, frescos grandes, foco en el arte interior, y a menudo, una proyección exterior central dramática.
Mientras las estructuras seculares claramente tenían la mayor influencia en el desarrollo de la arquitectura moderna, varios ejemplos excelentes de la arquitectura moderna se pueden encontrar en edificios religiosos del 20mo siglo.
Se ha descrito como una "falange de luchadores" vueltos en sus colas y apuntando hacia el cielo.
El Templo de Independence, Missouri fue concebido por el arquitecto japonés Gyo Obata después del concepto del nautilus de cámara.
Por otro lado, la Basílica de Nuestra Señora del Lichi es un edificio mucho más tradicional.
Un estilo arquitectónico es un conjunto de características y rasgos que hacen que un edificio u otra estructura sea notable o históricamente identificable.
La mayoría de la arquitectura puede clasificarse dentro de una cronología de estilos que cambia con el tiempo reflejando modas, creencias y religiones cambiantes, o la aparición de nuevas ideas, tecnología o materiales que hacen posibles nuevos estilos.
En cualquier momento, varios estilos pueden estar de moda, y cuando un estilo cambia, generalmente lo hace gradualmente, a medida que los arquitectos aprenden y se adaptan a las nuevas ideas.
Por ejemplo, las ideas del Renacimiento surgieron en Italia alrededor de 1425 y se extendieron a toda Europa durante los próximos 200 años, con los renacimientos francés, alemán, inglés y español mostrando reconociblemente el mismo estilo, pero con características únicas.
Después de que un estilo arquitectónico ha pasado de moda, pueden ocurrir reavivamientos y reinterpretaciones.
El estilo español de la misión se reanimó 100 años más tarde como el Renacimiento de la Misión, y esto pronto evolucionó en el Renacimiento Colonial español.
Un ejemplo de arquitectura manierista es la Villa Farnese en Caprarola en el campo accidentado fuera de Roma.
A través de Amberes, los estilos renacentista y manierista se introdujeron ampliamente en Inglaterra, Alemania y el norte y el este de Europa en general.
El ideal renacentista de la armonía dio paso a ritmos más libres y más imaginativos.
La teoría arquitectónica es el acto de pensar, discutir y escribir sobre la arquitectura.
La teoría arquitectónica es a menudo didáctica, y los teóricos tienden a permanecer cerca o trabajar desde dentro de las escuelas.
Esto no quiere decir, sin embargo, que tales obras no existieran, dado que muchas obras nunca sobrevivieron a la antiguedad.
Probablemente escrito entre 27 y 23 aC, es la única fuente contemporánea importante en la arquitectura clásica que ha sobrevivido.
También propone las tres leyes fundamentales que la arquitectura debe obedecer, para ser considerada así: firmitas, utilitas, venustas, traducidas en el siglo XVII por Sir Henry Wotton en el eslogan inglés firmeza, mercancía y deleite (lo que significa adecuación estructural, adecuación funcional y belleza).
Ya que las teorías arquitectónicas estaban en estructuras, menos de ellos se transcribieron.
Estas teorías anticiparon el desarrollo del funcionalismo en la arquitectura moderna.
Esto a su vez formó la base para el Art Nouveau en el Reino Unido, ejemplificado por el trabajo de Charles Rennie Mackintosh, e influyó en la Secesión de Viena.
La generación nacida durante el tercio medio del siglo XIX estaba en gran medida cautivada con las oportunidades presentadas por la combinación de Semper de un alcance histórico impresionante y una granularidad metodológica.
El Movimiento Moderno rechazó estos pensamientos y Le Corbusier rechazó enérgicamente la obra.
Otro teórico de la planificación influyente de este tiempo era Ebenezer Howard, que fundó el movimiento de la ciudad jardín.
Un uso temprano del término la arquitectura moderna en la letra ocurrió en el título de un libro por Otto Wagner, que dio ejemplos de su propio representante del trabajo de la Secesión de Viena con ilustraciones del art nouveau y enseñanzas didácticas a sus estudiantes.
Frank Lloyd Wright, mientras moderno en el rechazo del renacimiento histórico, era idiosincrásico en su teoría, que transmitió en la escritura copiosa.
Wright era más poético y mantuvo firmemente la visión del siglo XIX del artista creativo como un genio único.
Este también ha sido el caso de educadores de la academia como Dalibor Vesely o Alberto-Perez Gómez, y en años más recientes esta orientación filosófica se ha reforzado a través de la investigación de una nueva generación de teóricos (E.G. Jeffrey Kipnis o Sanford Kwinter).
Otros, como Beatriz Colomina y Mary McLeod, amplían la comprensión histórica de la arquitectura para incluir discursos menores o menores que han influido en el desarrollo de las ideas arquitectónicas a lo largo del tiempo.
En sus teorías, la arquitectura se compara con un lenguaje que puede ser inventado y reinventado cada vez que se utiliza.
Desde el año 2000, la teoría arquitectónica también ha tenido que enfrentar el rápido aumento del urbanismo y la globalización.
En la última década, ha habido el surgimiento de la llamada Arquitectura "Digital".
Los arquitectos también diseñan edificios de aspecto orgánico en el intento de desarrollar un nuevo lenguaje formal.
Desde que surgieron estas nuevas tendencias arquitectónicas, muchos teóricos y arquitectos han estado trabajando en estos temas, desarrollando teorías e ideas como el Parametricismo de Patrick Schumacher.
La arquitectura bizantina es la arquitectura del Imperio Bizantino, o Imperio Romano de Oriente.
Magníficos mosaicos dorados con su simplicidad gráfica trajeron luz y calidez al corazón de las iglesias.
Algunas de las columnas también estaban hechas de mármol.
Muebles de madera preciosa, como camas, sillas, taburetes, mesas, estanterías y copas de plata o oro con hermosos relieves, decorados interiores bizantinos.
Para los templos clásicos, solo el exterior era importante, porque solo los sacerdotes entraban al interior, donde se guardaba la estatua de la deidad a la que estaba dedicado el templo.
Aquellos en la Catedral de San Marcos, Venecia (1071) atrajeron especialmente la fantasía de John Ruskin.
En las columnas orientales, el águila, el león y el cordero son ocasionalmente tallados, pero tratados convencionalmente.
Las columnas compuestas alinean el espacio principal de la nave.
Las columnas están llenas de follaje en todo tipo de variaciones.
Otras estructuras incluyen las ruinas del Gran Palacio de Constantinopla, las innovadoras murallas de Constantinopla (con 192 torres) y la Cisterna Basílica (con cientos de columnas clásicas recicladas).
El período Paleólogo está bien representado en una docena de antiguas iglesias en Estambul, notablemente San Salvador en Chora y Santa María Pammakaristos.
La Iglesia de los Santos Apóstoles (Tesalónica) se cita como una estructura arquetípica de finales del período con sus paredes exteriores intrincadamente decoradas con complejos patrones de ladrillo o con cerámica esmaltada.
En San Sergio, Constantinopla, y San Vitale, Rávena, iglesias del tipo central, el espacio debajo de la cúpula se amplió al tener adiciones absidales hechas al octágono.
Esta área ininterrumpida, de aproximadamente 260 pies (80 m) de largo, la mayor parte de los cuales tiene más de 100 pies (30 m) de ancho, está completamente cubierta por un sistema de superficies domicas.
En los Santos Apóstoles (siglo VI) se aplicaron cinco cúpulas a un plan cruciforme; la cúpula central era la más alta.
A veces el espacio central era cuadrado, a veces octogonal, o al menos había ocho pilares que sostenían la cúpula en lugar de cuatro, y la nave y los transeptos eran más estrechos en proporción.
Todavía en frente de poner un patio cuadrado.
Directamente debajo del centro de la cúpula está el ambón, desde el cual se proclamaron las Escrituras, y debajo del ambón a nivel del piso estaba el lugar para el coro de cantantes.
Las filas de asientos crecientes alrededor de la curva del ábside con el trono del patriarca en el punto del oriente medio formaron el synthronon.
Las cúpulas y bóvedas al exterior estaban cubiertas con plomo o con azulejos de la variedad romana.
Hay considerables influencias bizantinas que se pueden detectar en los primeros monumentos islámicos distintivos en Siria (709-715).
Se usaron ladrillos de 70 cm x 35 cm x 5 cm, y estos ladrillos se pegaron juntos usando mortero de aproximadamente 5 cm de espesor.
Quizás la característica más definida de la Hagia Irene es el estricto contraste entre el diseño interior y exterior.
Este estilo influyó en la construcción de varios otros edificios, como la Basílica de San Pedro.
La construcción de la versión final de la Santa Sofía, que sigue en pie hoy en día, fue supervisada por el emperador Justiniano.
La arquitectura gótica (o arquitectura puntiaguda) es un estilo arquitectónico que fue particularmente popular en Europa desde finales del siglo XII hasta el siglo XVI, durante la Alta y Baja Edad Media, sobreviviendo en los siglos XVII y XVIII en algunas áreas.
El estilo en ese momento se conocía a veces como opus Francigenum (lit.
La principal innovación de ingeniería y uno de los otros componentes de diseño característicos es el contrafuerte volador.
Sin embargo, no hay evidencia que indique que hubo una conexión entre la arquitectura armenia y el desarrollo del estilo gótico en Europa occidental.
Así el estilo gótico, estando en la oposición a la arquitectura clásica, desde ese punto de vista se asoció con la destrucción de avance y sofisticación.
El término "Saracen" todavía estaba en el uso en el 18vo siglo y típicamente se refería a todos los conquistadores musulmanes, incluso los moros y árabes.
Su aversión al estilo era tan fuerte que se negó a poner un techo gótico en el nuevo St. Paul, a pesar de ser presionado para hacerlo.
Varios autores han adoptado una postura en contra de esta alegación, alegando que el estilo gótico probablemente se había filtrado en Europa de otras maneras, por ejemplo, a través de España o Sicilia.
También fue influenciado por doctrinas teológicas que requerían más luz y por mejoras técnicas en bóvedas y contrafuertes que permitían una altura mucho mayor y ventanas más grandes.
Las bóvedas de costillas se emplearon en algunas partes de la catedral en Durham (1093-) y en la Abadía de Lessay en Normandía (1098).
El Ducado de Normandía, parte del Imperio Angevino hasta el siglo XIII, desarrolló su propia versión del gótico.
Un ejemplo del gótico normando temprano es la catedral de Bayeux (1060-1070) donde la nave de la catedral románica y el coro se reconstruyeron en el estilo gótico.
La Catedral de Coutances se rehizo en el gótico que comienza aproximadamente 1220.
Suger reconstruyó porciones de la antigua iglesia románica con la bóveda de costilla para eliminar las paredes y hacer más espacio para las ventanas.
Además, instaló un rosetón circular sobre el portal en la fachada.
La Catedral de Durham fue la primera catedral en emplear una bóveda de costilla, construida entre 1093 y 1104.
Uno de los constructores que se cree que han trabajado en la Catedral de Sens, Guillermo de Sens, más tarde viajó a Inglaterra y se convirtió en el arquitecto que, entre 1175 y 1180, reconstruyó el coro de la Catedral de Canterbury en el nuevo estilo gótico.
Las iglesias góticas francesas fueron fuertemente influenciadas tanto por las capillas ambulatorias y laterales alrededor del coro en Saint-Denis, como por las torres emparejadas y las puertas triples en la fachada occidental.
Los constructores de Notre-Dame fueron más allá al introducir el contrafuerte volador, columnas pesadas de soporte fuera de las paredes conectadas por arcos a las paredes superiores.
Su trabajo fue continuado por Guillermo el inglés que sustituyó a su homónimo francés en 1178.
Los Tiercerons – costillas decorativas abovedadas – parecen haber sido utilizados primero en la bóveda en la Catedral de Lincoln, instalada c.1200.
El primer edificio en el alto gótico fue la Catedral de Chartres, una importante iglesia de peregrinación al sur de París.
Las paredes estaban llenas de vidrieras, principalmente representando la historia de la Virgen María, pero también, en un pequeño rincón de cada ventana, ilustrando las artesanías de los gremios que donaron esas ventanas.
En Europa central, el estilo gótico Alto apareció en el Sacro Imperio Romano, primero en Toul (1220'), cuya catedral románica se reconstruyó en el estilo de la Catedral de Reims; luego Liebfrauenkirche de Trier (1228'), y luego en todas partes del Reich, comenzando con Elisabethkirche en Marburg (1235') y la catedral en Metz (c.1235').
Las ventanas de la lanceta fueron suplantadas por múltiples luces separadas por tracería geométrica de barras.
Otras características del alto gótico fueron el desarrollo de rosetones de mayor tamaño, utilizando tracería de barras, contrafuertes voladores más altos y más largos, que podían llegar hasta las ventanas más altas, y paredes de escultura que ilustran historias bíblicas que llenan la fachada y los frentes del crucero.
Las altas y delgadas paredes del gótico francés Rayonnant permitidas por los contrafuertes voladores permitieron extensiones cada vez más ambiciosas de vidrio y tracería decorada, reforzada con herrajes.
Los masones elaboraron una serie de patrones de tracería para las ventanas, desde la geométrica básica hasta la reticulada y la curvilínea, que habían reemplazado a la ventana de la lanceta.
Las iglesias con características de este estilo incluyen la Abadía de Westminster (1245-19), las catedrales de Lichfield (después de 1257-) y Exeter (1275-), la Abadía de Bath (1298-), y el coro retro en la Catedral de Wells (c.1320-).
El uso de ogees era especialmente común.
Ejemplos de edificios extravagantes franceses incluyen la fachada oeste de la catedral de Rouen, y especialmente las fachadas de Sainte-Chapelle de Vincennes (1370) y la iglesia abacial del coro Mont-Saint-Michel (1448).
Apareció por primera vez en los claustros y la sala capitular (c. 1332) de la Catedral del Viejo San Pablo en Londres por William de Ramsey.
Perpendicular a veces se llama Third Pointed y se empleó durante tres siglos; la escalera abovedada en abanico en Christ Church, Oxford construida alrededor de 1640.
Los reyes de Francia tenían conocimiento de primera mano del nuevo estilo italiano, debido a la campaña militar de Carlos VIII a Nápoles y Milán (1494), y especialmente las campañas de Luis XII y Francisco I (1500-1505) para restaurar el control francés sobre Milán y Génova.
El castillo de Blois (1515-24) introdujo la logia renacentista y la escalera abierta.
Bajo Henry VIII e Elizabeth I, Inglaterra en gran parte se aisló de desarrollos arquitectónicos en el continente.
Shute publicó el primer libro en inglés sobre arquitectura clásica en 1570.
El arco puntiagudo no se originó en la arquitectura gótica; habían sido empleados durante siglos en el Cercano Oriente en la arquitectura preislámica e islámica para arcos, arcadas y bóvedas acanaladas.
También a veces se usaban para propósitos más prácticos, como llevar bóvedas transversales a la misma altura que las bóvedas diagonales, como en la nave y los pasillos de la Catedral de Durham, construida en 1093.
A diferencia de la bóveda de cañón semicircular de edificios romanos y románicos, donde el peso presionaba directamente hacia abajo, y requería paredes gruesas y ventanas pequeñas, la bóveda de costilla gótica estaba hecha de costillas arqueadas diagonales que cruzaban.
El empuje hacia afuera contra las paredes fue contrarrestado por el peso de los contrafuertes y más tarde contrafuertes voladores.
Eran muy difíciles de construir, y solo podían cruzar un espacio limitado.
Las filas alternas de columnas alternas y pilares que reciben el peso de las bóvedas fueron reemplazadas por pilares simples, cada uno recibiendo el mismo peso.
La primera de estas nuevas bóvedas tenía una costilla adicional, llamada tierceron, que corría por la mediana de la bóveda.
Estas bóvedas a menudo copiaban las formas de la elaborada tracería de los estilos góticos tardíos.
Un segundo tipo se llamaba bóveda reticulada, que tenía una red de costillas decorativas adicionales, en triángulos y otras formas geométricas, colocadas entre o sobre las costillas transversales.
Un ejemplo es el claustro de la Catedral de Gloucester (c. 1370).
Fueron utilizados más tarde en Sens, en Notre-Dame de París y en Canterbury en Inglaterra.
En el período gótico alto, se introdujo una nueva forma, compuesta por un núcleo central que rodeaba varias columnas delgadas adjuntas, o colonetas, que subían a las bóvedas.
En Inglaterra, las columnas agrupadas a menudo estaban ornamentadas con anillos de piedra, así como columnas con hojas talladas.
En lugar de la capital corintia, algunas columnas usaban un diseño de hoja rígida.
En estructuras posteriores, los contrafuertes a menudo tenían varios arcos, cada uno llegando a un nivel diferente de la estructura.
Los arcos tenían un propósito práctico adicional; contenían canales de plomo que transportaban el agua de lluvia desde el techo; fue expulsado de las bocas de gárgolas de piedra colocadas en filas en los contrafuertes.
También tenían un propósito práctico; a menudo servían como campanarios de apoyo a campanarios, cuyas campanas decían la hora anunciando servicios religiosos, advertían de fuego o ataque enemigo, y celebraban ocasiones especiales como victorias militares y coronaciones.
Dado que la construcción de la catedral generalmente tomaba muchos años, y era extremadamente costosa, cuando la torre se iba a construir, el entusiasmo público disminuyó y los gustos cambiaron.
Chartres habría sido aún más exuberante si se hubiera seguido el segundo plan; pedía siete torres alrededor del crucero y el santuario.
La Catedral de Laon gótica temprana y Alta tiene una torre de la linterna cuadrada sobre el cruce del crucero; dos torres en el frente occidental; y dos torres en los extremos de los transeptos.
En Normandía, las catedrales y las iglesias principales a menudo tenían torres múltiples, construidas durante los siglos; Abbaye aux Hommes (comenzó 1066), Caen tiene nueve torres y agujas, colocadas en la fachada, los transeptos y el centro.
Una variación de la aguja era la fléche, una aguja esbelta, similar a una lanza, que generalmente se colocaba en el crucero donde cruzaba la nave.
La catedral de Amiens tiene un fléche.
Fue removida en 1786 durante un programa para modernizar la catedral, pero fue puesta de nuevo en una nueva forma diseñada por Eugene Viollet-le-Duc.
En el gótico inglés, la torre principal a menudo se colocaba en el cruce del crucero y la nave, y era mucho más alta que la otra.
Una torre de cruce fue construida en la Catedral de Canterbury en 1493-1501 por John Wasdell, que había trabajado anteriormente en el King's College de Cambridge.
Un arco doble inusual tuvo que ser construido en el centro del cruce para dar a la torre el apoyo adicional que necesitaba.
La construcción comenzó otra vez en 1724 al diseño de Nicholas Hawksmoor, después de que primero Christopher Wren había propuesto un diseño en 1710, pero se detuvo otra vez en 1727.
La Catedral de Colonia se había iniciado en el siglo XIII, siguiendo el plan de la Catedral de Amiens, pero solo el ábside y la base de una torre se terminaron en el período gótico.
La torre de Ulm Minster tiene una historia similar, comenzó en 1377, se detuvo en 1543 y no se completó hasta el siglo XIX.
La Catedral de Burgos se inspiró más en el norte de Europa.
La tracería de placas fue el primer tipo de tracería que se desarrolló, emergiendo en la fase posterior del gótico temprano o primer punto.
La tracería es práctica y decorativa, porque los ventanales cada vez más grandes de los edificios góticos necesitaban el máximo apoyo contra el viento.
La tracería de placas alcanzó la altura de su sofisticación con las ventanas del siglo XII de la Catedral de Chartres y en el rosetón "Ojo de Dean" en la Catedral de Lincoln.
Barras de piedra, un importante elemento decorativo de estilos góticos, se utilizó por primera vez en la catedral de Reims poco después de 1211, en la cabecera construida por Jean D'Orbais.
Bar-tracería se hizo común después de c.1240, con el aumento de la complejidad y la disminución de peso.
Rayonnant también desplegó molduras de dos tipos diferentes en la tracería, donde los estilos más tempranos habían usado molduras de un tamaño solo, con tallas diferentes de parteluz.
Los montantes del estilo geométrico típicamente tenían capiteles con barras curvas que emergen de ellos.
En consecuencia, los montantes se ramificaron en diseños en forma de Y adornados con cúspides.
Second Pointed (siglo XIV) vio Intersecar la tracería elaborada con ogees, creando un complejo diseño reticular (en forma de red) conocido como Tracería reticulada.
Estas formas se conocen como dagas, vejigas de pescado o mouchettes.
Perpendicular se esforzó por la verticalidad y prescindió de las líneas sinuosas del estilo curvilíneo en favor de pilares rectos ininterrumpidos de arriba a abajo, atravesados por travesaños y barras horizontales.
Los travesaños a menudo eran coronados por almenas en miniatura.
Con frecuencia cubría las fachadas, y las paredes interiores de la nave y el coro estaban cubiertas con arcadas ciegas.
2 Bóvedas Las bóvedas acanaladas de cañón o ingle aparecieron en la época románica y se elaboraron en la época gótica.
Tienen una nave larga que hace el cuerpo de la iglesia, donde los feligreses adoraron; un brazo transversal llamó el crucero y, más allá de ello al este, el coro, también conocido como un presbiterio o presbiterio, que por lo general se reservaba para el clero.
Un pasaje llamado el ambulatorio rodeaba el coro.
Las primeras catedrales, como Notre-Dame, tenían bóvedas de costilla de seis partes, con columnas y pilares alternos, mientras que las catedrales posteriores tenían bóvedas de cuatro partes más simples y más fuertes, con columnas idénticas.
Los transeptos eran generalmente cortos en la arquitectura gótica francesa temprana, pero se hicieron más largos y se les dio grandes rosetones en el período Rayonnant.
En Inglaterra, los transeptos eran más importantes, y los planos eran por lo general mucho más complejos que en catedrales francesas, con la adición de Capillas de la Señora adjuntas, una Casa del Capítulo octagonal y otras estructuras.
Una elevación típicamente tenía cuatro niveles.
Por encima de eso había una galería más estrecha, llamada triforio, que también ayudó a proporcionar grosor y soporte adicionales.
Este sistema se usó en la Catedral de Noyon, la Catedral de Sens y otras estructuras tempranas.
La tribuna desapareció, lo que significaba que las arcadas podrían ser más altas.
Un arreglo similar se adaptó en Inglaterra, en la Catedral de Salisbury, la Catedral de Lincoln y la Catedral de Ely.
Esto fue posible gracias al desarrollo del contrafuerte volador, que transfirió el empuje del peso del techo a los soportes fuera de las paredes.
La Catedral de Beauvais llegó al límite de lo que era posible con la tecnología gótica.
Las fachadas góticas fueron adaptadas del modelo de las fachadas románicas.
La escultura del tímpano central estaba dedicada al Juicio Final, la de la izquierda a la Virgen María y la de la derecha a los santos honrados en esa catedral en particular.
Siguieron la doctrina expresada por Santo Tomás de Aquino de que la belleza era una "armonía de contrastes".
En Inglaterra, el rosetón fue reemplazado a menudo por varias ventanas de lanceta.
Los portales estaban coronados con frontones de arco alto, compuestos por arcos concéntricos llenos de escultura.
Las torres estaban adornadas con sus propios arcos, a menudo coronados con pináculos.
Mientras que las catedrales francesas enfatizaban la altura de la fachada, las catedrales inglesas, particularmente en el gótico anterior, a menudo enfatizaban el ancho.
Se separó del énfasis francés en la altura, y eliminó los estatutos de las columnas y las estatuas en las entradas arqueadas, y cubrió la fachada con coloridos mosaicos de escenas bíblicas (los mosaicos actuales son de una fecha posterior).
El escultor Andrea Pisano hizo las famosas puertas de bronce para el baptisterio de Florencia (1330-1336).
Por lo general, hay una sola o doble ambulatoria, o pasillo, alrededor del coro y el extremo este, por lo que los feligreses y peregrinos podían caminar libremente alrededor del extremo este.
El abad Suger utilizó por primera vez la novedosa combinación de bóvedas de costillas y contrafuertes para reemplazar las gruesas paredes y reemplazarlas con vidrieras, abriendo esa parte de la iglesia a lo que él consideraba "luz divina".
Hay tres capillas de este tipo en la Catedral de Chartres, siete en Notre Dame de París, la Catedral de Amiens, la Catedral de Praga y la Catedral de Colonia, y nueve en la Basílica de San Antonio de Padua en Italia.
Un edicto del Segundo Concilio de Nicea en 787 había declarado: "La composición de las imágenes religiosas no debe dejarse a la inspiración de los artistas; se deriva de los principios establecidos por la Iglesia Católica y la tradición religiosa.
Poco a poco, a medida que el estilo evolucionó, la escultura se hizo más y más prominente, tomando las columnas del portal, y subiendo gradualmente por encima de los portales, hasta que las estatuas en nichos cubrieron toda la fachada, como en la Catedral de Wells, a los transeptos, y, como en la Catedral de Amiens, incluso en el interior de la fachada.
Esto estableció un patrón de iconografía compleja que se siguió en otras iglesias.
El tímpano sobre el portal central en la fachada oeste de Notre-Dame de París ilustra vívidamente el Juicio Final, con figuras de pecadores que son llevados al infierno y buenos cristianos llevados al cielo.
Los tormentos del infierno fueron aún más vívidamente representados.
Eran parte del mensaje visual para los adoradores analfabetos, símbolos del mal y el peligro que amenazaban a aquellos que no seguían las enseñanzas de la iglesia.
Fueron reemplazados por figuras de estilo gótico, diseñadas por Eug-ne Viollet-le-Duc durante la restauración del siglo XIX.
Las enseñanzas religiosas en la Edad Media, particularmente los escritos de Pseudo-Dionisio el Areopagita, un místico del siglo VI cuyo libro, De Coelesti Hierarchia, era popular entre los monjes en Francia, enseñaba que toda la luz era divina.
Las ventanas en el lado norte, con frecuencia en la sombra, tenían ventanas que representan el Antiguo Testamento.
Los detalles se pintaron sobre el vidrio en esmalte vítreo, luego se hornearon en un horno para fusionar el esmalte en el vidrio.
Sainte-Chapelle se convirtió en el modelo para otras capillas en toda Europa.
Se sumergió vidrio transparente en vidrio coloreado, luego se molieron porciones del vidrio coloreado para dar exactamente el tono correcto.
Uno de los edificios Flamboyant más célebres fue la Sainte-Chapelle de Vincennes (1370), con paredes de vidrio desde el suelo hasta el techo.
Las vidrieras eran extremadamente complejas y caras de crear.
La rosa era un símbolo de la Virgen María, y se usaban particularmente en iglesias dedicadas a ella, incluida Notre-Dame de Paris.
El Palacio de la Cité en París, cerca de Notre-Dame de París, comenzó en 1119, que fue la residencia principal de los reyes franceses hasta 1417.
Sin embargo, pronto fue hecho obsoleto por el desarrollo de la artillería, y en el 15to siglo se remodeló en un palacio residencial cómodo.
El ejemplo más antiguo existente en Inglaterra es probablemente el Mob Quad del Merton College de la Universidad de Oxford, construido entre 1288 y 1378.
Un tipo similar de claustro académico fue creado en el Queen's College de Oxford en la década de 1140, probablemente diseñado por Reginald Ely.
Algunas universidades, como Balliol College, Oxford, tomaron prestado un estilo militar de los castillos góticos, con almenas y paredes crenoladas.
Escribió en 1447 que quería que su capilla "procediera en forma grande, limpia y sustancial, separando la superfluidad de trabajos demasiado grandes y curiosos de implicación y moldeo ocupado".
Las paredes tenían dos niveles de pasarelas en el interior, un parapeto almenado con merlones, y proyectó maquinaciones desde las cuales se podían lanzar misiles sobre los sitiadores.
Los castillos estaban rodeados por un foso profundo, atravesado por un solo puente levadizo.
Un buen ejemplo de supervivencia es el Castillo de Dourdan, cerca de Nemours.
La conversión implicó compromisos ya que las iglesias latinas se orientan hacia el Este y las mezquitas se orientan hacia La Meca.
Mezquita de Lala Mustafa Pasha, en Famagusta, Chipre del Norte.
El estilo gótico comenzó a describirse como anticuado, feo e incluso bárbaro.
Irlanda fue una isla de arquitectura gótica en los siglos XVII y XVIII, con la construcción de la Catedral de Derry (completada en 1633), la Catedral de Sligo (c. 1730) y la Catedral de Down (1790-1818) son otros ejemplos.
Las dos torres occidentales de la Abadía de Westminster fueron construidas entre 1722 y 1745 por Nicholas Hawksmoor, abriendo un nuevo período de renacimiento gótico.
Este período de la apelación más universal, atravesando 1855-1885, se conoce en Gran Bretaña como el gótico victoriano Alto.
A partir de la segunda mitad del 19no siglo adelante, se hizo más común en Gran Bretaña para el neogótico para usarse en el diseño de tipos de edificios no eclesiásticos y no gubernamentales.
Los arquitectos paisajistas trabajan en estructuras y espacios externos en el aspecto paisajístico del diseño: grandes o pequeños, urbanos, suburbanos y rurales, y con materiales "duros" (construidos) y "suaves" (plantados), al tiempo que integran la sostenibilidad ecológica.
También pueden revisar las propuestas para autorizar y supervisar los contratos para las obras de construcción.
La primera persona en escribir sobre hacer un paisaje fue Joseph Addison en 1712.
Durante el último 19no siglo, el arquitecto del paisaje del término comenzó a ser usado por diseñadores de paisajes profesionales y firmemente se estableció después de Frederick Law Olmsted, Hijo, y Beatrix Jones (más tarde Farrand) con otros fundó la Sociedad americana de Arquitectos del Paisaje (ASLA) en 1899.
Sus proyectos pueden abarcar desde estudios de sitio hasta la evaluación ecológica de áreas amplias con fines de planificación o gestión.
Su trabajo se materializa en declaraciones escritas de política y estrategia, y su cometido incluye la planificación maestra para nuevos desarrollos, evaluaciones y evaluaciones del paisaje, y la preparación de planes de gestión o políticas del campo.
En los últimos años, la necesidad y el interés de los jardines terapéuticos han ido en aumento.
Entre estos se encontraban Central Park en la ciudad de Nueva York, Prospect Park en Brooklyn, Nueva York y el sistema de parques Emerald Necklace de Boston.
Fue consultora de diseño para más de una docena de universidades, incluyendo: Princeton en Princeton, Nueva Jersey; Yale en New Haven, Connecticut; y el Arnold Arboretum para Harvard en Boston, Massachusetts.
Los planificadores urbanos están calificados para realizar tareas independientes de los arquitectos paisajistas, y en general, el plan de estudios de los programas de arquitectura paisajista no preparan a los estudiantes para convertirse en planificadores urbanos.
Roberto Burle Marx en Brasil combinó el estilo internacional y las plantas brasileñas nativas y la cultura para una nueva estética.
Él popularizó un sistema de análisis de las capas de un sitio con el fin de compilar una comprensión completa de los atributos cualitativos de un lugar.
Una vez reconocidos por AILA, los arquitectos paisajistas usan el título 'Arquitecto Paisajista Registrado' en los seis estados y territorios dentro de Australia.
Dentro de NZ, los miembros de NZILA cuando alcancen su posición profesional, pueden usar el título de Arquitecto Paisajista Registrado NZILA.
La misión de ILASA es promover la profesión de la arquitectura del paisaje y mantener altos estándares de servicio profesional a sus miembros, y representar la profesión de la arquitectura del paisaje en cualquier asunto que pueda afectar los intereses de los miembros del instituto.
En la actualidad hay quince programas acreditados en el Reino Unido.
En 2008, el LI lanzó una importante campaña de reclutamiento titulada "Quiero ser un arquitecto paisajista" para fomentar el estudio de la arquitectura del paisaje.
Varios estados también requieren la aprobación de un examen estatal.
En el siglo VI aC, la arena ya cubría las estatuas del templo principal hasta las rodillas.
Un esquema para salvar los templos se basó en una idea de William MacQuitty para construir una presa de agua dulce transparente alrededor de los templos, con el agua en el interior mantenida a la misma altura que el Nilo.
Consideraron que elevar los templos ignoraba el efecto de la erosión de la piedra arenisca por los vientos del desierto.
Entre 1964 y 1968, todo el sitio fue cortado cuidadosamente en grandes bloques (hasta 30 toneladas, con un promedio de 20 toneladas), desmantelado, levantado y vuelto a montar en una nueva ubicación 65 metros más alto y 200 metros de distancia del río, en uno de los mayores desafíos de la ingeniería arqueológica en la historia.
Muchos visitantes también llegan en avión a un aeródromo que fue construido especialmente para el complejo del templo, o por carretera desde Asuán, la ciudad más cercana.
Las colosales estatuas a lo largo de la pared izquierda llevan la corona blanca del Alto Egipto, mientras que las del lado opuesto llevan la doble corona del Alto y Bajo Egipto (pschent).
El relieve más famoso muestra al rey en su carro disparando flechas contra sus enemigos que huyen, que están siendo hechos prisioneros.
Hay representaciones de Ramsés y Nefertari con los barcos sagrados de Amón y Ra-Horakhty.
Estas fechas son supuestamente el cumpleaños y el día de la coronación del rey, respectivamente.
De hecho, según los cálculos realizados sobre la base de la elevación helicoidal de la estrella Sirio (Sothis) y las inscripciones encontradas por los arqueólogos, esta fecha debe haber sido el 22 de octubre.
Esta fue la segunda vez en la historia del antiguo Egipto que un templo fue dedicado a una reina.
Tradicionalmente, las estatuas de las reinas estaban junto a las del faraón, pero nunca eran más altas que sus rodillas.
Los capiteles de los pilares llevan la cara de la diosa Hathor; este tipo de columna se conoce como Hathoric.
En las paredes sur y norte de esta cámara hay dos bajorrelieves elegantes y poéticos del rey y su consorte que presentan plantas de papiro a Hathor, que se representa como una vaca en un barco que navega en un matorral de papiros.
Ninguno de los edificios actuales se cree que datan de antes del siglo 17, pero es probable que se construyeron con los mismos métodos de construcción y diseños que se habían utilizado durante siglos antes.
Otras kasbahs y ksour se localizaron a lo largo de esta ruta, como Tamdaght cercano al norte.
Los edificios del pueblo se agrupan dentro de un muro defensivo que incluye torres de esquina y una puerta.
El pueblo también tiene una serie de edificios públicos o comunitarios, como una mezquita, un caravasar, una kasbah (fortificación similar a un castillo) y el Marabou de Sidi Ali o Amer.
Estaba hecho de tierra comprimida y barro, generalmente mezclado con otros materiales para ayudar a la adhesión.
La presa de Asuán, o más específicamente desde la década de 1960, la presa alta de Asuán, es la presa de terraplén más grande del mundo, que se construyó al otro lado del Nilo en Asuán, Egipto, entre 1960 y 1970.
Al igual que la implementación anterior, la Presa Alta ha tenido un efecto significativo en la economía y la cultura de Egipto.
Sin embargo, esta inundación natural varió, ya que los años de aguas altas podrían destruir toda la cosecha, mientras que los años de aguas bajas podrían crear una sequía generalizada y, en consecuencia, hambruna.
En cambio, se favoreció el Plan del Valle del Nilo del hidrólogo británico Harold Edwin Hurst, que propuso almacenar agua en Sudán y Etiopía, donde la evaporación es mucho menor.
Inicialmente, tanto los Estados Unidos como la URSS estaban interesados en ayudar al desarrollo de la presa.
En ese momento, Estados Unidos temía que el comunismo se extendiera al Medio Oriente, y vio a Nasser como un líder natural de una Liga Árabe procapitalista anticomunista.
Después de que la ONU criticara una incursión de Israel contra las fuerzas egipcias en Gaza en 1955, Nasser se dio cuenta de que no podía presentarse como el líder del nacionalismo panárabe si no podía defender a su país militarmente contra Israel.
Nasser no aceptó estas condiciones y consultó a la URSS para obtener apoyo.
Dulles se enfureció más por el reconocimiento diplomático de Nasser de China, que estaba en el conflicto directo con la política de Dulles de la contención del comunismo.
También se irritó por la neutralidad de Nasser y los intentos de jugar en ambos lados de la Guerra Fría.
La enorme presa de roca y arcilla fue diseñada por el Instituto Soviético Hydroproject junto con algunos ingenieros egipcios.
Por el contrario, la presa inundó una gran área, causando la reubicación de más de 100.000 personas.
La evaluación de los costos y beneficios de la presa sigue siendo controvertida décadas después de su finalización.
Al no tener en cuenta los efectos ambientales y sociales negativos de la presa, se estima que sus costos se recuperaron en solo dos años.
Otro observador no estuvo de acuerdo y recomendó que se derribara la presa.
La presa mitigó los efectos de las inundaciones, como las de 1964, 1973 y 1988.
Se ha creado una nueva industria pesquera alrededor del lago Nasser, aunque está luchando debido a su distancia de cualquier mercado significativo.
Alrededor de medio millón de familias se asentaron en estas nuevas tierras.
En otras tierras de regadío previamente, los rendimientos aumentaron porque el agua podría estar disponible en períodos críticos de bajo flujo.
En Sudán, entre 50.000 y 70.000 nubios sudaneses fueron trasladados del casco antiguo de Wadi Halfa y sus aldeas circundantes.
El gobierno desarrolló un proyecto de riego, llamado el Nuevo Esquema de Desarrollo Agrícola de Halfa para cultivar algodón, granos, caña de azúcar y otros cultivos.
Se construyeron viviendas e instalaciones para 47 unidades de aldea cuya relación entre sí se aproximaba a la de Old Nubia.
El valor nutritivo añadido a la tierra por el sedimento fue de sólo 6.000 toneladas de potasa, 7.000 toneladas de pentóxido de fósforo y 17.000 toneladas de nitrógeno.
La salinidad del suelo también aumentó porque la distancia entre la superficie y la capa freática era lo suficientemente pequeña (1/2 m dependiendo de las condiciones del suelo y la temperatura) para permitir que el agua se elevara por evaporación, de modo que las concentraciones relativamente pequeñas de sal en el agua subterránea se acumularon en la superficie del suelo a lo largo de los años.
En la década de 1950, solo una pequeña proporción del Alto Egipto no se había convertido del riego de cuenca (baja transmisión) al perenne (alta transmisión).
S. haematobium ha desaparecido desde entonces por completo.
Esto significa que el volumen de almacenamiento muerto se llenaría después de 300-500 años si el sedimento se acumulara a la misma velocidad en toda el área del lago.
Después de la construcción de la presa, las malas hierbas acuáticas crecieron mucho más rápido en el agua más clara, ayudadas por los residuos de fertilizantes.
La pesca en el Mediterráneo y la pesca en el lago de agua salobre disminuyeron después de que se terminó la presa porque los nutrientes que fluían por el Nilo hacia el Mediterráneo quedaron atrapados detrás de la presa.
Una preocupación antes de la construcción de la Presa Alta había sido la posible caída del nivel del lecho del río aguas abajo de la Presa como resultado de la erosión causada por el flujo de agua libre de sedimentos.
La industria de la construcción de ladrillo rojo, que consistía en cientos de fábricas que usaban depósitos de sedimentos del Nilo a lo largo del río, también se ha visto afectada negativamente.
Debido a la menor turbidez del agua, la luz solar penetra más profundamente en el agua del Nilo.
Los trabajos de construcción comenzaron en 1995 y, después de haber gastado unos 220 millones de dólares, el complejo fue inaugurado oficialmente el 16 de octubre de 2002.
La recreación de la antigua biblioteca no solo fue adoptada por otros individuos y agencias, sino que obtuvo el apoyo de los políticos egipcios.
La participación de la UNESCO a partir de 1986 creó una gran oportunidad para que el proyecto fuera verdaderamente internacional.
Este equipo arquitectónico estaba formado por diez miembros que representaban a seis países.
Las primeras promesas se hicieron para financiar el proyecto en una conferencia celebrada en 1990 en Asuán: USD $ 65 millones, en su mayoría de los estados de MENA.
En 2010, la biblioteca recibió una donación de 500.000 libros de la Biblioteca Nacional de Francia, Biblioteca Nacional de Francia (BnF).
La sala de lectura principal se encuentra debajo de un techo con paneles de vidrio de 32 metros de altura, inclinado hacia el mar como un reloj de sol, y mide unos 160 m de diámetro.
Con aproximadamente 1.316 artefactos, la colección del Museo de Antigs ofrece una visión de la historia egipcia desde la era faraónica hasta la conquista de Alejandro Magno y las civilizaciones romanas antes del advenimiento del Islam en todo Egipto.
Microfilm: Esta sección incluye microfilmes de alrededor de 30.000 manuscritos raros y 50.000 documentos, así como una colección de la Biblioteca Británica de alrededor de 14.000 manuscritos árabes, persas y turcos, que se considera la colección más grande de Europa.
Sin embargo, en 2010 la biblioteca recibió 500.000 libros adicionales de la Biblioteca Nacional de Francia.)
La Gran Mezquita de Djenné es un gran banco o edificio de adobe que es considerado por muchos arquitectos como uno de los mayores logros del estilo arquitectónico sudano-saheliano.
El documento más antiguo que menciona la mezquita es Tarikh al-Sudán de Abd al-Sadi, que da la historia temprana, presumiblemente de la tradición oral tal como existía a mediados del siglo XVII.
Su sucesor inmediato construyó las torres de la mezquita, mientras que el siguiente sultán construyó el muro circundante.
Este habría sido el edificio que vio Caillié.
La nueva mezquita era un edificio grande y bajo que carecía de torres u ornamentación.
La reconstrucción se completó en 1907 usando el trabajo forzado bajo la dirección de Ismaila Traoré, jefe del gremio de albañiles de Djenné.
Se ha debatido hasta qué punto el diseño de la mezquita reconstruida estaba sujeto a la influencia francesa.
Pensó que los conos hacían que el edificio se pareciera a un templo barroco dedicado al dios de los supositorios.
También dice que la gente local estaba tan infeliz con el nuevo edificio que se negaron a limpiarlo, solo lo hicieron cuando fueron amenazados con prisión.
La tumba más grande al sur contiene los restos de Almany Ismala, un importante imán del siglo XVIII.
En algunos casos, las superficies originales de una mezquita incluso han sido embaldosadas, destruyendo su aspecto histórico y, en algunos casos, comprometiendo la integridad estructural del edificio.
En 1996, la revista Vogue realizó una sesión fotográfica de moda dentro de la mezquita.
Se accede por seis juegos de escaleras, cada una decorada con pináculos.
El muro de oración o qibla de la Gran Mezquita mira hacia el este hacia La Meca y con vistas al mercado de la ciudad.
Las agujas en forma de cono o pináculos en la parte superior de cada minarete están cubiertos con huevos de avestruz.
Las pequeñas ventanas, colocadas irregularmente en las paredes norte y sur permiten poca luz natural para llegar al interior de la sala.
El imán dirige las oraciones del mihrab en la torre central más grande.
A la derecha del mihrab en la torre central hay un segundo nicho, el púlpito o minbar, desde el cual el imán predica su sermón del viernes.
Las paredes de las galerías que dan al patio están salpicadas por aberturas arqueadas.
En lugar de un solo nicho central, la torre mihrab originalmente tenía un par de grandes huecos que hacían eco de la forma de los arcos de entrada en la pared norte.
Requiere varios días para curarse, pero debe agitarse periódicamente, una tarea que generalmente recae en los niños pequeños que juegan en la mezcla, agitando así el contenido.
Una carrera se lleva a cabo al comienzo del festival para ver quién será el primero en entregar el yeso a la mezquita.
En 1930, se construyó una réplica inexacta de la Mezquita Djenné en la ciudad de Fréjus, en el sur de Francia.
La mezquita original presidió uno de los centros de aprendizaje islámicos más importantes de África durante la Edad Media, con miles de estudiantes que vinieron a estudiar el Corán en las madrazas de Djenné.
El 20 de enero de 2006, la vista de un equipo de hombres pirateando el techo de la mezquita provocó un motín en la ciudad.
En la mezquita, la multitud arrancó los ventiladores de ventilación que habían sido presentados por la Embajada de los Estados Unidos en el momento de la Guerra de Irak y luego se lanzó en un alboroto por la ciudad.
La Gran Esfinge de Giza, comúnmente conocida como la Esfinge de Giza o simplemente la Esfinge, es una estatua de piedra caliza de una esfinge reclinada, una criatura mítica.
Además, el ángulo y la ubicación de la pared sur del recinto sugieren que la calzada que conecta la Pirámide de Khafre y el Templo del Valle ya existía antes de que se planeara la Esfinge.
Cuando la Estela fue reexcavada en 1925, las líneas de texto que se referían a Khaf se desprendieron y fueron destruidas.
El culto de la Esfinge continuó en la época medieval.
Alejandría, Rosetta, Damietta, El Cairo y las pirámides de Giza se describen repetidamente, pero no necesariamente de manera exhaustiva.
Siete años después de visitar Giza, André Thévet (Cosmographie de Levant, 1556) describió la Esfinge como "la cabeza de un coloso, hecha por Isis, hija de Inachus, entonces tan amada por Júpiter".
La Esfinge de Johannes Helferich (1579) es una mujer pelirroja y de pecho redondo con una peluca de pelo liso; la única ventaja sobre Thévet es que el cabello sugiere las lapatas llameantes del tocado.
Aunque ciertas extensiones en la Estela son probablemente precisas, este pasaje se contradice con la evidencia arqueológica, por lo tanto, se considera revisionismo histórico del Período Tardío, una falsificación intencional, creada por los sacerdotes locales como un intento de impregnar el templo contemporáneo de Isis con una historia antigua que nunca tuvo.
Los descubrimientos recientes, sin embargo, muestran fuertemente que realmente no se construyó antes del reinado de Khafre, en la cuarta dinastía.
Maspero creía que la Esfinge era "el monumento más antiguo de Egipto".
Parte de su tocado se había caído en 1926 debido a la erosión, que también había cortado profundamente en su cuello.
La capa en la que se esculpió la cabeza es mucho más dura.
Otros cuentos lo atribuyen a ser el trabajo de los mamelucos.
Según al-Maqr?z?, muchas personas que viven en el área creyeron que la arena aumentada que cubre la Meseta de Giza era la retribución para el acto del desfiguramiento de al-Dahr.
Al-Minufi declaró que la Cruzada alejandrina en 1365 era el castigo divino para un jeque sufí del khanqah de Sa'id que se separaba de la nariz.
La idea es considerada pseudoarqueología por la academia, porque ninguna evidencia textual o arqueológica apoya que esta sea la razón de la orientación de la Esfinge.
Hay una larga historia de especulaciones sobre cámaras ocultas debajo de la Esfinge, por figuras esotéricas como H. Spencer Lewis.
Se cree que es el segundo sitio histórico más visitado de Egipto; solo el complejo de la pirámide de Giza cerca de El Cairo recibe más visitas.
Las otras tres partes, el Recinto de Mut, el Recinto de Montu y el Templo desmantelado de Amenhotep IV, están cerrados al público.
El templo original fue destruido y parcialmente restaurado por Hatshepsut, aunque otro faraón construyó a su alrededor para cambiar el enfoque u orientación del área sagrada.
La construcción de templos comenzó en el Reino Medio y continuó en la época ptolemaica.
Las deidades representadas van desde algunas de las primeras adoradas hasta las adoradas mucho más tarde en la historia de la cultura del Antiguo Egipto.
Estos arquitrabes pueden haber sido levantados a estas alturas usando palancas.
Si se hubiera usado piedra para las rampas, habrían podido usar mucho menos material.
El tallado final se ejecutó después de que los tambores se pusieron en el lugar de modo que no se dañara mientras se colocara.
La ciudad de Tebas no parece haber sido de gran importancia antes de la Undécima Dinastía y la construcción anterior del templo habría sido relativamente pequeña, con santuarios dedicados a las primeras deidades de Tebas, la diosa de la Tierra Mut y Montu.
Amón (a veces llamado Amén) fue durante mucho tiempo la deidad tutelar local de Tebas.
Los principales trabajos de construcción en el Recinto de Amón-Ra tuvieron lugar durante la XVIII Dinastía, cuando Tebas se convirtió en la capital del Antiguo Egipto unificado.
Otro de sus proyectos en el sitio, la Capilla Roja de Karnak o Chapelle Rouge, fue pensado como un santuario de barcas y originalmente pudo haber estado entre sus dos obeliscos.
Conocido como el obelisco inacabado, proporciona evidencia de cómo se extrajeron los obeliscos.
El último cambio importante en el diseño del recinto de Amón-Re fue la adición del Primer Pilón y las enormes paredes del recinto que rodean todo el recinto, ambos construidos por Nectanebo I de la Dinastía Treinta.
El complejo del templo de Karnak es descrito por primera vez por un veneciano desconocido en 1589, aunque su relato no da nombre al complejo.
La escritura de Protais sobre su viaje fue publicada por Melchisédech Thévenot (Relations de divers voyages curieux, 1670s-1696 ediciones) y Johann Michael Vansleb (The Present State of Egypt, 1678).
Tras los trabajos de excavación y restauración realizados por el equipo de la Universidad Johns Hopkins, dirigido por Betsy Bryan (ver más abajo), el Recinto de Mut se ha abierto al público.
En 2006, Betsy Bryan presentó sus hallazgos de un festival que incluía aparente exceso intencional en el alcohol.
Estos hallazgos se hicieron en el templo de Mut porque cuando Tebas se elevó a una mayor prominencia, Mut absorbió a las diosas guerreras, Sekhmet y Bast, como algunos de sus aspectos.
En un mito posterior desarrollado alrededor del festival anual de Sekhmet borracho, Ra, para entonces el dios del sol del Alto Egipto, la creó de un ojo ardiente ganado de su madre, para destruir a los mortales que conspiraron contra él (Bajo Egipto).
El Templo de Luxor es un gran complejo de templos del Antiguo Egipto ubicado en la orilla este del río Nilo en la ciudad hoy conocida como Luxor (antigua Tebas) y fue construido aproximadamente 1400 aC.
Cuatro de los principales templos mortuorios visitados por los primeros viajeros incluyen el Templo de Seti I en Gurnah, el Templo de Hatshepsut en Deir el Bahri, el Templo de Ramsés II (es decir, Ramsseum) y el Templo de Ramsés III en Medinet Habu.
En la parte trasera del templo hay capillas construidas por Amenhotep III de la XVIII Dinastía y Alejandro.
Esta piedra arenisca se conoce como arenisca nubia.
Alexander Badawy, "Ilusionismo en Arquitectura egipcia", Estudios en la Civilización Oriental Antigua, 35 (1969): 23.
A lo largo de la avenida se establecieron las estaciones para ceremonias como la Fiesta de Opet, que tenía importancia para el templo.
Lalibela es una ciudad en el distrito de Lasta de la zona norte de Wollo en la región de Amhara, Etiopía.
Para los cristianos, Lalibela es una de las ciudades más sagradas de Etiopía, solo superada por Axum y un centro de peregrinación.
Se dice que los nombres de varios lugares en la ciudad moderna y el diseño general de las iglesias excavadas en la roca imitan los nombres y patrones observados por Lalibela durante el tiempo que pasó como joven en Jerusalén y Tierra Santa.
La fe cristiana inspira muchas características con nombres bíblicos, incluso el río Lalibela es conocido como el río Jordán.
El sacerdote portugués Francisco Álvares (1465-1540), acompañó al embajador portugués en su visita a Dawit II en la década de 1520.
El siguiente visitante europeo reportado a Lalibela fue Miguel de Castanhoso, quien sirvió como soldado bajo Cristóvao da Gama y salió de Etiopía en 1544.
Sus pilares también fueron cortados de la montaña.
Existe cierta controversia en cuanto a cuándo se construyeron algunas de las iglesias.
Su informe describe dos tipos de viviendas vernáculas que se encuentran en la zona.
El Monasterio de Santa Catalina, oficialmente Monasterio Sagrado del Dios Trodden Mount Sinai, es un monasterio ortodoxo oriental ubicado en la península del Sinaí, en la desembocadura de un desfiladero al pie del Monte Sinaí, cerca de la ciudad de Santa Catalina, Egipto.
El monasterio de Santa Catalina se encuentra a la sombra de un grupo de tres montañas; Ras Sufsafeh (posiblemente "Monte Horeb" c.1 km al oeste), Jebel Arrenziyeb y Jebel Musa, el "Monte Bíblico Sinaí" (pico c.2 km al sur).
La propia Catherine ordenó que comenzara la ejecución.
El monasterio fue construido por orden del emperador Justiniano I (reinó 527-565), que encierra la Capilla de la zarza ardiente (también conocida como "Capilla de Santa Helena") que fue construida por la emperatriz consorte Helena, madre de Constantino el Grande, en el sitio donde se supone que Moisés vio la zarza ardiente.
El sitio es sagrado para el cristianismo, el islam y el judaísmo.
Durante el siglo VII, los anacoretas cristianos aislados del Sinaí fueron eliminados: solo quedó el monasterio fortificado.
Desde la época de la Primera Cruzada, la presencia de los cruzados en el Sinaí hasta 1270 estimuló el interés de los cristianos europeos y aumentó el número de peregrinos intrépidos que visitaron el monasterio.
El estado administrativo exacto de la iglesia dentro de la Iglesia Ortodoxa del Este es ambiguo: por unos, incluso la propia iglesia, se considera autocephalous, por otros una iglesia autónoma bajo la jurisdicción de la Iglesia Ortodoxa griega de Jerusalén.
Pero en 2003 los eruditos rusos descubrieron el acto de donación para el manuscrito firmado por el Consejo de El Cairo Metochion y el arzobispo Callistratus el 13 de noviembre de 1869.
Los palimpsestos son notables por haber sido reutilizados una o más veces a lo largo de los siglos.
Cada página tardó aproximadamente ocho minutos en escanearse por completo.
La gran colección de iconos comienza con unos pocos que datan de los siglos V (posiblemente) y VI, que son supervivencias únicas; el monasterio no ha sido tocado por la iconoclasia bizantina, y nunca fue saqueado.
La conservación de sus estructuras arquitectónicas, pinturas y libros constituyen gran parte del propósito de la Fundación.
Su tamaño refleja la relativa prosperidad de la época.
Fue construido en el sitio de un templo anterior, más pequeño también dedicado a Horus, aunque la estructura anterior estaba orientada de este a oeste en lugar de norte a sur como en el sitio actual.
El templo de Edfu cayó en desuso como monumento religioso después de la persecución de Teodosio I de los paganos y edicto que prohíbe el culto no cristiano dentro del Imperio Romano en 391.
A lo largo de los siglos, el templo quedó enterrado a una profundidad de 12 metros (39 pies) debajo de la arena del desierto a la deriva y las capas de limo del río depositadas por el Nilo.
En 1860 Auguste Mariette, un egiptólogo francés, comenzó el trabajo de liberar el templo de Edfu de las arenas.
Gran Zimbabwe es una ciudad medieval en las colinas del sudeste de Zimbabwe, cerca del lago Mutirikwe y la ciudad de Masvingo.
Se cree que el Gran Zimbabue sirvió como palacio real para el monarca local.
Fueron construidos sin mortero (piedra seca).
Las primeras visitas confirmadas por los europeos fueron a finales del siglo XIX, y las investigaciones del sitio comenzaron en 1871.
El área de Gran Zimbabwe fue colocada por el 4to siglo d.
David Beach cree que la ciudad y su estado, el Reino de Zimbabwe, florecieron de 1200 a 1500, aunque una fecha algo anterior para su desaparición está implícita en una descripción transmitida a principios del siglo XVI a Joao de Barros.
Se les conoce como el Complejo Hill, el Complejo Valley y el Gran Recinto.
El Complejo del Valle se divide en las Ruinas del Valle Superior e Inferior, con diferentes períodos de ocupación.
El foco del poder se movió del Complejo de la Colina en el 12do siglo, al Gran Recinto, el Valle Superior y finalmente el Valle Inferior a principios del 16to siglo.
Otros artefactos incluyen figuritas de esteatita (una de las cuales se encuentra en el Museo Británico), cerámica, gongs de hierro, marfil, hierro y alambre de cobre elaborados, azadas de hierro, puntas de bronce, lingotes y crisoles de cobre, y cuentas de oro, pulseras, colgantes y vainas.
Ese comercio internacional fue además del comercio agrícola local, en el que el ganado era especialmente importante.
Los comerciantes portugueses se enteraron de los restos de la ciudad medieval a principios del siglo XVI, y los registros sobreviven de entrevistas y notas hechas por algunos de ellos, vinculando al Gran Zimbabwe con la producción de oro y el comercio a larga distancia.
Afirmó que la estatuilla en cambio pareció datar a la era de Ptolemaic subsecuente (c. 323-30 A.C.), cuando los comerciantes griegos situados en Alejandría exportarían antiguedades egipcias y pseudo-antiquidades al África del sur.
Bent no tenía formación arqueológica formal, pero había viajado muy ampliamente en Arabia, Grecia y Asia Menor.
Tienen una tradición de la antigua ascendencia judía o árabe del sur a través de su línea masculina.
La reclamación de Lemba también fue relatada por Guillermo Bolts (en 1777, a las autoridades de los Habsburgo austríacas), y por A.A. Anderson (escribiendo sobre sus viajes al norte del Río Limpopo en el 19no siglo).
Primero había hundido tres pozos de prueba en lo que habían sido montones de basura en las terrazas superiores del complejo de la colina, produciendo una mezcla de cerámica y herrajes poco notables.
Caton Thompson inmediatamente anunció su teoría de origen bantú en una reunión de la Asociación Británica en Johannesburgo.
La evidencia de radiocarbono es un conjunto de 28 mediciones, para las cuales todos menos los primeros cuatro, desde los primeros días del uso de ese método y ahora visto como inexacto, apoyan la cronología de los siglos XII al XV.
La eliminación de oro y artefactos en excavaciones de aficionados por parte de los primeros anticuarios coloniales causó daños generalizados, en particular las excavaciones de Richard Nicklin Hall.
Preben Kaarsholm escribe que tanto los grupos nacionalistas coloniales como los negros invocaron el pasado de Gran Zimbabwe para apoyar su visión del presente del país, a través de los medios de comunicación de la historia popular y de la ficción.
Pikirayi y Kaarsholm sugieren que esta presentación del Gran Zimbabwe tenía en parte la intención de alentar el asentamiento y la inversión en la zona.
En 1980, el nuevo país independiente reconocido internacionalmente fue renombrado para el sitio, y sus famosas tallas de aves de esteatita se conservaron de la bandera de Rhodesia y el Escudo de Armas como símbolo nacional y se representaron en la nueva bandera de Zimbabwe.
Un ejemplo del primero es el folleto de Ken Mufuka, aunque el trabajo ha sido muy criticado.
Fue creado para preservar la rica historia de este país que se enfrentaba a un futuro oscuro debido a la globalización.
El sitio exhibe una multitud de estilos arquitectónicos, que recuerdan a los estilos vistos en el centro de México y de los estilos Puuc y Chenes de las tierras bajas mayas del norte.
La ciudad pudo haber tenido la población más diversa del mundo maya, un factor que podría haber contribuido a la variedad de estilos arquitectónicos en el sitio.
Una traducción posible para Itza es "encantador (o encantamiento) del agua", de su (itz), "hechicero", y ha, "agua".
Esta forma conserva la distinción fonémica entre ch y ch, ya que la palabra de base ch'e'en (que, sin embargo, no se enfatiza en maya) comienza con una consonante africada eyectiva postalveolar.
De estos cenotes, el "Cenote Sagrado" o Cenote Sagrado (también conocido como el Pozo Sagrado o Pozo del Sacrificio), es el más famoso.
En cambio, la organización política de la ciudad podría haber sido estructurada por un sistema "multepal", que se caracteriza por gobernar a través de un consejo compuesto por miembros de linajes gobernantes de élite.
Fue, sin embargo, hacia el final del Clásico Tardío y en la primera parte del Clásico Terminal que el sitio se convirtió en una importante capital regional, centralizando y dominando la vida política, sociocultural, económica e ideológica en las tierras bajas mayas del norte.
Hunac Ceel supuestamente profetizó su propio ascenso al poder.
Si bien hay algunas pruebas arqueológicas que indican que Chichén Itzá fue saqueado y saqueado en algún momento, parece haber una mayor evidencia de que no pudo haber sido por Mayapan, al menos no cuando Chichén Itzá era un centro urbano activo.
Después de que las actividades de élite de Chichén Itzá cesaron, la ciudad no pudo haber sido abandonada.
Montejo regresó a Yucatán en 1531 con refuerzos y estableció su base principal en Campeche, en la costa oeste.
Montejo el Joven finalmente llegó a Chichén Itzá, a la que renombró Ciudad Real.
Pasaron los meses, pero no llegaron refuerzos.
En 1535, todos los españoles habían sido expulsados de la península de Yucatán.
En 1860, Désiré Charnay inspeccionó a Chichén Itzá y tomó numerosas fotografías que publicó en Cités et ruines américaines (1863).
Augustus Le Plongeon lo llamó "Chaacmol" (más tarde renombrado "Chac Mool", que ha sido el término para describir todos los tipos de esta estatua que se encuentran en Mesoamérica).
En 1894, el Cónsul de los Estados Unidos en Yucatán, Edward Herbert Thompson, compró la Hacienda Chichén, que incluía las ruinas de Chichén Itzá.
Thompson es más famoso por dragar el Cenote Sagrado de 1904 a 1910, donde recuperó artefactos de oro, cobre y jade tallado, así como los primeros ejemplos de lo que se creía que eran telas mayas precolombinas y armas de madera.
La Revolución Mexicana y la siguiente inestabilidad del gobierno, así como la Primera Guerra Mundial, retrasaron el proyecto por una década.
Al mismo tiempo, el gobierno mexicano excavó y restauró El Castillo (Templo de Kukulcán) y la Gran Corte de Baile.
Thompson, quien estaba en los Estados Unidos en ese momento, nunca regresó a Yucatán.
En 1944, la Corte Suprema de México dictaminó que Thompson no había violado ninguna ley y devolvió a Chichén Itzá a sus herederos.
El primero fue patrocinado por National Geographic, y el segundo por intereses privados.
La ciudad fue construida sobre un terreno roto, que fue artificialmente nivelado con el fin de construir los principales grupos arquitectónicos, con el mayor esfuerzo que se gasta en la nivelación de las áreas de la pirámide del Castillo, y las Monjas, Osario y principales grupos del suroeste.
Muchos de estos edificios de piedra fueron pintados originalmente en colores rojo, verde, azul y púrpura.
Al igual que las catedrales góticas en Europa, los colores proporcionaron una mayor sensación de integridad y contribuyeron en gran medida al impacto simbólico de los edificios.
El edificio de estilo Puuc presenta las habituales fachadas superiores decoradas con mosaicos características del estilo, pero difieren de la arquitectura del corazón de Puuc en sus paredes de mampostería de bloques, a diferencia de las finas chapas de la región de Puuc.
En la base de las balaustradas de la escalera noreste están talladas las cabezas de una serpiente.
Después de varias salidas en falso, descubrieron una escalera debajo del lado norte de la pirámide.
El gobierno mexicano excavó un túnel desde la base de la escalera norte, por la escalera de la pirámide anterior hasta el templo oculto, y lo abrió a los turistas.
En un panel, uno de los jugadores ha sido decapitado; la herida emite corrientes de sangre en forma de serpientes retorciéndose.
En el extremo sur hay otro templo mucho más grande, pero en ruinas.
En el interior hay un gran mural, muy destruido, que representa una escena de batalla.
Está construido en una combinación de estilos maya y tolteca, con una escalera que asciende por cada uno de sus cuatro lados.
En su interior, los arqueólogos descubrieron una colección de grandes conos tallados en piedra, cuyo propósito se desconoce.
Su nombre proviene de una serie de altares en la parte superior de la estructura que están apoyados por pequeñas figuras talladas de hombres con brazos levantados, llamados "atlantes".
Este complejo es análogo al Templo B en la capital tolteca de Tula, e indica alguna forma de contacto cultural entre las dos regiones.
Este templo encierra o sepulta una antigua estructura llamada El Templo del Chac Mool.
Al sur del Grupo de las Mil Columnas hay un grupo de tres edificios más pequeños e interconectados.
Una sección de la fachada superior con un motivo de x y o se muestra delante de la estructura.
El Templo de Xtoloc es un templo recientemente restaurado fuera de la plataforma de Osario.
Entre el templo de Xtoloc y el Osario hay varias estructuras alineadas: la Plataforma de Venus, que es similar en diseño a la estructura del mismo nombre junto a Kukulkán (El Castillo), la Plataforma de las Tumbas y una estructura pequeña y redonda que no tiene nombre.
La Casa Colorada (español para "Casa Roja") es uno de los edificios mejor conservados en Chichén Itzá.
En 2009, el INAH restauró una pequeña cancha de pelota que lindaba con la pared trasera de la Casa Colorada.
El nombre de este edificio ha sido utilizado durante mucho tiempo por los mayas locales, y algunos autores mencionan que fue nombrado después de una pintura de ciervos sobre estuco que ya no existe.
Los españoles llamaron a este complejo Las Monjas, pero era un palacio gubernamental.
Estos textos mencionan con frecuencia a un gobernante con el nombre de Krishnaupakal.
Recibe su nombre de la escalera de caracol de piedra en el interior.
La fachada larga, orientada al oeste tiene siete puertas.
El extremo sur del edificio tiene una entrada.
Dentro de una de las cámaras, cerca del techo, hay una impresión pintada a mano.
La ubicación de la cueva ha sido bien conocida en los tiempos modernos.
E. Wyllys Andrews IV también exploró la cueva en la década de 1930.
El 15 de septiembre de 1959, José Humberto Gómez, un guía local, descubrió una pared falsa en la cueva.
Incluso antes de que se publicara el libro, Benjamin Norman y el barón Emanuel von Friedrichsthal viajaron a Chichén después de conocer a Stephens, y ambos publicaron los resultados de lo que encontraron.
En 1923, el gobernador Carrillo Puerto abrió oficialmente la carretera a Chichén Itzá.
En 1930, se abrió el Hotel Mayaland, justo al norte de la Hacienda Chichén, que había sido tomada por la Institución Carnegie.
En 1972, México promulgó la Ley Federal Sobre Monumentos y Zonas Arqueológicas, Artísticas e Históricas (Ley Federal sobre Monumentos y Sitios Arqueológicos, Artísticos e Históricos) que puso todos los monumentos precolombinos de la nación, incluidos los de Chichén Itzá, bajo propiedad federal.
Los guías turísticos también demostrarán un efecto acústico único en Chichén Itzá: un apretón de manos antes del frente de la escalera que la pirámide de El Castillo producirá por un eco que se asemeja al chirrido de un pájaro, similar al del quetzal según lo investigado por Declercq.
El INAH, que administra el sitio, ha cerrado una serie de monumentos al acceso público.
Originalmente un proyecto del promotor inmobiliario y ex senador del estado de Nueva York William H. Reynolds, el edificio fue construido por Walter Chrysler, el jefe de la Corporación Chrysler.
Un anexo se completó en 1952, y el edificio fue vendido por la familia Chrysler al año siguiente, con numerosos propietarios posteriores.
La época se caracterizó por profundos cambios sociales y tecnológicos.
Al año siguiente, Chrysler fue nombrado "Persona del Año" de la revista Time.
Después del final de la Primera Guerra Mundial, los arquitectos europeos y estadounidenses llegaron a ver el diseño simplificado como el epítome de la era moderna y los rascacielos Art Deco como un símbolo de progreso, innovación y modernidad.
Antes de su participación en la planificación del edificio, Reynolds era mejor conocido por desarrollar el parque de atracciones Dreamland de Coney Island.
En 1927, después de varios años de retrasos, Reynolds contrató al arquitecto William Van Alen para diseñar un edificio de cuarenta pisos allí.
Van Alen y Severance se complementaron, con Van Alen siendo un arquitecto original e imaginativo y Severance siendo un empresario astuto que manejó las finanzas de la empresa.
La propuesta fue cambiada nuevamente dos semanas después, con planes oficiales para un edificio de 63 pisos.
El edificio Chanin de 56 pisos adyacente también estaba en construcción.
Estos planes fueron aprobados en junio de 1928.
En su lugar, ideó un diseño alternativo para el edificio Reynolds, que se publicó en agosto de 1928.
Un contrato se adjudicó el 28 de octubre, y la demolición se completó el 9 de noviembre.
Desde finales de 1928 hasta principios de 1929, las modificaciones en el diseño de la cúpula continuaron.
Más abajo, el diseño fue afectado por la intención de Walter Chrysler de hacer el edificio la oficina central de Chrysler Corporation, y como tal, varios detalles arquitectónicos se modelaron después de productos del automóvil de Chrysler, como los ornamentos del capó de Plymouth (ver).
La construcción del edificio comenzó el 21 de enero de 1929.
A pesar de un ritmo frenético de construcción de acero de aproximadamente cuatro pisos por semana, ningún trabajador murió durante la construcción de la estructura de acero del rascacielos.
40 Wall Street y el edificio Chrysler comenzaron a competir por la distinción de "edificio más alto del mundo".
El 23 de octubre de 1929, una semana después de superar la altura del edificio Woolworth y un día antes de que comenzara el catastrófico accidente de Wall Street de 1929, la aguja fue ensamblada.
Incluso el New York Herald Tribune, que tenía una cobertura prácticamente continua de la construcción de la torre, no informó sobre la instalación de la aguja hasta días después de que la aguja había sido levantada.
En el vestíbulo del edificio, se dio a conocer una placa de bronce que decía "en reconocimiento a la contribución del Sr. Chrysler al avance cívico".
El edificio Chrysler fue tasado en $ 14 millones, pero estaba exento de impuestos municipales por una ley de 1859 que dio exenciones fiscales a los sitios propiedad de la Cooper Union.
La satisfacción de Van Alen por estos logros probablemente fue silenciada por la posterior negativa de Walter Chrysler a pagar el saldo de su tarifa arquitectónica.
Sin embargo, la demanda contra Chrysler disminuyó notablemente la reputación de Van Alen como arquitecto, lo que, junto con los efectos de la Gran Depresión y las críticas negativas, terminó arruinando su carrera.
En 1944, la corporación presentó planes para construir un anexo de 38 pisos al este del edificio, en 666 Third Avenue.
La piedra para el edificio original ya no se fabricaba, y tuvo que ser especialmente replicada.
La familia vendió el edificio en 1953 a William Zeckendorf por un precio estimado de 18 millones de dólares.
En ese momento, se informó que era la venta de bienes raíces más grande en la historia de la ciudad de Nueva York.
En 1961, los elementos de acero inoxidable del edificio, incluida la aguja, la corona, las gárgolas y las puertas de entrada, se pulieron por primera vez.
La compañía compró el edificio por 35 millones de dólares.
La aguja se sometió a una restauración que se completó en 1995.
La limpieza recibió el Premio Lucy G. Moses Preservation de New York Landmarks Conservancy en 1997.
En el junio de 2008, se relató que el Consejo de Inversión de Abu Dhabi estaba en negociaciones para comprar el interés económico del 75% de TMW, un interés del 15% de Tishman Speyer Properties en el edificio y una parte de la estructura detallista de Trylons al lado por US$800 millones.
Esto resultó en una disminución del 21% en el consumo total de energía del edificio, una disminución del 64% en el consumo de agua y una tasa de reciclaje del 81%.
La ética de la práctica filosófica”.
La filosofía es el pensamiento racionalmente crítico, de un tipo más o menos sistemático sobre la naturaleza general del mundo (metafísica o teoría de la existencia), la justificación de la creencia (epistemología o teoría del conocimiento) y la conducta de la vida (ética o teoría del valor).
La metafísica reemplaza los supuestos claros incorporados en tal concepción con un cuerpo racional y organizado de creencias sobre el mundo en su conjunto.
En el siglo XIX, el crecimiento de las universidades modernas de investigación llevó a la filosofía académica y otras disciplinas a profesionalizarse y especializarse.
En Contra los Lógicos, el filósofo pirrónico Sexto Empírico detalló la variedad de formas en que los antiguos filósofos griegos habían dividido la filosofía, señalando que esta división en tres partes fue acordada por Platón, Aristóteles, Jenócrates y los estoicos.
Otras antiguas tradiciones filosóficas influenciadas por Sócrates incluyeron el cinismo, el cirenaicismo, el estoicismo y el escepticismo académico.
Algunos pensadores medievales clave incluyen a San Agustín, Tomás de Aquino, Boecio, Anselmo y Roger Bacon.
Los principales filósofos modernos incluyen Spinoza, Leibniz, Locke, Berkeley, Hume y Kant.
La astronomía babilónica también incluyó muchas especulaciones filosóficas sobre la cosmología que pueden haber influido en los antiguos griegos.
La filosofía judía posterior vino bajo influencias intelectuales Occidentales fuertes e incluye los trabajos de Moisés Mendelssohn que marcó el comienzo de Haskalah (la Aclaración judía), existencialismo judío y Judaísmo de la Reforma.
La filosofía islámica es el trabajo filosófico originado en la tradición islámica y generalmente se hace en árabe.
La filosofía islámica temprana desarrolló las tradiciones filosóficas griegas en nuevas direcciones innovadoras.
La obra de Aristóteles fue muy influyente entre filósofos como Al-Kindi (siglo IX), Avicena (980 – junio de 1037) y Averroes (siglo XII).
Ibn Khaldun fue un pensador influyente en la filosofía de la historia.
Las tradiciones filosóficas indias comparten varios conceptos e ideas clave, que se definen de diferentes maneras y son aceptadas o rechazadas por las diferentes tradiciones.
La filosofía india comúnmente se agrupa basada en su relación con los Vedas y las ideas contenidas en ellos.
Las escuelas que se alinean con el pensamiento de los Upanishads, las llamadas tradiciones "ortodoxas" o "hindúes", a menudo se clasifican en seis daranas o filosofías: Sunkhya, Yoga, Nyaya, Vaisheshika, Mimams y Vedanta.
También reflejan una tolerancia para una diversidad de interpretaciones filosóficas dentro del hinduismo mientras comparten la misma base.
También hay otras escuelas de pensamiento que a menudo se ven como "hindúes", aunque no necesariamente ortodoxas (ya que pueden aceptar diferentes escrituras como normativas, como los Shaiva Agamas y los Tantras), estas incluyen diferentes escuelas de Shavismo como Pashupata, Shaiva Siddhanta, Shavismo tántrico no dual (es decir, Trika, Kaula, etc.).
La negación de que un ser humano posee un "yo" o "alma" es probablemente la enseñanza budista más famosa.
La filosofía jainista es una de las dos únicas tradiciones "poco ortodoxas" que sobreviven (junto con el budismo).
El pensamiento jainista sostiene que toda la existencia es cíclica, eterna e increada.
En estas regiones, el pensamiento budista se desarrolló en tradiciones filosóficas diferentes que usaron varias lenguas (como tibetano, chino y Pali).
La filosofía de la escuela Theravada es dominante en países del sudeste asiático como Sri Lanka, Birmania y Tailandia.
Después de la muerte de Buda, varios grupos comenzaron a sistematizar sus enseñanzas principales, eventualmente desarrollando sistemas filosóficos integrales llamados Abhidharma.
Había numerosas escuelas, sub-escuelas y tradiciones de la filosofía budista en la India antigua y medieval.
Estas tradiciones filosóficas desarrollaron teorías metafísicas, políticas y éticas como Tao, Yin y Yang, Ren y Li.
El neoconfucianismo llegó a dominar el sistema educativo durante la dinastía Song (960-1297), y sus ideas sirvieron como la base filosófica de los exámenes imperiales para la clase académica oficial.
Durante dinastías chinas posteriores como la dinastía Ming (1368-1644), así como en la dinastía coreana Joseon (1392-1897), un neoconfucianismo resurgente dirigido por pensadores como Wang Yangming (1472-1529) se convirtió en la escuela de pensamiento dominante y fue promovido por el estado imperial.
En la era moderna, los pensadores chinos incorporaron ideas de la filosofía occidental.
Por ejemplo, el Nuevo Confucianismo, liderado por figuras como Xiong Shili, se ha vuelto bastante influyente.
Otra tendencia en la filosofía japonesa moderna era la tradición de "Estudios Nacionales" (Kokugaku).
Durante el 17mo siglo, la filosofía etíope desarrolló una tradición literaria robusta como ejemplificado por Zera Yacob.
Otra característica de las cosmovisiones indígenas americanas fue su extensión de la ética a los animales y plantas no humanos.
La teoría de Teotl puede ser vista como una forma de panteísmo.
Sin embargo, los informes del Departamento de Educación de los Estados Unidos de la década de 1990 indican que pocas mujeres terminaron en filosofía, y que la filosofía es uno de los campos menos proporcionales al género en las humanidades, con mujeres que representan entre el 17% y el 30% de la facultad de filosofía según algunos estudios.
Ver también "Características y actitudes de la facultad de instrucción y el personal en las humanidades."
Sus investigaciones principales incluyen cómo vivir una buena vida e identificar estándares de moralidad.
Los epistemólogos examinan supuestas fuentes de conocimiento, incluyendo la experiencia perceptiva, la razón, la memoria y el testimonio.
Surgió temprano en la filosofía presocrática y se formalizó con Pyrrho, el fundador de la primera escuela occidental de escepticismo filosófico.
El empirismo pone énfasis en la evidencia observacional a través de la experiencia sensorial como fuente de conocimiento.
El racionalismo está asociado con el conocimiento a priori, que es independiente de la experiencia (como la lógica y las matemáticas).
La metafísica incluye la cosmología, el estudio del mundo en su totalidad y la ontología, el estudio del ser.
La esencia es el conjunto de atributos que hacen que un objeto sea lo que es fundamentalmente y sin el cual pierde su identidad, mientras que el accidente es una propiedad que el objeto tiene, sin la cual el objeto aún puede conservar su identidad.
Como el razonamiento sano es un elemento esencial de todas las ciencias, ciencias sociales y disciplinas de humanidades, la lógica se hizo una ciencia formal.
Nueva York: Oxford University Press.
Sin embargo, la mayoría de los estudiantes de filosofía académica más tarde contribuyen a la ley, el periodismo, la religión, las ciencias, la política, los negocios o diversas artes.
En la filosofía analítica, la filosofía del lenguaje investiga la naturaleza del lenguaje, las relaciones entre el lenguaje, los usuarios del lenguaje y el mundo.
Estos escritores fueron seguidos por Ludwig Wittgenstein (Tractatus Logico-Philosophicus), el Círculo de Viena, así como los positivistas lógicos, y Willard Van Orman Quine.
Criticó el convencionalismo porque condujo a la extraña consecuencia de que cualquier cosa puede denominarse convencionalmente con cualquier nombre.
Para hacer esto, señaló que las palabras y frases compuestas tienen un rango de corrección.
Sin embargo, hacia el final de Cratylus, había admitido que algunas convenciones sociales también se implicaron, y que había faltas en la idea que los fonemas tenían sentidos individuales.
Separó todas las cosas en categorías de especies y géneros.
Sin embargo, ya que Aristóteles tomó estas semejanzas para estar constituidas por una comunidad real de la forma, más a menudo se considera un defensor del "realismo moderado".
Este lektón era el significado (o sentido) de cada término.
Había varios filósofos notables de la lengua en el período medieval.
Los escolásticos del alto período medieval, como Ockham y John Duns Scotus, consideraron que la lógica era una scientia sermocinalis (ciencia del lenguaje).
Se analizaron intensamente los fenómenos de vaguedad y ambiguedad, y esto condujo a un creciente interés en los problemas relacionados con el uso de palabras sincategoremáticas como y, o, no, si y todos.
El supuesto de un término es la interpretación que se da de él en un contexto específico.
Tal esquema de clasificación es el precursor de las distinciones modernas entre uso y mención, y entre lenguaje y metalenguaje.
Una parte de la oración común es la palabra léxica, que se compone de sustantivos, verbos y adjetivos.
La semántica filosófica tiende a centrarse en el principio de la composicionalidad para explicar la relación entre las partes significativas y las oraciones completas.
Es posible usar el concepto de funciones para describir algo más que cómo funcionan los significados léxicos: también se pueden usar para describir el significado de una oración.
Una función proposicional es una operación del lenguaje que toma una entidad (en este caso, el caballo) como una entrada y produce un hecho semántico (es decir, la proposición que se representa por "El caballo es rojo").
¿Es la adquisición del lenguaje una facultad especial en la mente?
La primera es la perspectiva conductista, que dicta que no solo se aprende la masa sólida del lenguaje, sino que se aprende a través del condicionamiento.
Los modelos nativistas afirman que hay dispositivos especializados en el cerebro que se dedican a la adquisición del lenguaje.
Sapir y Whorf sugirieron que el lenguaje limitaba la medida en que los miembros de una "comunidad linguística" pueden pensar en ciertos temas (una hipótesis paralela a la novela de George Orwell Mil novecientos ochenta y cuatro).
Lo totalmente opuesto a la posición de Sapir-Whorf es la noción de que el pensamiento (o, más ampliamente, el contenido mental) tiene prioridad sobre el lenguaje.
Otro argumento es que es difícil explicar cómo los signos y símbolos en el papel pueden representar algo significativo a menos que algún tipo de significado sea infundido en ellos por el contenido de la mente.
Otra tradición de filósofos ha intentado mostrar que el lenguaje y el pensamiento son coextensivos, que no hay manera de explicar uno sin el otro.
Hasta cierto punto, los fundamentos teóricos de la semántica cognitiva (incluida la noción de encuadre semántico) sugieren la influencia del lenguaje sobre el pensamiento.
Hay estudios que demuestran que las lenguas dan forma a la forma en que las personas entienden la causalidad.
Sin embargo, los hablantes de español o japonés serían más propensos a decir "el jarrón se rompió".
Los hispanohablantes y japoneses no recordaban a los agentes de eventos accidentales como lo hicieron los angloparlantes.
En un estudio, se pidió a hablantes de alemán y español que describieran objetos que tenían asignación de género opuesto en esos dos idiomas.
Para describir un "puente", que es femenino en alemán y masculino en español, los hablantes de alemán dijeron "hermoso", "elegante", "frágil", "pacífico", "bonito" y "esbelto", y los hablantes de español dijeron "grande", "peligroso", "largo", "fuerte", "turbo" y "torreando".
Si cada alienígena era amistoso u hostil estaba determinado por ciertas características sutiles, pero a los participantes no se les dijo cuáles eran.
Por lo demás, los alienígenas permanecieron sin nombre.
Se concluyó que nombrar objetos nos ayuda a categorizarlos y memorizarlos.
Dentro de esta área, las cuestiones incluyen: la naturaleza de la sinonimia, los orígenes del significado en sí, nuestra aprehensión del significado y la naturaleza de la composición (la cuestión de cómo las unidades significativas del lenguaje están compuestas de partes significativas más pequeñas, y cómo el significado del todo se deriva del significado de sus partes).
La teoría ideacional del significado, más comúnmente asociada con el empirista británico John Locke, afirma que los significados son representaciones mentales provocadas por signos.
(Véase también la teoría pictórica del lenguaje de Wittgenstein).
Bogotá, Colombia: Harvard University Press.
La teoría de referencia del significado, también conocida colectivamente como externalismo semántico, considera que el significado es equivalente a aquellas cosas en el mundo que están realmente conectadas a los signos.
La formulación tradicional de tal teoría es que el significado de una oración es su método de verificación o falsificación.
En esta versión, la comprensión (y por lo tanto el significado) de una oración consiste en la capacidad del oyente para reconocer la demostración (matemática, empírica u otra) de la verdad de la oración.
Una teoría pragmática del significado es cualquier teoría en la que el significado (o comprensión) de una oración está determinado por las consecuencias de su aplicación.
Gottlob Frege fue un defensor de una teoría de referencia mediada.
Tal pensamiento es abstracto, universal y objetivo.
Los referentes son los objetos en el mundo que las palabras escogen.
Veía los nombres propios del tipo descrito anteriormente como "descripciones definidas abreviadas" (véase Teoría de las descripciones).
Tales frases denotan en el sentido de que hay un objeto que satisface la descripción.
Por cuenta de Frege, cualquier expresión de referencia tiene un sentido, así como un referente.
A pesar de las diferencias entre las opiniones de Frege y Russell, generalmente se agrupan como descriptivistas sobre nombres propios.
Considere el nombre de Aristóteles y las descripciones "el mayor estudiante de Platón", "el fundador de la lógica" y "el maestro de Alejandro".
Puede haber existido y no haber llegado a ser conocido por la posteridad en absoluto o puede haber muerto en la infancia.
Pero esto es profundamente contrario a la intuición.
Inevitablemente surgen preguntas sobre los temas circundantes.
David Kellogg Lewis propuso una respuesta digna a la primera pregunta exponiendo el punto de vista de que una convención es una regularidad en el comportamiento que se autoperpetúa racionalmente.
Noam Chomsky propuso que el estudio de la lengua se podría hacer en términos de la I-Idioma o lengua interna de personas.
Una fuente fructífera de investigación implica la investigación de las condiciones sociales que dan lugar a, o están asociados con, significados y lenguajes.
Las presunciones que sostienen cada punto de vista teórico son de interés para el filósofo del lenguaje.
La retórica es el estudio de las palabras particulares que las personas usan para lograr el efecto emocional y racional adecuado en el oyente, ya sea para persuadir, provocar, enamorar o enseñar.
También tiene aplicaciones para el estudio y la interpretación de la ley, y ayuda a dar una idea del concepto lógico del dominio del discurso.
La idea del lenguaje a menudo se relaciona con la de la lógica en su sentido griego como "logos", que significa discurso o dialéctica.
Heidegger combina la fenomenología con la hermenéutica de Wilhelm Dilthey.
Por ejemplo, Sein (ser), la palabra misma, está saturada de múltiples significados.
Heidegger afirma que la escritura es solo un complemento del habla, porque incluso los lectores construyen o contribuyen su propia "conversación" mientras leen.
En Verdad y Método, Gadamer describe el lenguaje como "el medio en el cual el entendimiento sustantivo y el acuerdo ocurren entre dos personas.
Paul Ricseur, por otra parte, propuso una hermenéutica que, reconectándose con el sentido griego original del término, enfatizó el descubrimiento de sentidos escondidos en los términos equívocos (o "símbolos") de la lengua ordinaria.
Les permite aprovechar y manipular eficazmente el mundo externo para crear significado para sí mismos y transmitir este significado a los demás.
Algunas figuras importantes en la historia de la semiótica son Charles Sanders Peirce, Roland Barthes y Roman Jakobson.
El romanticismo del siglo XIX enfatizó la agencia humana y el libre albedrío en la construcción del significado.
Las visiones humanistas son desafiadas por teorías biológicas de la lengua que consideran lenguas como fenómenos naturales.
En el neodarwinismo, Richard Dawkins y otros defensores de las teorías replicadoras culturales consideran los lenguajes como poblaciones de virus mentales.
Algunos han dicho que la expresión representa algo real, abstracto universal en el mundo llamado "rocas".
El tema aquí se puede explicar si examinamos la proposición "Sócrates es un Hombre".
Estas dos cosas se conectan de alguna manera o se superponen.
Otra perspectiva es considerar "hombre" como una propiedad de la entidad, "Sócrates".
Algunos de los miembros más prominentes de esta tradición de la semántica formal incluyen a Tarski, Carnap, Richard Montague y Donald Davidson.
No creían que las dimensiones sociales y prácticas del significado linguístico pudieran ser capturadas por cualquier intento de formalización utilizando las herramientas de la lógica.
Muchas de sus ideas han sido absorbidas por teóricos como Kent Bach, Robert Blandon, Paul Horwich y Stephen Neale.
En Word and Object, Quine pide a los lectores que imaginen una situación en la que se enfrentan a un grupo de indígenas previamente indocumentados, donde deben intentar dar sentido a las expresiones y gestos que hacen sus miembros.
Todo lo que se puede hacer es examinar la pronunciación como parte del comportamiento global del individuo, y luego usar estas observaciones para interpretar el significado de todas las demás expresiones.
Para Quine, como para Wittgenstein y Austin, el significado no es algo que se asocia con una sola palabra u oración, sino que es algo que, si se puede atribuir, solo se puede atribuir a un idioma completo.
Los casos específicos de vaguedad que más interesan a los filósofos del lenguaje son aquellos en los que la existencia de "casos límite" hace aparentemente imposible decir si un predicado es verdadero o falso.
La filosofía de las matemáticas es la rama de la filosofía que estudia los supuestos, fundamentos e implicaciones de las matemáticas.
Hoy en día, algunos filósofos de las matemáticas tienen como objetivo dar cuenta de esta forma de investigación y sus productos tal como están, mientras que otros enfatizan un papel para sí mismos que va más allá de la simple interpretación del análisis crítico.
La filosofía griega de las matemáticas fue fuertemente influenciada por su estudio de la geometría.
Por lo tanto, 3, por ejemplo, representaba una cierta multitud de unidades, y por lo tanto no era "verdaderamente" un número.
Estas ideas griegas más tempranas de números fueron puestas patas arriba más tarde por el descubrimiento de la irracionalidad de la raíz cuadrada de dos.
Según la leyenda, sus compañeros pitagóricos estaban tan traumatizados por este descubrimiento que asesinaron a Hippasus para evitar que difundiera su idea herética.
Es un profundo enigma que, por un lado, las verdades matemáticas parecen tener una inevitabilidad convincente, pero por otro lado, la fuente de su "verdad" sigue siendo esquiva.
Tres escuelas, el formalismo, el intuicionismo y el logicismo, surgieron en este momento, en parte como respuesta a la preocupación cada vez más generalizada de que las matemáticas en su forma actual, y el análisis en particular, no estaban a la altura de los estándares de certeza y rigor que se habían dado por sentados.
A medida que el siglo se desarrolló, el foco inicial de preocupación se expandió a una exploración abierta de los axiomas fundamentales de las matemáticas, el enfoque axiomático se ha dado por sentado desde la época de Euclides alrededor del 300 aC como la base natural para las matemáticas.
En matemáticas, como en física, habían surgido ideas nuevas e inesperadas y se avecinaban cambios significativos.
No creo que las dificultades que la filosofía encuentra con las matemáticas clásicas hoy en día sean dificultades genuinas; y creo que las interpretaciones filosóficas de las matemáticas que se nos ofrecen en todas partes están equivocadas, y que la "interpretación filosófica" es justo lo que las matemáticas no necesitan.
Muchos matemáticos que trabajan han sido realistas matemáticos; se ven a sí mismos como descubridores de objetos naturales.
Ciertos principios (por ejemplo, para cualquier dos objetos, hay una colección de objetos que consisten precisamente en esos dos objetos) podrían verse directamente como verdaderos, pero la conjetura de la hipótesis del continuo podría resultar indecidible solo sobre la base de tales principios.
Tanto la cueva de Platón como el platonismo tienen conexiones significativas, no solo superficiales, porque las ideas de Platón fueron precedidas y probablemente influenciadas por los pitagóricos enormemente populares de la antigua Grecia, que creían que el mundo era, literalmente, generado por los números.
Este punto de vista tiene similitudes con muchas cosas que Husserl dijo sobre las matemáticas, y apoya la idea de Kant de que las matemáticas son sintéticas a priori.)
El platonismo de sangre completa es una variación moderna del platonismo, que es en reacción al hecho de que se puede demostrar que existen diferentes conjuntos de entidades matemáticas dependiendo de los axiomas y las reglas de inferencia empleadas (por ejemplo, la ley del medio excluido y el axioma de elección).
El realismo teórico de conjuntos (también platonismo teórico de conjuntos), una posición defendida por Penelope Maddy, es la opinión de que la teoría de conjuntos se trata de un único universo de conjuntos.
Atribuyeron la paradoja a la "circularidad viciosa" y construyeron lo que llamaron la teoría de tipos ramificados para tratar con ella.
Incluso Russell dijo que este axioma no pertenecía realmente a la lógica.
Frege requería la Ley Básica V para poder dar una definición explícita de los números, pero todas las propiedades de los números se pueden derivar del principio de Hume.
Pero sí permite que el matemático que trabaja continúe en su trabajo y deje tales problemas al filósofo o científico.
Hilbert pretendió mostrar la consistencia de sistemas matemáticos de la asunción que la "aritmética finitary" (un subsistema de la aritmética habitual de los números enteros positivos, elegidos para ser filosóficamente indiscutible) era consecuente.
Por lo tanto, con el fin de demostrar que cualquier sistema axiomático de las matemáticas es, de hecho, consistente, primero hay que asumir la consistencia de un sistema de matemáticas que es en un sentido más fuerte que el sistema para ser probado consistente.
Otros formalistas, como Rudolf Carnap, Alfred Tarski y Haskell Curry, consideraron que las matemáticas eran la investigación de los sistemas formales de axiomas.
Cuantos más juegos estudiemos, mejor.
La crítica principal del formalismo es que las ideas matemáticas reales que ocupan los matemáticos están muy lejos de los juegos de manipulación de cuerdas mencionados anteriormente.
Brouwer, el fundador del movimiento, sostuvo que los objetos matemáticos surgen de las formas a priori de las voliciones que informan la percepción de los objetos empíricos.
El axioma de elección también es rechazado en la mayoría de las teorías de conjuntos intuicionistas, aunque en algunas versiones es aceptado.
En este punto de vista, las matemáticas son un ejercicio de la intuición humana, no un juego jugado con símbolos sin sentido.
Del mismo modo, todos los demás números enteros se definen por sus lugares en una estructura, la línea numérica.
Sin embargo, su afirmación central solo se refiere a qué tipo de entidad es un objeto matemático, no a qué tipo de existencia tienen los objetos o estructuras matemáticos (en otras palabras, no a su ontología).
Se cree que las estructuras tienen una existencia real pero abstracta e inmaterial.
Se considera que las estructuras existen en la medida en que algún sistema concreto las ejemplifica.
Al igual que el nominalismo, el enfoque post rem niega la existencia de objetos matemáticos abstractos con propiedades distintas a su lugar en una estructura relacional.
Se sostiene que las matemáticas no son universales y no existen en ningún sentido real, excepto en los cerebros humanos.
Sin embargo, la mente humana no tiene ningún derecho especial sobre la realidad o los enfoques construidos a partir de las matemáticas.
El tratamiento más accesible, famoso e infame de esta perspectiva es De dónde vienen las matemáticas, de George Lakoff y Rafael E. Núñez.
Franklin, James (2014), "Una filosofía realista aristotélica de las matemáticas", Palgrave Macmillan, Basingstoke; Franklin, James (2021), "Las matemáticas como una ciencia de la realidad no abstracta: filosofías realistas aristotélicas de las matemáticas", Fundamentos de la ciencia 25.
La aritmética euclidiana desarrollada por John Penn Mayberry en su libro The Foundations of Mathematics in the Theory of Sets también cae en la tradición realista aristotélica.
Edmund Husserl, en el primer volumen de sus Investigaciones Lógicas, llamado "Los Prolegómenos de la Lógica Pura", criticó el psicologismo a fondo y trató de distanciarse de él.
Es decir, dado que la física necesita hablar de electrones para decir por qué las bombillas se comportan como lo hacen, entonces los electrones deben existir.
Aboga por la existencia de entidades matemáticas como la mejor explicación para la experiencia, despojando así a las matemáticas de ser distintas de las otras ciencias.
Esto surgió de la afirmación cada vez más popular a finales del siglo XX de que no se podía demostrar que existiera un solo fundamento de las matemáticas.
Un argumento matemático puede transmitir la falsedad de la conclusión a las premisas tan bien como puede transmitir la verdad de las premisas a la conclusión.
Él dio un argumento detallado para esto en New Directions.
Si las matemáticas son tan empíricas como las otras ciencias, entonces esto sugiere que sus resultados son tan falibles como los suyos, y tan contingentes.
Para una filosofía de las matemáticas que intenta superar algunas de las deficiencias de los enfoques de Quine y Godel tomando aspectos de cada uno, véase Realismo en matemáticas de Penélope Maddy.
Comenzó con la "intermediación" de los axiomas de Hilbert para caracterizar el espacio sin coordinarlo, y luego agregó relaciones adicionales entre los puntos para hacer el trabajo realizado anteriormente por los campos vectoriales.
Por esta razón, no hay problemas metafísicos o epistemológicos especiales para las matemáticas.
Sin embargo, mientras que en una visión empirista la evaluación es una especie de comparación con la "realidad", los constructivistas sociales enfatizan que la dirección de la investigación matemática está dictada por las modas del grupo social que la realiza o por las necesidades de la sociedad que la financia.
Pero los constructivistas sociales argumentan que las matemáticas se basan en mucha incertidumbre: a medida que la práctica matemática evoluciona, el estado de las matemáticas anteriores se pone en duda y se corrige en la medida en que es requerido o deseado por la comunidad matemática actual.
La naturaleza social de las matemáticas se destaca en sus subculturas.
Los constructivistas sociales ven el proceso de "hacer matemáticas" como realmente creando el significado, mientras que los realistas sociales ven una deficiencia ya sea de la capacidad humana para abstraer, o del sesgo cognitivo humano, o de la inteligencia colectiva de los matemáticos como la prevención de la comprensión de un universo real de objetos matemáticos.
Más recientemente, Paul Ernest ha formulado explícitamente una filosofía constructivista social de las matemáticas.
Por ejemplo, las herramientas de la linguística no se aplican generalmente a los sistemas de símbolos de las matemáticas, es decir, las matemáticas se estudian de una manera marcadamente diferente de otras lenguas.
Sin embargo, los métodos desarrollados por Frege y Tarski para el estudio del lenguaje matemático han sido extendidos en gran medida por el estudiante de Tarski Richard Montague y otros linguistas que trabajan en semántica formal para mostrar que la distinción entre lenguaje matemático y lenguaje natural puede no ser tan grande como parece.
La afirmación de que "todas" las entidades postuladas en las teorías científicas, incluidos los números, deben aceptarse como reales está justificada por el holismo de confirmación.
Field desarrolló sus puntos de vista en el ficcionalismo.
El argumento se basa en la idea de que se puede dar una explicación naturalista satisfactoria de los procesos de pensamiento en términos de procesos cerebrales para el razonamiento matemático junto con todo lo demás.
Otra línea de defensa es mantener que los objetos abstractos son relevantes para el razonamiento matemático de una manera que no sea causal, y no análoga a la percepción.
A modo de ejemplo, proporcionan dos pruebas de la irracionalidad de .
Era bien conocido por su noción de un hipotético "Libro" que contenía las pruebas matemáticas más elegantes o hermosas.
De la misma manera, sin embargo, los filósofos de las matemáticas han tratado de caracterizar lo que hace que una prueba sea más deseable que otra cuando ambas son lógicamente sólidas.
La filosofía de la mente es una rama de la filosofía que estudia la ontología y la naturaleza de la mente y su relación con el cuerpo.
El dualismo y el monismo son las dos escuelas centrales de pensamiento sobre el problema mente-cuerpo, aunque han surgido puntos de vista matizados que no encajan perfectamente en una u otra categoría.
Hart, W.D. (1996) "dualismo", en Samuel Guttenplan (org) Un compañero de la filosofía de la mente, Blackwell, Oxford, 265-7.
Pinel, J. Psicobiología, (1990) Prentice Hall, Inc. LeDoux, J. (2002) El yo sináptico: cómo nuestros cerebros se convierten en quienes somos, Nueva York: Ping-ino vikingo.
Predicados psicológicos", en W. H. Capitan y D. D. Merrill, eds.,
En segundo lugar, los estados intencionales de conciencia no tienen sentido en el fisicalismo no reductor.
El deseo de alguien de una rebanada de pizza, por ejemplo, tenderá a hacer que esa persona mueva su cuerpo de una manera específica y en una dirección específica para obtener lo que quiere.
Robinson, H. (1983): "dualismo aristotélico", estudios de Oxford en filosofía antigua 1, 123-44.
Casi con toda seguridad negarían que la mente simplemente es el cerebro, o viceversa, encontrando la idea de que solo hay una entidad ontológica en juego para ser demasiado mecanicista o ininteligible.
Entonces, por ejemplo, uno puede preguntar razonablemente cómo se siente un dedo quemado, o cómo se ve un cielo azul, o cómo suena la música agradable para una persona.
Hay qualia involucrados en estos eventos mentales que parecen particularmente difíciles de reducir a algo físico.
Por lo tanto, el dualismo debe explicar cómo la conciencia afecta la realidad física.
El conocimiento, sin embargo, es aprehendido por el razonamiento de la base a la consecuencia.
La idea básica es que uno puede imaginar el propio cuerpo, y por lo tanto concebir la existencia del propio cuerpo, sin que ningún estado consciente esté asociado con este cuerpo.
Otros, como Dennett, han argumentado que la noción de un zombi filosófico es un concepto incoherente o improbable.
Es la opinión de que los estados mentales, como las creencias y los deseos, interactúan causalmente con los estados físicos.
El argumento de Descartes depende de la premisa de que lo que Seth cree que son ideas "claras y distintas" en su mente son necesariamente ciertas.
Por ejemplo, Joseph Agassi sugiere que varios descubrimientos científicos realizados desde principios del siglo XX han socavado la idea del acceso privilegiado a las propias ideas.
Este punto de vista fue defendido más prominentemente por Gottfried Leibniz.
Estas propiedades emergentes tienen un estado ontológico independiente y no pueden reducirse o explicarse en términos del sustrato físico del que emergen.
El epifenomenalismo es una doctrina formulada por primera vez por Thomas Henry Huxley.
Este punto de vista ha sido defendido por Frank Jackson.
El panpsiquismo es la visión de que toda la materia tiene un aspecto mental, o, alternativamente, todos los objetos tienen un centro unificado de experiencia o punto de vista.
Un ejemplo de estos grados dispares de libertad es dado por Allan Wallace, quien señala que es "experimentalmente evidente que uno puede estar físicamente incómodo, por ejemplo, mientras realiza un entrenamiento físico extenuante, mientras está mentalmente alegre; por el contrario, uno puede estar mentalmente angustiado mientras experimenta comodidad física".
Los estados mentales pueden causar cambios en los estados físicos y viceversa.
El dualismo experiencial es aceptado como el marco conceptual del Budismo Madhyamaka.
Al negar la autoexistencia independiente de todos los fenómenos que componen el mundo de nuestra experiencia, la visión Madhyamaka se aparta tanto del dualismo de sustancia de Descartes como del monismo de sustancia, es decir, el fisicalismo, que es característico de la ciencia moderna.
De hecho, el fisicalismo, o la idea de que la materia es la única sustancia fundamental de la realidad, es explícitamente rechazado por el budismo.
Mientras que los primeros comúnmente tienen masa, ubicación, velocidad, forma, tamaño y muchos otros atributos físicos, estos no son generalmente característicos de los fenómenos mentales.
La naturaleza fundamentalmente dispar de la realidad ha sido central para las formas de filosofías orientales durante más de dos milenios.
El monismo fisicalista afirma que la única sustancia existente es física, en cierto sentido de ese término para ser aclarada por nuestra mejor ciencia.
Aunque el idealismo puro, como el de George Berkeley, es poco común en la filosofía occidental contemporánea, una variante más sofisticada llamada panpsiquismo, según la cual la experiencia mental y las propiedades pueden estar en la base de la experiencia física y las propiedades, ha sido adoptada por algunos filósofos como Alfred North Whitehead y David Ray Griffin.
Una tercera posibilidad es aceptar la existencia de una sustancia básica que no es ni física ni mental.
Los informes introspectivos sobre la propia vida mental interior no están sujetos a un examen cuidadoso de precisión y no pueden usarse para formar generalizaciones predictivas.
Paralelamente a estos desarrollos en psicología, se desarrolló un conductismo filosófico (a veces llamado conductismo lógico).
Estos filósofos razonaron que, si los estados mentales son algo material, pero no conductual, entonces los estados mentales son probablemente idénticos a los estados internos del cerebro.
Según las teorías de identidad simbólica, el hecho de que un cierto estado cerebral esté conectado con un solo estado mental de una persona no tiene por qué significar que exista una correlación absoluta entre los tipos de estado mental y los tipos de estado cerebral.
Finalmente, la idea de Wittgenstein del sentido como el uso llevó a una versión del funcionalismo como una teoría del sentido, adelante desarrollado por Wilfrid Sellars y Gilbert Harman.
Por lo tanto, surge la pregunta de si todavía puede haber un fisicalismo no reductivo.
Davidson utiliza la tesis de la superveniencia: los estados mentales sobrevienen en los estados físicos, pero no son reducibles a ellos.
El cerebro pasa de un momento a otro; así, el cerebro tiene identidad a través del tiempo.
Una analogía del yo o del yo sería la llama de una vela.
La llama muestra un tipo de continuidad en que la vela no se apaga mientras está ardiendo, pero realmente no hay ninguna identidad de la llama de un momento a otro a lo largo del tiempo.
Del mismo modo, es una ilusión que uno es el mismo individuo que entró en clase esta mañana.
Esto es análogo a las propiedades físicas del cerebro que dan lugar a un estado mental.
Las Tierras de la Iglesia a menudo invocan el destino de otras teorías populares erróneas y ontologías que han surgido en el curso de la historia.
Algunos filósofos argumentan que esto se debe a que hay una confusión conceptual subyacente.
Más bien, debería aceptarse simplemente que la experiencia humana puede describirse de diferentes maneras, por ejemplo, en un vocabulario mental y biológico.
El cerebro es simplemente el contexto equivocado para el uso del vocabulario mental: la búsqueda de estados mentales del cerebro es, por lo tanto, un error de categoría o una especie de falacia de razonamiento.
Y es característico de un estado mental que tenga alguna cualidad experiencial, por ejemplo, de dolor, que duela.
La existencia de eventos cerebrales, en sí mismos, no puede explicar por qué están acompañados por estas experiencias cualitativas correspondientes.
Esto se deduce de una suposición sobre la posibilidad de explicaciones reductoras.
El filósofo alemán del siglo XX Martin Heidegger criticó los supuestos ontológicos que sustentan tal modelo reductor, y afirmó que era imposible dar sentido a la experiencia en estos términos.
Este problema de explicar los aspectos introspectivos en primera persona de los estados mentales y la conciencia en general en términos de neurociencia cuantitativa en tercera persona se llama la brecha explicativa.
Hay dos categorías separadas involucradas y una no se puede reducir a la otra.
Para Nagel, la ciencia aún no es capaz de explicar la experiencia subjetiva porque aún no ha llegado al nivel o tipo de conocimiento que se requiere.
Esta propiedad de los estados mentales implica que tienen contenidos y referentes semánticos y, por lo tanto, se les pueden asignar valores de verdad.
Pero las ideas o juicios mentales son verdaderos o falsos, entonces, ¿cómo pueden los estados mentales (ideas o juicios) ser procesos naturales?
Si el hecho es verdadero, entonces la idea es verdadera; de lo contrario, es falsa.
Dado que los procesos mentales están íntimamente relacionados con los procesos corporales, las descripciones que las ciencias naturales proporcionan a los seres humanos juegan un papel importante en la filosofía de la mente.
Dentro del campo de la neurobiología, hay muchas subdisciplinas que se ocupan de las relaciones entre los estados y procesos mentales y físicos: La neurofisiología sensorial investiga la relación entre los procesos de percepción y estimulación.
Por último, la biología evolutiva estudia los orígenes y el desarrollo del sistema nervioso humano y, en la medida en que esta es la base de la mente, también describe el desarrollo ontogenético y filogenético de los fenómenos mentales a partir de sus etapas más primitivas.
Un ejemplo sencillo es la multiplicación.
Esta pregunta ha sido impulsada a la vanguardia de muchos debates filosóficos debido a las investigaciones en el campo de la inteligencia artificial (IA).
El objetivo de la IA fuerte, por el contrario, es una computadora con conciencia similar a la de los seres humanos.
La prueba de Turing ha recibido muchas críticas, entre las cuales la más famosa es probablemente el experimento de pensamiento de habitación chino formulado por Searle.
La psicología investiga las leyes que unen estos estados mentales entre sí o con entradas y salidas al organismo humano.
Una ley de la psicología de las formas dice que los objetos que se mueven en la misma dirección se perciben como relacionados entre sí.
Incluye la investigación sobre la inteligencia y el comportamiento, especialmente centrándose en cómo se representa, procesa y transforma la información (en facultades como la percepción, el lenguaje, la memoria, el razonamiento y la emoción) dentro de los sistemas nerviosos (humanos u otros animales) y las máquinas (por ejemplo, computadoras).
Sin embargo, el trabajo de Hegel difiere radicalmente del estilo de la filosofía de la mente angloamericana.
La fenomenología, fundada por Edmund Husserl, se centra en los contenidos de la mente humana (ver noema) y cómo los procesos dan forma a nuestras experiencias.
Este es el caso de los deterministas materialistas.
Algunos llevan este razonamiento un paso más allá: las personas no pueden determinar por sí mismas lo que quieren y lo que hacen.
Los que adoptan esta posición sugieren que la pregunta "¿Somos libres?"
No es apropiado identificar la libertad con la indeterminación.
El compatibilista más importante en la historia de la filosofía fue David Hume.
Estos filósofos afirman que el curso del mundo está a) no completamente determinado por la ley natural donde la ley natural es interceptada por una agencia físicamente independiente, b) determinada solo por la ley natural indeterminista, o c) determinada por la ley natural indeterminista en línea con el esfuerzo subjetivo de la agencia físicamente no reducible.
Ellos argumentan de la siguiente manera: si nuestra voluntad no está determinada por nada, entonces deseamos lo que deseamos por pura casualidad.
La idea de un yo como núcleo esencial inmutable deriva de la idea de un alma inmaterial.
Mantranga, el principal órgano de gobierno de estos estados, consistía en el Rey, el Primer Ministro, el Comandante en jefe del ejército y el Sacerdote Principal del Rey.
El Arthashastra proporciona una explicación de la ciencia de la política para un gobernante sabio, las políticas para los asuntos exteriores y las guerras, el sistema de un estado espía y la vigilancia y la estabilidad económica del estado.
Las filosofías principales durante el período, Confucianism, Legalism, Mohism, Agrarianism y Taoism, cada uno tenía un aspecto político a sus escuelas filosóficas.
El legalismo abogaba por un gobierno altamente autoritario basado en castigos y leyes draconianas.
A finales del período antiguo, sin embargo, la visión "tradicionalista" Ashari del Islam había triunfado en general.
Sin embargo, en el pensamiento occidental, generalmente se supone que era un área específica peculiar meramente para los grandes filósofos del Islam: al-Kindi (Alkindus), al-Farabi (Abunaser), Ibn Sina (Avicenna), Ibn Bajjah (Avempace) e Ibn Rushd (Averroes).
Por ejemplo, las ideas de Khawarij en los primeros años de la historia islámica en Khilafa y Ummah, o ese del Islam chiíta en el concepto de Imamah se consideran pruebas del pensamiento político.
El aristotelismo floreció cuando la Edad de Oro islámica vio surgir una continuación de los filósofos peripatéticos que implementaron las ideas de Aristóteles en el contexto del mundo islámico.
Otros filósofos políticos notables del tiempo incluyen Nizam al-Mulk, un erudito persa y visir del Imperio de Seljuq que formó Siyasatnama o el "Libro de Gobierno" en inglés.
Tal vez el filósofo político más influyente de la Europa medieval fue Santo Tomás de Aquino, que ayudó a reintroducir las obras de Aristóteles, que sólo se habían transmitido a la Europa católica a través de la España musulmana, junto con los comentarios de Averroes.
Otros, como Nicole Oresme en su Livre de Politiques, negaron categóricamente este derecho a derrocar a un gobernante injusto.
Ese trabajo, así como Los Discursos, un análisis riguroso de la antiguedad clásica, hizo mucho para influir en el pensamiento político moderno en el Oeste.
En cualquier caso, Maquiavelo presenta una visión pragmática y algo consecuencialista de la política, por la cual el bien y el mal son meros medios utilizados para lograr un fin, es decir, la adquisición y el mantenimiento del poder absoluto.
Estos teóricos fueron impulsados por dos preguntas básicas: una, por qué derecho o necesidad forman los estados; y dos, cuál podría ser la mejor forma para un estado.
El término "gobierno" se referiría a un grupo específico de personas que ocupaban las instituciones del estado y creaban las leyes y ordenanzas por las cuales el pueblo, incluido él mismo, estaría obligado.
También se puede entender como la idea de libre mercado aplicada al comercio internacional.
El crítico más abierto de la iglesia en Francia era Fran?ois Marie Arouet de Voltaire, una cifra representativa de la iluminación.
Lo único que lamento al morir es que no puedo ayudarte en esta noble empresa, la más fina y respetable que la mente humana pueda señalar.
Locke se puso de pie para refutar la teoría política paternalmente fundada del señor Robert Filmer a favor de un sistema natural basado en la naturaleza en un sistema dado particular.
A diferencia de la visión preponderante de Aquino sobre la salvación del alma del pecado original, Locke cree que la mente del hombre viene a este mundo como tabula rasa.
Aunque uno podría estar preocupado por las restricciones a la libertad por parte de monarcas benévolos o aristócratas, la preocupación tradicional es que cuando los gobernantes no rinden cuentas políticamente a los gobernados, gobernarán en sus propios intereses, en lugar de los intereses de los gobernados.
La justicia implica deberes que son deberes perfectos, es decir, deberes que están correlacionados con derechos.
Sobre la libertad para discutir la igualdad de género en la sociedad.
La Libertad de los Antiguos era la libertad republicana participativa, que dio a los ciudadanos el derecho de influir directamente en la política a través de debates y votos en la asamblea pública.
La antigua libertad también se limitaba a sociedades relativamente pequeñas y homogéneas, en las que la gente podía reunirse convenientemente en un solo lugar para realizar transacciones públicas.
En cambio, los votantes elegirían representantes, que deliberarían en el Parlamento en nombre del pueblo y salvarían a los ciudadanos de la necesidad de una participación política diaria.
En Leviathan, Hobbes expuso su doctrina de la fundación de estados y gobiernos legítimos y creación de una ciencia objetiva de la moralidad.
En ese estado, cada persona tendría un derecho, o licencia, a todo en el mundo.
Publicado en 1762, se convirtió en una de las obras más influyentes de la filosofía política en la tradición occidental.
Los que se creen dueños de los demás son, en verdad, más esclavos que ellos.
La revolución industrial produjo una revolución paralela en el pensamiento político.
A mediados del siglo XIX, se desarrolló el marxismo, y el socialismo en general ganó un creciente apoyo popular, principalmente de la clase obrera urbana.
A diferencia de Marx, que creía en el materialismo histórico, Hegel creía en la Fenomenología del Espíritu.
En el mundo angloamericano, el antiimperialismo y el pluralismo comenzaron a ganar vigencia a principios del siglo XX.
Esta fue la época de Jean-Paul Sartre y Louis Althusser, y las victorias de Mao Zedong en Chína y Fidel Castro en Cuba, así como los acontecimientos de mayo de 1968, llevaron a un mayor interés en la ideología revolucionaria, especialmente por la Nueva Izquierda.
El colonialismo y el racismo fueron cuestiones importantes que surgieron.
El auge del feminismo, los movimientos sociales LGBT y el fin del dominio colonial y de la exclusión política de minorías como los afroamericanos y las minorías sexuales en el mundo desarrollado ha llevado a que el pensamiento feminista, postcolonial y multicultural se vuelva significativo.
Rawls utilizó un experimento mental, la posición original, en la que los partidos representativos eligen los principios de justicia para la estructura básica de la sociedad desde detrás de un velo de ignorancia.
Contemporáneamente con el surgimiento de la ética analítica en el pensamiento angloamericano, en Europa, surgieron varias nuevas líneas de filosofía dirigidas a la crítica de las sociedades existentes entre los años 1950 y 1980.
A lo largo de líneas algo diferentes, un número de otros pensadores continentales - todavía en gran parte bajo la influencia del marxismo - puso nuevos énfasis en el estructuralismo y en un "retorno a Hegel".
Otro debate se desarrolló alrededor de las críticas (distintas) de la teoría política liberal hechas por Michael Walzer, Michael Sandel y Charles Taylor.
Los comunitarios tienden a apoyar un mayor control local, así como las políticas económicas y sociales que fomentan el crecimiento del capital social.
Un par de perspectivas políticas superpuestas que surgen hacia el final del siglo XX son el republicanismo (o neo- o republicanismo cívico) y el enfoque de capacidad.
Para un republicano, el mero estatus de esclavo, independientemente de cómo se trate a ese esclavo, es objetable.
Tanto el enfoque de la capacidad como el republicanismo tratan la elección como algo que debe ser financiado.
Notable para las teorías que los seres humanos son animales sociales, y que la polis (ciudad-estado griega antigua) existió para traer la buena vida apropiada a tales animales.
Burke fue uno de los mayores partidarios de la Revolución Americana.
Chomsky es un destacado crítico de la política exterior de Estados Unidos, el neoliberalismo y el capitalismo de Estado contemporáneo, el conflicto israelí-palestino y los principales medios de comunicación.
William E. Connolly: Ayudó a introducir la filosofía posmoderna en la teoría política y promovió nuevas teorías del pluralismo y la democracia agonista.
Thomas Hill Green: pensador liberal moderno y partidario temprano de la libertad positiva.
Sus primeros trabajos fueron fuertemente influenciados por la Escuela de Frankfurt.
Abogó por el capitalismo de libre mercado en el que el papel principal del Estado es mantener el estado de derecho y dejar que se desarrolle el orden espontáneo.
David Hume: Hume criticó la teoría del contrato social de John Locke y otros como basada en un mito de algún acuerdo real.
Más famoso por la Declaración de Independencia de los Estados Unidos.
Argumentó que se necesitaba una organización internacional para preservar la paz mundial.
Se marchó de Hobbes en esto, basado en la asunción de una sociedad en la cual los valores morales son independientes de la autoridad gubernamental y extensamente compartidos, abogó por un gobierno con el poder limitado a la protección de la propiedad personal.
Uno de los fundadores del marxismo occidental.
Dio una cuenta del arte de gobernar en un punto de vista realista en lugar de confiar en el idealismo.
Como teórico político, creía en la separación de poderes y propuso un conjunto completo de controles y equilibrios que son necesarios para proteger los derechos de un individuo de la tiranía de la mayoría.
Introducido el concepto de "dessublimación represiva", en el que el control social puede operar no sólo por el control directo, sino también por la manipulación del deseo.
Creó el concepto de ideología en el sentido de creencias (verdaderas o falsas) que dan forma y controlan las acciones sociales.
Mencio: Uno de los pensadores más importantes de la escuela confuciana, es el primer teórico en hacer un argumento coherente a favor de una obligación de los gobernantes a los gobernados.
Montesquieu: Analizada la protección del pueblo por un "equilibrio de poderes" en las divisiones de un estado.
Sus intérpretes han debatido el contenido de su filosofía política.
Platón: Escribió un largo diálogo La República en el que expuso su filosofía política: los ciudadanos deben dividirse en tres categorías.
Ayn Rand: Fundadora del Objetivismo y principal impulsora de los movimientos Objetivista y Libertario en la América de mediados del siglo XX.
El gobierno debía ser separado de la economía de la misma manera y por las mismas razones estaba separado de la religión.
Adam Smith: A menudo se dice que fundó la economía moderna; explicó la aparición de beneficios económicos del comportamiento egoísta ("la mano invisible") de artesanos y comerciantes.
Sócrates: Ampliamente considerado el fundador de la filosofía política occidental, a través de su influencia hablada en los contemporáneos atenienses; ya que Sócrates nunca escribió nada, gran parte de lo que sabemos sobre él y sus enseñanzas proviene de su estudiante más famoso, Platón.
Max Stirner: Pensador importante dentro del anarquismo y el principal representante de la corriente anarquista conocida como anarquismo individualista.
Otras formas de filosofía social incluyen la filosofía política y la jurisprudencia, que se ocupan en gran medida de las sociedades de estado y gobierno y su funcionamiento.
La filosofía presócrata, también conocida como filosofía griega temprana, es la filosofía griega antigua antes de Sócrates.
Su trabajo y su escritura se han perdido casi por completo.
La filosofía presócrata comenzó en el siglo VI a.C. con los tres milesios: Tales, Anaximandro y Anaxímenes.
Jenófanes es conocido por su crítica del antropomorfismo de los dioses.
La escuela eleática (Parménides, Zenón de Elea y Melissus) siguió en el siglo V aC.
Anaxágoras y Empédocles ofrecieron un relato plural de cómo se creó el universo.
Fue utilizado por primera vez por el filósofo alemán J.A. Eberhard como "vorsokratische Philosophie" a finales del siglo XVIII.
El término viene con inconvenientes, ya que varios de los presocráticos estaban muy interesados en la ética y en cómo vivir la mejor vida.
Según James Warren, la distinción entre los filósofos presocráticos y los filósofos de la era clásica está demarcada no tanto por Sócrates, sino por la geografía y los textos que sobrevivieron.
El erudito André Laks distingue dos tradiciones de separar a presocráticos de socráticos, remontándose a la era clásica y corriendo a través de tiempos corrientes.
Muchos de los trabajos se titulan Peri Physeos, o En la Naturaleza, un título probablemente atribuido más tarde por otros autores.
Añadiendo más dificultad a su interpretación es el lenguaje oscuro que usaron.
Theophrastus, el sucesor de Aristóteles, escribió un libro enciclopédico Opinión de los físicos que era el trabajo estándar sobre los presocráticos en tiempos antiguos.
Los estudiosos ahora usan este libro para hacer referencia a los fragmentos utilizando un esquema de codificación llamado numeración Diels-Kranz.
Después de eso hay un código con respecto a si el fragmento es un testimonio, codificado como "A" o "B" si es una cita directa del filósofo.
La era presocrática duró aproximadamente dos siglos, durante los cuales el Imperio Aqueménida persa en expansión se extendía hacia el oeste, mientras que los griegos avanzaban en el comercio y las rutas marítimas, llegando a Chipre y Siria.
Los griegos se rebelaron en 499 aC, pero finalmente fueron derrotados en 494 aC.
Varios factores contribuyeron al nacimiento de la filosofía presocrática en la antigua Grecia.
Otro factor fue la facilidad y la frecuencia de los viajes intragriegos, lo que llevó a la combinación y comparación de ideas.
El sistema político democrático de poleis independiente también contribuyó al surgimiento de la filosofía.
Las ideas de los filósofos eran, hasta cierto punto, respuestas a preguntas que estaban sutilmente presentes en la obra de Homero y Hesíodo.
Son considerados predecesores de los presocráticos ya que buscan abordar el origen del mundo y organizar sistemáticamente el folclore y las leyendas tradicionales.
Los primeros filósofos presocráticos también viajaron extensamente a otras tierras, lo que significa que el pensamiento presocrático tenía raíces en el extranjero, así como en el país.
Los filósofos presocráticos compartían la intuición de que había una sola explicación que podía explicar tanto la pluralidad como la singularidad del todo, y que la explicación no sería acciones directas de los dioses.
Muchos buscaron el principio material (arque) de las cosas, y el método de su origen y desaparición.
En su esfuerzo por dar sentido al cosmos, acuñaron nuevos términos y conceptos como ritmo, simetría, analogía, deduccionismo, reduccionismo, matemática de la naturaleza y otros.
Podría significar el comienzo u origen con el trasfondo de que hay un efecto en las cosas a seguir.
Esto puede deberse a la falta de instrumentos, o a una tendencia a ver el mundo como una unidad, irreconstruible, por lo que sería imposible para un ojo externo observar pequeñas fracciones de la naturaleza bajo control experimental.
Sistemática porque intentaron universalizar sus hallazgos.
Los presocráticos no eran ateos; sin embargo, minimizaron el alcance de la participación de los dioses en fenómenos naturales como el trueno o eliminaron totalmente a los dioses del mundo natural.
La primera fase de la filosofía presocrática, principalmente los milesios, xenófanes y Heráclito, consistió en rechazar la cosmogonía tradicional e intentar explicar la naturaleza basada en observaciones e interpretaciones empíricas.
Los Eleatics también eran monistas (creyendo que solo existe una cosa y que todo lo demás es solo una transformación de ella).
Es considerado el primer filósofo occidental desde que fue el primero en usar la razón, la prueba y la generalización.
Tales puede haber sido de ascendencia fenicia.
Tales, sin embargo, avanzó la geometría con su razonamiento deductivo abstracto alcanzando generalizaciones universales.
Tales visitó Sardis, como muchos griegos entonces, donde los registros astronómicos se guardaron y usaron observaciones astronómicas para asuntos prácticos (recolección de petróleo).
Él atribuyó el origen del mundo a un elemento en lugar de un ser divino.
Era miembro de la élite de Mileto, rico y estadista.
En respuesta a Thales, postuló como primer principio una sustancia indefinida e ilimitada sin cualidades (apeiron), de la cual se diferenciaron los opuestos primarios, caliente y frío, húmedo y seco.
También es conocido por especular sobre el origen de la humanidad.
También escribió un libro sobre la naturaleza en prosa.
Era un poeta muy viajado cuyos principales intereses eran la teología y la epistemología.
Dijo que si los bueyes, caballos o leones podían dibujar, dibujarían a sus dioses como bueyes, caballos o leones.
Jenófanes también ofreció explicaciones naturalistas para fenómenos como el sol, el arco iris y el fuego de San Elmo.
Mientras Jenófanes era un pesimista sobre la capacidad de los humanos para alcanzar el conocimiento, también creía en el progreso gradual a través del pensamiento crítico.
Heráclito postula que todas las cosas en la naturaleza están en un estado de flujo perpetuo.
El fuego se convierte en agua y tierra y viceversa.
Allí, Heráclito afirma que no podemos entrar en el mismo río dos veces, una posición resumida con el lema ta panta rhei (todo fluye).
Otro concepto clave de Heráclito es que los opuestos de alguna manera se reflejan entre sí, una doctrina llamada unidad de opuestos.
La doctrina de Heráclito sobre la unidad de los opuestos sugiere que la unidad del mundo y sus diversas partes se mantiene a través de la tensión producida por los opuestos.
Una idea fundamental en Heráclito es logos, una palabra griega antigua con una variedad de significados; Heráclito podría haber usado un significado diferente de la palabra con cada uso en su libro.
Algunas décadas más tarde tuvo que huir de Croton y trasladarse a Metapontum.
Promovieron sus ideas, llegando a la afirmación de que todo consiste en números, el universo está hecho por números y todo es un reflejo de analogías y relaciones geométricas.
Su forma de vida era ascética, restringiéndose de diversos placeres y alimentos.
Otros filósofos presocráticos se burlaron de Pitágoras por su creencia en la reencarnación.
El pitagórico influyó en las corrientes cristianas posteriores como el neoplatonismo, y sus métodos pedagógicos fueron adaptados por Platón.
Según Aristóteles y Diógenes Laercio, Jenófanes fue el maestro de Parménides, y se debate si Jenófanes también debe ser considerado un eleático.
Él fue el primero en deducir que la tierra es esférica.
Parménides escribió un poema difícil de interpretar, llamado On Nature o On What-is, que influyó sustancialmente en la filosofía griega posterior.
El poema consta de tres partes, el proema (es decir, el prefacio), el Camino de la Verdad y el Camino de la Opinión.
El Camino de la Verdad era entonces, y sigue siendo hoy en día, considerado de mucha más importancia.
Por lo tanto, todas las cosas que pensamos que son verdad, incluso nosotros mismos, son falsas representaciones.
La diosa enseña a Kouros a usar su razonamiento para entender si varias afirmaciones son verdaderas o falsas, descartando los sentidos como falaces.
Zenón y Melissus continuaron el pensamiento de Parménides sobre la cosmología.
Intentó explicar por qué pensamos que existen varios objetos inexistentes.
Anaxágoras nació en Jonia, pero fue el primer filósofo importante en emigrar a Atenas.
Anaxágoras fue también una gran influencia en Sócrates.
Las interpretaciones difieren en cuanto a lo que quiso decir.
Todos los objetos eran mezclas de varios elementos, como aire, agua y otros.
Nous también se consideraba un bloque de construcción del cosmos, pero existe solo en objetos vivos.
Anaxágoras avanzó el pensamiento mileciano sobre la epistemología, esforzándose por establecer una explicación que pudiera ser válida para todos los fenómenos naturales.
Según Diógenes Laercio, Empédocles escribió dos libros en forma de poemas: Peri Physeos (Sobre la naturaleza) y el Katharmoi (Purificaciones).
También continúa el pensamiento de Anaxágoras sobre las cuatro "raíces" (es decir, elementos clásicos), que al entremezclarse, crean todas las cosas a nuestro alrededor.
Estas dos fuerzas son opuestas y al actuar sobre el material de las cuatro raíces se unen en armonía o desgarran las cuatro raíces, siendo la mezcla resultante todas las cosas que existen.
Son más famosos por su cosmología atómica a pesar de que su pensamiento incluía muchos otros campos de la filosofía, como la ética, las matemáticas, la estética, la política e incluso la embriología.
Demócrito y Leucipo eran escépticos con respecto a la fiabilidad de nuestros sentidos, pero estaban seguros de que el movimiento existe.
Los átomos se mueven dentro del vacío, interactúan entre sí y forman la pluralidad del mundo en el que vivimos, de una manera puramente mecánica.
Demócrito llegó a la conclusión de que, dado que todo es átomos y vacío, varios de nuestros sentidos no son reales sino convencionales.
Atacaron el pensamiento tradicional, desde los dioses hasta la moralidad, allanando el camino para nuevos avances de la filosofía y otras disciplinas como el drama, las ciencias sociales, las matemáticas y la historia.
Los sofistas enseñaron retórica y cómo abordar los problemas desde múltiples puntos de vista.
Gorgias escribió un libro llamado On Nature, en el que atacó los conceptos de los Eleatics de qué es y qué no es.
Antífona puso la ley natural contra la ley de la ciudad.
Intentó explicar tanto la variedad como la unidad del cosmos.
Diógenes de Apolonia volvió al monismo Milesiano, pero con un pensamiento bastante más elegante.
Mientras Pitágoras y Empédocles vinculaban su autoproclamada sabiduría con su estatus de inspiración divina, trataban de enseñar o instar a los mortales a buscar la verdad sobre el reino natural: Pitágoras por medio de las matemáticas y la geometría y Empédocles por exposición a las experiencias.
Atacaron las representaciones tradicionales de los dioses que Homero y Hesíodo habían establecido y pusieron la religión popular griega bajo escrutinio, iniciando el cisma entre la filosofía natural y la teología.
El pensamiento teológico comienza con los filósofos milesios.
Jenófanes estableció tres condiciones previas para Dios: tenía que ser todo bueno, inmortal y no parecerse a los humanos en apariencia, lo que tuvo un gran impacto en el pensamiento religioso occidental.
Anaxágoras afirmó que la inteligencia cósmica (nous) da vida a las cosas.
Fue Hipócrates (a menudo aclamado como el padre de la medicina) quien separó, pero no completamente, los dos dominios.
La naturaleza siempre transformadora se resume en el axioma de Heráclito panta rhei (todo está en un estado de flujo).
Los presocráticos trataron de comprender los diversos aspectos de la naturaleza por medio del racionalismo, las observaciones y las explicaciones que podrían considerarse científicas, dando a luz a lo que se convirtió en el racionalismo occidental.
Anaximandro ofreció el principio de la razón suficiente, un argumento revolucionario que también daría el principio de que nada sale de la nada.
Jenófanes también avanzó una crítica de la religión antropomórfica destacando de una manera racional la inconsistencia de las representaciones de los dioses en la religión popular griega.
Otros presocráticos también trataron de responder a la pregunta de Arche, ofreciendo varias respuestas, pero el primer paso hacia el pensamiento científico ya se había dado.
El pensamiento filosófico producido por los presocráticos influyó fuertemente en filósofos, historiadores y dramaturgos posteriores.
Los naturalistas impresionaron al joven Sócrates y estaba interesado en la búsqueda de la sustancia del cosmos, pero su interés disminuyó a medida que se centró cada vez más en la epistemología, la virtud y la ética en lugar del mundo natural.
Cicerón analizó sus puntos de vista sobre los presocráticos en sus Disputations Tusculanae, ya que distinguió la naturaleza teórica del pensamiento presocrático de los "sabios" anteriores que estaban interesados en cuestiones más prácticas.
Aristóteles discutió los presocráticos en el primer libro de Metafísica, como una introducción a su propia filosofía y la búsqueda de arque.
Francis Bacon, un filósofo del 16to siglo conocido por avanzar el método científico, era probablemente el primer filósofo de la era moderna para usar axiomas presocráticos extensivamente en sus textos.
Friedrich Nietzsche admiró profundamente a los presocráticos, llamándolos "tiranos del espíritu" para marcar su antítesis y su preferencia contra Sócrates y sus sucesores.
Según su narrativa, limitada en muchos de sus libros, la era presocrática fue la gloriosa era de Grecia, mientras que la llamada Edad de Oro que siguió fue una era de decadencia, según Nietzsche.
A pesar de que este período, conocido en su primera parte como el período de primavera y otoño y el período de los Estados Combatientes, en su última parte estuvo plagado de caos y batallas sangrientas, también se conoce como la Edad de Oro de la filosofía china porque una amplia gama de pensamientos e ideas se desarrollaron y discutieron libremente.
El taoísmo (también llamado taoísmo), una filosofía que enfatiza las Tres Joyas del Tao: compasión, moderación y humildad, mientras que el pensamiento taoísta generalmente se centra en la naturaleza, la relación entre la humanidad y el cosmos; salud y longevidad; y wu wei (acción a través de la inacción).
Agrarismo, o la Escuela de Agrarismo, que abogaba por el comunalismo utópico campesino y el igualitarismo.
Los eruditos de esta escuela eran buenos oradores, polemistas y tácticos.
La escuela de las "conversaciones menores", que no era una escuela de pensamiento única, sino una filosofía construida de todos los pensamientos que fueron discutidos por y se originaron de personas normales en la calle.
El confucianismo fue particularmente fuerte durante la dinastía Han, cuyo mayor pensador fue Dong Zhongshu, quien integró el confucianismo con los pensamientos de la Escuela Zhongshu y la teoría de los Cinco Elementos.
En particular, refutaron la asunción de Confucio como una figura divina y lo consideraron como el mayor sabio, pero simplemente un humano y mortal.
El budismo llegó a Chína alrededor del siglo I dC, pero no fue hasta las dinastías norte y sur, Sui y Tang que ganó considerable influencia y reconocimiento.
Esto conduce a la investigación del único ser que subyace a la diversidad de los fenómenos empíricos y al origen de todas las cosas.
Atri, Bharadwaja, Gautama, Jamadagni, Kasyapa, Vasishtha, Viswamitra.
La filosofía griega antigua se levantó en el 6to siglo A.C., marcando el final de las Edades Oscuras griegas.
Se ocupó de una amplia variedad de temas, incluyendo astronomía, epistemología, matemáticas, filosofía política, ética, metafísica, ontología, lógica, biología, retórica y estética.
Las líneas claras e ininterrumpidas de influencia conducen de los antiguos filósofos griegos y helenísticos a la filosofía romana, la filosofía islámica temprana, el escolasticismo medieval, el renacimiento europeo y la era de la Ilustración.
Se enseñaron a sí mismos a razonar.
Tales inspiró la escuela de Milesian de la filosofía y fue seguido de Anaximander, que sostuvo que el sustrato o arche no podrían ser el agua o ninguno de los elementos clásicos, pero era en cambio algo "ilimitado" o "indefinido" (en griego, el apeiron).
Contrariamente a la escuela Milesiana, que postula un elemento estable como el arco, Heráclito enseñó que panta rhei ("todo fluye"), el elemento más cercano a este flujo eterno es el fuego.
Ser, argumentó, por definición implica la eternidad, mientras que sólo lo que es puede ser pensado; una cosa que es, además, no puede ser más o menos, y por lo tanto la rarefacción y la condensación de los milesios es imposible con respecto al Ser; por último, como el movimiento requiere que algo exista aparte de la cosa en movimiento (a saber.
En apoyo de esto, el discípulo de Parménides, Zenón de Elea, intentó demostrar que el concepto de movimiento era absurdo y, como tal, el movimiento no existía.
Leucipo también propuso un pluralismo ontológico con una cosmogonía basada en dos elementos principales: el vacío y los átomos.
Mientras que la filosofía era una búsqueda establecida antes de Sócrates, Cicerón lo acredita como "el primero que hizo descender la filosofía de los cielos, la colocó en las ciudades, la introdujo en las familias y la obligó a examinar la vida y la moral, y el bien y el mal".
El hecho de que muchas conversaciones que involucran a Sócrates (como lo relataron Platón y Jenofonte) terminen sin haber llegado a una conclusión firme, o aporéticamente, ha estimulado el debate sobre el significado del método socrático.
Sócrates enseñó que nadie desea lo que es malo, y por lo tanto, si alguien hace algo que realmente es malo, debe ser de mala gana o por ignorancia; en consecuencia, toda virtud es conocimiento.
El gran estadista Pericles estaba estrechamente asociado con este nuevo aprendizaje y un amigo de Anaxágoras, sin embargo, y sus oponentes políticos lo golpearon aprovechando una reacción conservadora contra los filósofos; se convirtió en un crimen investigar las cosas sobre los cielos o debajo de la tierra, temas considerados impíos.
Sócrates, sin embargo, es el único sujeto registrado como acusado bajo esta ley, condenado y sentenciado a muerte en 399 aC (ver Juicio de Sócrates).
Platón lanza a Socrates como el interlocutor principal en sus diálogos, sacando de ellos la base de Platonism (y por la extensión, Neoplatonism).
Zenón de Citio a su vez adaptó la ética del cinismo para articular el estoicismo.
Junto con Jenofonte, Platón es la principal fuente de información sobre la vida y las creencias de Sócrates y no siempre es fácil distinguir entre los dos.
Aunque la regla de un hombre sabio sería preferible a la regla por la ley, el sabio no puede dejar de ser juzgado por el imprudente, y por lo tanto, en la práctica, la regla por la ley se considera necesaria.
Los diálogos de Platón también tienen temas metafísicos, el más famoso de los cuales es su teoría de las formas.
Compara a la mayoría de los humanos con personas atadas en una cueva, que solo miran las sombras en las paredes y no tienen otra concepción de la realidad.
Si estos viajeros volvieran a entrar en la cueva, las personas dentro (que todavía están familiarizadas con las sombras) no estarían equipadas para creer los informes de este "mundo exterior".
Bertrand Russell, Una Historia de Filosofía Occidental (Nueva York: Simon & Schuster, 1972).
Critica los regímenes descritos en la República y las Leyes de Platón, y se refiere a la teoría de las formas como "palabras vacías y metáforas poéticas".
Antístenes se inspiró en el ascetismo de Sócrates, y acusó a Platón de orgullo y vanidad.
Fue fundada por Euclides de Megara, uno de los alumnos de Sócrates.
El pirrhonismo coloca el logro de la ataraxia (un estado de ecuanimidad) como la forma de lograr la eudaimonia.
Su ética se basaba en "la búsqueda del placer y la evitación del dolor".
Sus contribuciones lógicas todavía figuran en el cálculo proposicional contemporáneo.
Este período escéptico del platonismo antiguo, de Arcesilaus a Philo de Larissa, se hizo conocido como la Nueva Academia, aunque algunos autores antiguos añadieran subdivisiones adicionales, como una Academia Media.
Mientras que el objetivo de los pirronistas era el logro de la ataraxia, después de Arcesilao los escépticos académicos no sostuvieron la ataraxia como el objetivo central.
En el Imperio Bizantino las ideas griegas se conservaron y se estudiaron, y no mucho después de la primera extensión principal del Islam, sin embargo, los califas Abbasid autorizaron la reunión de manuscritos griegos y contrataron a traductores para aumentar su prestigio.
La filosofía medieval es la filosofía que existió durante la Edad Media, el período que se extiende aproximadamente desde la caída del Imperio Romano de Occidente en el siglo V hasta el Renacimiento en el siglo XV.
Con las posibles excepciones de Avicena y Averroes, los pensadores medievales no se consideraban filósofos en absoluto: para ellos, los filósofos eran los antiguos escritores paganos como Platón y Aristóteles.
Una de las cosas más debatidas de la época fue la fe versus la razón.
Se acuerda generalmente que comienza con Agustín (354-430) que estrictamente pertenece al período clásico, y termina con el renacimiento durable del aprendizaje a finales del undécimo siglo, al principio del período medieval alto.
En períodos posteriores, los monjes fueron utilizados para la formación de administradores y eclesiásticos.
Gran parte de la obra de Aristóteles era desconocida en Occidente en este período.
Agustín es considerado como el más grande de los Padres de la Iglesia.
Durante más de mil años, apenas hubo una obra latina de teología o filosofía que no citara su escritura o invocara su autoridad.
Se convirtió en cónsul en 510 en el reino de los ostrogodos.
Escribió comentarios sobre estas obras, y sobre la Isagoge de Porfirio (un comentario sobre las categorías).
Alrededor de este período surgieron varias controversias doctrinales, como la cuestión de si Dios había predestinado a algunos para la salvación y a otros para la condenación.
¿Es la hostia lo mismo que el cuerpo histórico de Cristo?
Este período también fue testigo de un renacimiento de la erudición.
Más tarde, bajo San Abbo de Fleury (abad 988-1004), jefe de la escuela reformada de la abadía, Fleury disfrutó de una segunda edad de oro.
A principios del siglo XIII fue testigo de la culminación de la recuperación de la filosofía griega.
Poderosos reyes normandos reunieron a hombres de conocimiento de Italia y otras áreas en sus cortes como un signo de su prestigio.
Las universidades se desarrollaron en las grandes ciudades de Europa durante este período, y las órdenes clericales rivales dentro de la Iglesia comenzaron a luchar por el control político e intelectual sobre estos centros de la vida educativa.
Los grandes representantes del pensamiento dominicano en este período fueron Alberto Magno y (especialmente) Tomás de Aquino, cuya ingeniosa síntesis del racionalismo griego y la doctrina cristiana finalmente llegó a definir la filosofía católica.
Aquino mostró cómo era posible incorporar gran parte de la filosofía de Aristóteles sin caer en los "errores" del comentarista Averroes.
El problema del mal: Los filósofos clásicos habían especulado sobre la naturaleza del mal, pero el problema de cómo un Dios todopoderoso, omnisciente y amoroso podría crear un sistema de cosas en el que el mal existe surgió por primera vez en el período medieval.
Sin embargo, desde el siglo XIV en adelante, el creciente uso del razonamiento matemático en la filosofía natural preparó el camino para el surgimiento de la ciencia en el período moderno temprano.
En el período más temprano, los escritores como Peter Abelardo escribieron comentarios sobre los trabajos de la Vieja lógica (las Categorías de Aristóteles, Sobre la interpretación y el Isagoge de Porfirio).
(La palabra "intencionalidad" fue revivida por Franz Brentano, que tenía la intención de reflejar el uso medieval).
La denominación "filosofía renacentista" es usada por eruditos de la historia intelectual para referirse al pensamiento del período que corre en Europa aproximadamente entre 1355 y 1650 (las fechas cambian adelante para Europa central y del norte y para áreas como América española, India, Japón y Chína bajo la influencia europea).
La suposición de que las obras de Aristóteles eran fundamentales para la comprensión de la filosofía no disminuyó durante el Renacimiento, que vio un florecimiento de nuevas traducciones, comentarios y otras interpretaciones de sus obras, tanto en latín como en la lengua vernácula.
Este último, similar en algunos aspectos a los debates modernos, examinó los pros y los contras de posiciones o interpretaciones filosóficas particulares.
Platón, conocido directamente sólo a través de dos diálogos y medio en la Edad media, vino para conocerse a través de numerosas traducciones latinas en el 15to siglo Italia, que culmina en la traducción enormemente influyente de sus trabajos completos por Marsilio Ficino en Florencia en 1484.
No todos los humanistas del Renacimiento siguieron su ejemplo en todas las cosas, pero Petrarca contribuyó a una ampliación del "canon" de su tiempo (la poesía pagana había sido considerada previamente frívola y peligrosa), algo que también sucedió en la filosofía.
Otros movimientos de la filosofía antigua también volvieron a entrar en la corriente principal.
Esta posición vino bajo la tensión creciente en el Renacimiento, ya que varios pensadores afirmaron que las clasificaciones de Thomas eran inexactas, y que la ética era la parte más importante de la moralidad.
Como hemos visto, creían que la filosofía podía ser puesta bajo el ala de la retórica.
En 1416-1417, Leonardo Bruni, el humanista preeminente de su tiempo y canciller de Florencia, volvió a traducir la ética de Aristóteles en un latín más fluido, idiomático y clásico.
La convicción principal era que la filosofía debía liberarse de su jerga técnica para que más personas pudieran leerla.
Desiderio Erasmo, el gran humanista holandés, incluso preparó una edición griega de Aristóteles, y finalmente los que enseñaban filosofía en las universidades tenían que al menos fingir que sabían griego.
Sin embargo, una vez que se determinó que el italiano era una lengua con mérito literario y que podía llevar el peso de la discusión filosófica, comenzaron a aparecer numerosos esfuerzos en esta dirección, particularmente desde la década de 1540 en adelante.
Sabemos que los debates sobre la libertad de la voluntad continuaron estallando (por ejemplo, en los famosos intercambios entre Erasmo y Martín Lutero), que los pensadores españoles estaban cada vez más obsesionados con la noción de nobleza, que el duelo era una práctica que generó una gran literatura en el siglo XVI (¿era permisible o no?).
No debemos olvidar que la mayoría de los filósofos de la época eran al menos nominales, si no devotos, cristianos, que el siglo XVI vio tanto las reformas protestantes como las católicas, y que la filosofía renacentista culmina con el período de la Guerra de los Treinta Años (1618-1648).
En conclusión, como en cualquier otro momento de la historia del pensamiento, no se puede considerar que la filosofía renacentista haya proporcionado algo completamente nuevo ni que haya continuado durante siglos repitiendo las conclusiones de sus predecesores.
La filosofía moderna es una filosofía desarrollada en la era moderna y asociada con la modernidad.
En los siglos XVII y XVIII, las principales figuras de la filosofía de la mente, la epistemología y la metafísica se dividieron aproximadamente en dos grupos principales.
Los "empiristas", por el contrario, sostenían que el conocimiento debe comenzar con la experiencia sensorial.
Otras figuras importantes en la filosofía política incluyen a Thomas Hobbes y Jean-Jacques Rousseau.
Kant provocó una tormenta de trabajo filosófico en Alemania a principios del siglo XIX, comenzando con el idealismo alemán.
Karl Marx se apropió tanto de la filosofía de la historia de Hegel como de la ética empírica dominante en Gran Bretaña, transformando las ideas de Hegel en una forma estrictamente materialista, sentando las bases para el desarrollo de una ciencia de la sociedad.
Arthur Schopenhauer llevó el idealismo a la conclusión de que el mundo no era más que la inútil interacción interminable de imágenes y deseos, y abogó por el ateísmo y el pesimismo.
Descartes argumentó que muchas doctrinas metafísicas escolásticas predominantes no tenían sentido o eran falsas.
Trata de apartar todo lo que pueda de todas sus creencias, para determinar si sabe algo con certeza.
A partir de esta base construye su conocimiento de nuevo.
Si bien el historicismo también reconoce el papel de la experiencia, difiere del empirismo al asumir que los datos sensoriales no pueden entenderse sin considerar las circunstancias históricas y culturales en las que se realizan las observaciones.
Como tal, el empirismo se caracteriza ante todo por el ideal de dejar que los datos observacionales "hablen por sí mismos", mientras que los puntos de vista en competencia se oponen a este ideal.
En otras palabras: el empirismo como concepto tiene que ser construido junto con otros conceptos, que juntos hacen posible hacer discriminaciones importantes entre diferentes ideales subyacentes a la ciencia contemporánea.
Epistemológicamente, el idealismo se manifiesta como un escepticismo sobre la posibilidad de conocer cualquier cosa independiente de la mente.
Describe un proceso en el que la teoría se extrae de la práctica y se aplica de nuevo a la práctica para formar lo que se llama práctica inteligente.
Brian Leiter (2006) Página web "Analítica" y "Continental" Filosofía.
La filosofía contemporánea es el período actual en la historia de la filosofía occidental que comienza a principios del siglo XX con la creciente profesionalización de la disciplina y el surgimiento de la filosofía analítica y continental.
Alemania fue el primer país en profesionalizar la filosofía.
Además, a diferencia de muchas de las ciencias para las cuales ha llegado a haber una industria saludable de libros, revistas y programas de televisión destinados a popularizar la ciencia y comunicar los resultados técnicos de un campo científico a la población en general, las obras de filósofos profesionales dirigidas a un público fuera de la profesión siguen siendo raras.
Cada división organiza una gran conferencia anual.
Entre sus muchas otras tareas, la asociación es responsable de administrar muchos de los principales honores de la profesión.
Este desarrollo fue más o menos contemporáneo con el trabajo de Gottlob Frege y Bertrand Russell inaugurando un nuevo método filosófico basado en el análisis del lenguaje a través de la lógica moderna (de ahí el término "filosofía analítica").
Algunos filósofos, como Richard Rorty y Simon Glendinning, sostienen que esta división "analítica-continental" es hostil a la disciplina en su conjunto.
Después, los filósofos analíticos y continentales difieren en la importancia e influencia de filósofos subsecuentes en sus tradiciones respectivas.
Aunque, ya que la filosofía analítica y continental tengan visiones tan completamente diferentes de la filosofía después de Kant, la filosofía continental a menudo también se entiende en un sentido ampliado para incluir a cualquier filósofo post-Kant o movimientos importantes para la filosofía continental, pero no filosofía analítica.
Así, la filosofía continental tiende hacia el historicismo, donde la filosofía analítica tiende a tratar la filosofía en términos de problemas discretos, capaces de ser analizados aparte de sus orígenes históricos.
Las principales escuelas ortodoxas surgieron en algún momento entre el comienzo de la Era Común y el Imperio Gupta.
Estas tradiciones religio-filosóficas se agruparon más tarde bajo la etiqueta Hinduismo.
Los estudiosos occidentales consideran el hinduismo como una fusión o síntesis de varias culturas y tradiciones indias, con diversas raíces y sin un solo fundador.
Los filósofos indios desarrollaron un sistema de razonamiento epistemológico (pramana) y lógica e investigaron temas como Ontología (metafísica, Brahman-Atman, Sunyata-Anatta), medios confiables de conocimiento (epistemología, Pramanas), sistema de valores (axiología) y otros temas.
Los desarrollos posteriores incluyen el desarrollo del Tantra y las influencias iraní-islámicas.
Nyaya tradicionalmente acepta cuatro Pramanas como medio confiable para obtener conocimiento: Pratyaksa (percepción), Anumsa (inferencia), Upamsa (comparación y analogía) y ‘abda (palabra, testimonio de expertos confiables del pasado o del presente).
Esta filosofía sostenía que el universo era reducible a param-u (átomos), que son indestructibles (anitya), indivisibles, y tienen un tipo especial de dimensión, llamada "pequeña" (a-u).
Más tarde, los vaiseiikas (rudhara, udayana y iveditya) añadieron una categoría más: abhava (inexistencia).
Debido a su enfoque en el estudio y la interpretación textuales, Mamés también desarrolló teorías de la filología y la filosofía del lenguaje que influyeron en otras escuelas indias.
Los rasgos distintivos de la filosofía jainista incluyen un dualismo mente-cuerpo, la negación de un Dios creativo y omnipotente, el karma, un universo eterno e increado, la no violencia, la teoría de las múltiples facetas de la verdad y la moralidad basada en la liberación del alma.
También ha sido llamado un modelo de liberalismo filosófico por su insistencia en que la verdad es relativa y multifacética y por su disposición a acomodar todos los puntos de vista posibles de las filosofías rivales.
Filósofos como Brihaspati fueron extremadamente críticos con otras escuelas de filosofía de la época.
Es la tradición filosófica dominante en el Tíbet y en países del sudeste asiático como Sri Lanka y Birmania.
Más tarde, las tradiciones filosóficas budistas desarrollaron complejas psicologías fenomenológicas llamadas 'Abhidharma'.
Esta tradición contribuyó a lo que se ha llamado un "giro epistemológico" en la filosofía india.
Los exponentes importantes del modernismo budista incluyen a Anagarika Dharmapala (1864-1933) y el converso estadounidense Henry Steel Olcott, los modernistas chinos Taixu (1890-1947) y Yin Shun (1906-2005), el erudito zen D.T. Suzuki y el tibetano Gend?n Ch?phel (1903-1951).
La antropología es el estudio científico de la humanidad, que se ocupa del comportamiento humano, la biología humana, las culturas y las sociedades, tanto en el presente como en el pasado, incluidas las especies humanas del pasado.
La antropología biológica o física estudia el desarrollo biológico de los seres humanos.
Varias organizaciones efímeras de antropólogos ya se habían formado.
Cuando la esclavitud fue abolida en Francia en 1848, la Société fue abandonada.
Para ellos, la publicación de El origen de las especies de Charles Darwin fue la epifanía de todo lo que habían comenzado a sospechar.
Hubo una prisa inmediata por llevarlo a las ciencias sociales.
Su definición ahora se hizo "el estudio del grupo humano, considerado en conjunto, en sus detalles, y en relación con el resto de la naturaleza".
Descubrió el centro del habla del cerebro humano, hoy llamado el área de Broca después de él.
Los dos últimos volúmenes fueron publicados póstumamente.
Él enfatiza que los datos de comparación deben ser empíricos, recopilados por experimentación.
Waitz fue influyente entre los etnólogos británicos.
Representantes de la Société francesa estuvieron presentes, aunque no Broca.
Anteriormente, Edward se había referido a sí mismo como un etnólogo; posteriormente, un antropólogo.
Una excepción notable fue la Sociedad de Berlín para la Antropología, Etnología y Prehistoria (1869) fundada por Rudolph Virchow, conocido por sus ataques vituperativos contra los evolucionistas.
Los principales teóricos pertenecían a estas organizaciones.
La antropología práctica, el uso del conocimiento antropológico y la técnica para resolver problemas específicos, ha llegado; por ejemplo, la presencia de víctimas enterradas podría estimular el uso de un arqueólogo forense para recrear la escena final.
Esto ha sido particularmente prominente en los Estados Unidos, desde los argumentos de Boas contra la ideología racial del siglo XIX, a través de la defensa de Margaret Mead por la igualdad de género y la liberación sexual, hasta las críticas actuales a la opresión poscolonial y la promoción del multiculturalismo.
En Gran Bretaña y los países de la Commonwealth, la tradición británica de antropología social tiende a dominar.
La antropología cultural es el estudio comparativo de las múltiples formas en que las personas dan sentido al mundo que las rodea, mientras que la antropología social es el estudio de las relaciones entre individuos y grupos.
No hay una distinción dura y rápida entre ellos, y estas categorías se superponen en un grado considerable.
Este proyecto a menudo se acomoda en el campo de la etnografía.
La observación participante es uno de los métodos fundamentales de la antropología social y cultural.
El estudio del parentesco y la organización social es un foco central de la antropología sociocultural, ya que el parentesco es un universal humano.
La etnografía considera importante la experiencia de primera mano y el contexto social.
La etnomusicología se puede utilizar en una amplia variedad de campos, como la enseñanza, la política, la antropología cultural, etc.
La antropología económica permanece, en su mayor parte, centrada en el intercambio.
La primera de estas áreas se refería a las sociedades "precapitalistas" que estaban sujetas a estereotipos "tribales" evolutivos.
¿Por qué los que trabajan en el desarrollo están tan dispuestos a ignorar la historia y las lecciones que podría ofrecer?
Dentro del parentesco tienes dos familias diferentes.
La antropología se relaciona a menudo con feministas de tradiciones no occidentales, cuyas perspectivas y experiencias pueden diferir de las de las feministas blancas de Europa, América y otros lugares.
La antropología política se desarrolló como una disciplina preocupada principalmente por la política en las sociedades sin estado, un nuevo desarrollo comenzó a partir de la década de 1960 y aún se está desarrollando: los antropólogos comenzaron cada vez más a estudiar entornos sociales más "complejos" en los que la presencia de estados, burocracias y mercados entró tanto en cuentas etnográficas como en análisis de fenómenos locales.
En segundo lugar, los antropólogos comenzaron lentamente a desarrollar una preocupación disciplinaria con los estados y sus instituciones (y sobre la relación entre las instituciones políticas formales e informales).
A veces se agrupa con la antropología sociocultural, y a veces se considera parte de la cultura material.
También es el estudio de la historia de varios grupos étnicos que pueden o no existir hoy en día.
Varios procesos sociales en el mundo occidental, así como en el "Tercer Mundo" (este último es el foco habitual de atención de los antropólogos) atrajo la atención de los "especialistas en 'otras culturas'" más cerca de sus hogares.
Es un campo interdisciplinario que se superpone con una serie de otras disciplinas, incluyendo la antropología, la etología, la medicina, la psicología, la medicina veterinaria y la zoología.
Es el estudio de los humanos antiguos, como se encuentra en la evidencia fósil de homínidos, como huesos y huellas petrifactados.
En 1989, un grupo de académicos europeos y estadounidenses en el campo de la antropología establecieron la Asociación Europea de Antropólogos Sociales (EASA) que sirve como una importante organización profesional para los antropólogos que trabajan en Europa.
Esta es la noción de que las culturas no deben ser juzgadas por los valores o puntos de vista de otros, sino que deben ser examinadas desapasionadamente en sus propios términos.
Franz Boas se opuso públicamente a la participación de Estados Unidos en la Primera Guerra Mundial, y después de la guerra publicó una breve exposición y condena de la participación de varios arqueólogos estadounidenses en el espionaje en México bajo su cobertura como científicos.
Al mismo tiempo, el trabajo de David H. Price sobre la antropología estadounidense durante la Guerra Fría proporciona relatos detallados de la búsqueda y el despido de varios antropólogos de sus trabajos por simpatías comunistas.
Numerosas resoluciones condenando la guerra en todos sus aspectos fueron aprobadas abrumadoramente en las reuniones anuales de la Asociación Americana de Antropología (AAA).
La Asociación de Antropólogos Sociales del Reino Unido y la Commonwealth (ASA) ha calificado a ciertas becas como éticamente peligrosas.
Una de las características centrales es que la antropología tiende a proporcionar una explicación comparativamente más holística de los fenómenos y tiende a ser altamente empírica.
Estas relaciones dinámicas, entre lo que se puede observar en el terreno, a diferencia de lo que se puede observar mediante la compilación de muchas observaciones locales siguen siendo fundamentales en cualquier tipo de antropología, ya sea cultural, biológica, linguística o arqueológica.
En el aspecto biológico o físico, las mediciones humanas, las muestras genéticas y los datos nutricionales pueden recopilarse y publicarse como artículos o monografías.
Otras subdivisiones culturales según los tipos de herramientas, como Olduwan o Mousterian o Levalloisian ayudan a los arqueólogos y otros antropólogos a comprender las principales tendencias en el pasado humano.
Una norma cultural codifica una conducta aceptable en la sociedad; sirve como una guía para el comportamiento, la vestimenta, el idioma y el comportamiento en una situación, que sirve como plantilla para las expectativas en un grupo social.
Estos incluyen formas expresivas como el arte, la música, la danza, el ritual, la religión y tecnologías como el uso de herramientas, la cocina, el refugio y la ropa.
El nivel de sofisticación cultural también se ha utilizado a veces para distinguir las civilizaciones de las sociedades menos complejas.
La cultura de masas se refiere a las formas de cultura de consumo producidas en masa y mediadas en masa que surgieron en el siglo XX.
En las ciencias sociales más amplias, la perspectiva teórica del materialismo cultural sostiene que la cultura simbólica humana surge de las condiciones materiales de la vida humana, ya que los humanos crean las condiciones para la supervivencia física, y que la base de la cultura se encuentra en disposiciones biológicas evolucionadas.
En este sentido, el multiculturalismo valora la convivencia pacífica y el respeto mutuo entre las diferentes culturas que habitan el mismo planeta.
En 1986, el filósofo Edward S. Casey escribió: "La misma palabra cultura significaba 'lugar cultivado' en inglés medio, y la misma palabra se remonta al colere latino, 'habitar, cuidar, cultivar' y culto, 'un culto, especialmente uno religioso'.
Por lo tanto, un contraste entre "cultura" y "civilización" suele estar implícito en estos autores, incluso cuando no se expresa como tal.
Esta capacidad surgió con la evolución de la modernidad conductual en los seres humanos hace unos 50.000 años y, a menudo, se cree que es única para los humanos.
Rein Raud, basándose en el trabajo de Umberto Eco, Pierre Bourdieu y Jeffrey C. Alexander, ha propuesto un modelo de cambio cultural basado en reclamaciones y ofertas, que se juzgan por su adecuación cognitiva y respaldadas o no por la autoridad simbólica de la comunidad cultural en cuestión.
El reposicionamiento de la cultura significa la reconstrucción del concepto cultural de una sociedad.
El conflicto social y el desarrollo de tecnologías pueden producir cambios dentro de una sociedad alterando las dinámicas sociales y promoviendo nuevos modelos culturales, y estimulando o permitiendo la acción generativa.
Las condiciones ambientales también pueden entrar como factores.
La guerra o la competencia por los recursos pueden afectar el desarrollo tecnológico o la dinámica social.
Por ejemplo, las cadenas de restaurantes occidentales y las marcas culinarias despertaron la curiosidad y la fascinación de los chinos cuando Chína abrió su economía al comercio internacional a fines del siglo XX.
Sostuvo que esta inmadurez no proviene de una falta de comprensión, sino de una falta de coraje para pensar de forma independiente.
Por otra parte, Herder propuso una forma colectiva de Bildung: "Para Herder, Bildung era la totalidad de las experiencias que proporcionan una identidad coherente, y el sentido de destino común, a un pueblo."
Según esta escuela de pensamiento, cada grupo étnico tiene una visión del mundo distinta que es inconmensurable con las visiones del mundo de otros grupos.
Propuso que una comparación científica de todas las sociedades humanas revelaría que las distintas visiones del mundo consistían en los mismos elementos básicos.
"una forma particular de vida, ya sea de un pueblo, un período o un grupo".
En otras palabras, la idea de "cultura" que se desarrolló en Europa durante los siglos XVIII y principios del XIX reflejaba las desigualdades dentro de las sociedades europeas.
De acuerdo con esta forma de pensar, uno podría clasificar algunos países y naciones como más civilizados que otros y algunas personas como más cultas que otras.
Otros críticos del siglo XIX, siguiendo a Rousseau, han aceptado esta diferenciación entre la cultura superior y la inferior, pero han visto el refinamiento y la sofisticación de la alta cultura como desarrollos corruptos y antinaturales que oscurecen y distorsionan la naturaleza esencial de las personas.
En 1870, el antropólogo Edward Tylor (1832-1917) aplicó estas ideas de cultura superior versus inferior para proponer una teoría de la evolución de la religión.
Para el sociólogo Georg Simmel (1858-1918), la cultura se refería al "cultivo de individuos a través de la agencia de formas externas que han sido objetivadas en el curso de la historia".
La cultura no material se refiere a las ideas no físicas que los individuos tienen sobre su cultura, incluidos los valores, los sistemas de creencias, las reglas, las normas, la moral, el lenguaje, las organizaciones y las instituciones, mientras que la cultura material es la evidencia física de una cultura en los objetos y la arquitectura que hacen o han hecho.
La sociología cultural se "reinventó" entonces en el mundo de habla inglesa como un producto del "giro cultural" de los años 1960, que marcó el comienzo de enfoques estructuralistas y postmodernos a la ciencia social.
Desde entonces, la cultura se ha convertido en un concepto importante en muchas ramas de la sociología, incluidos campos decididamente científicos como la estratificación social y el análisis de redes sociales.
Vieron patrones de consumo y ocio determinados por las relaciones de producción, lo que los llevó a centrarse en las relaciones de clase y la organización de la producción.
Desde entonces se ha asociado fuertemente con Stuart Hall, quien sucedió a Hoggart como director.
Estas prácticas comprenden las formas en que las personas hacen cosas particulares (como ver televisión o salir a comer) en una cultura determinada.
Ver la televisión para ver una perspectiva pública sobre un evento histórico no debe considerarse como cultura a menos que se refiera al medio de la televisión en sí, que puede haber sido seleccionado culturalmente; sin embargo, los escolares que ven la televisión después de la escuela con sus amigos para "encajar" ciertamente califica ya que no hay una razón fundamentada para la participación de uno en esta práctica.
Cultura" para un investigador de estudios culturales no solo incluye la alta cultura tradicional (la cultura de los grupos sociales gobernantes) y la cultura popular, sino también los significados y prácticas cotidianas.
Los eruditos en el Reino Unido y los Estados Unidos desarrollaron versiones algo diferentes de estudios culturales después de finales de los años 1970.
Sin embargo, la distinción entre los hilos estadounidenses y británicos se ha desvanecido.
El enfoque principal de un enfoque marxista ortodoxo se concentra en la producción de significado.
Otros enfoques de los estudios culturales, como los estudios culturales feministas y los desarrollos estadounidenses posteriores del campo, se distancian de esta visión.
Los psicólogos culturales comenzaron a tratar de explorar la relación entre las emociones y la cultura, y responder si la mente humana es independiente de la cultura.
Por otro lado, algunos investigadores tratan de buscar diferencias entre las personalidades de las personas a través de las culturas.
Por ejemplo, las personas que son criadas en una cultura con un ábaco son entrenadas con un estilo de razonamiento distintivo.
Básicamente, la Convención de La Haya para la Protección de los Bienes Culturales en caso de Conflicto Armado y la Convención de la UNESCO para la Protección de la Diversidad Cultural se ocupan de la protección de la cultura.
Bajo el derecho internacional, la ONU y la UNESCO tratan de establecer y hacer cumplir las reglas para esto.
El objetivo del ataque es la identidad del oponente, por lo que los activos culturales simbólicos se convierten en un objetivo principal.
Un festival es un evento normalmente celebrado por una comunidad y centrado en algún aspecto característico de esa comunidad y su religión o culturas.
Junto a la religión y el folclore, un origen significativo es la agricultura.
Los festivales a menudo sirven para cumplir propósitos comunitarios específicos, especialmente en lo que respecta a la conmemoración o agradecimiento a los dioses, diosas o santos: se llaman festivales patronales.
En la antigua Grecia y Roma, los festivales como la Saturnalia estaban estrechamente asociados con la organización social y los procesos políticos, así como con la religión.
En inglés medio, un "dai festivo" era una fiesta religiosa.
El término "fiesta" también se usa en el lenguaje secular común como sinónimo de cualquier comida grande o elaborada.
Las fiestas religiosas más importantes como la Navidad, Rosh Hashaná, Diwali, Eid al-Fitr y Eid al-Adha sirven para marcar el año.
Un ejemplo temprano es el festival establecido por el antiguo faraón egipcio Ramsés III celebrando su victoria sobre los libios.
Hay numerosos tipos de festivales en el mundo y la mayoría de los países celebran eventos o tradiciones importantes con eventos y actividades culturales tradicionales.
Los festivales del antiguo Egipto podían ser religiosos o políticos.
El festival de Sed, por ejemplo, celebró el trigésimo año del gobierno de un faraón egipcio y luego cada tres (o cuatro en un caso) años después de eso.
En el calendario litúrgico cristiano, hay dos fiestas principales, propiamente conocidas como la Fiesta de la Natividad de nuestro Señor (Navidad) y la Fiesta de la Resurrección (Pascua), pero las fiestas menores en honor de los santos patronos locales se celebran en casi todos los países influenciados por el cristianismo.
Los festivales religiosos budistas, como Esala Perahera se llevan a cabo en Sri Lanka y Tailandia.
Los festivales de cine implican la proyección de varias películas diferentes, y por lo general se llevan a cabo anualmente.
También hay festivales de bebidas específicos, como el famoso Oktoberfest en Alemania para la cerveza.
Los antiguos egipcios dependían de la inundación estacional causada por el río Nilo, una forma de riego, que proporcionaba tierras fértiles para los cultivos.
El Festival de Dree de los Apatanis que viven en el Distrito de Subansiri Inferior de Arunachal Pradesh se celebra cada año del 4 al 7 de julio orando por una cosecha abundante.
Un día festivo es un día reservado por la costumbre o por la ley en el que las actividades normales, especialmente los negocios o el trabajo, incluida la escuela, se suspenden o reducen.
El grado en que las actividades normales se ven reducidas por unas vacaciones puede depender de las leyes locales, las costumbres, el tipo de trabajo realizado o las opciones personales.
En la mayoría de las sociedades modernas, sin embargo, las vacaciones sirven tanto de una función recreativa como cualquier otro fin de semana o actividades.
En algunos casos, un día festivo solo se puede observar nominalmente.
El uso moderno varía geográficamente.
Por ejemplo, el Día del Mono se celebra el 14 de diciembre, el Día Internacional de Hablar como un Pirata se celebra el 19 de septiembre, y el Día de la Blasfemia se celebra el 30 de septiembre.
Los Testigos de Jehová anualmente conmemoran "El Monumento conmemorativo de la muerte de Jesucristo", pero no celebran otras vacaciones con cualquier significado religioso como Pascua, Navidad o Año Nuevo.
Los musulmanes áhmadis también celebran el Día del Mesías Prometido, el Día del Reformador Prometido y el Día del Jalifato, pero contrariamente a la creencia popular, ninguno de los dos se consideran festivos.
Las fiestas celtas, nórdicas y neopaganas siguen el orden de la Rueda del Año.
Los investigadores en bioarqueología combinan los conjuntos de habilidades de la osteología humana, la paleopatología y la arqueología, y a menudo consideran el contexto cultural y mortuorio de los restos.
La psicología evolutiva es el estudio de las estructuras psicológicas desde una perspectiva evolutiva moderna.
La ecología conductual humana es el estudio de las adaptaciones conductuales (foraging, reproducción, ontogenia) desde las perspectivas evolutivas y ecológicas (ver ecología conductual).
La paleoantropología es el estudio de la evidencia fósil para la evolución humana, principalmente utilizando restos de homínidos extintos y otras especies de primates para determinar los cambios morfológicos y de comportamiento en el linaje humano, así como el entorno en el que ocurrió la evolución humana.
El nombre es incluso relativamente nuevo, después de haber sido "antropología física" durante más de un siglo, y algunos practicantes todavía aplican ese término.
Algunos editores, ver más abajo, han arraigado el campo aún más profundo que la ciencia formal.
Esto se convirtió en el sistema principal a través del cual los estudiosos pensaron en la naturaleza durante los próximos aproximadamente 2.000 años.
También escribió sobre la fisonomía, una idea derivada de los escritos en el Corpus hipocrático.
En el siglo XIX, los antropólogos físicos franceses, dirigidos por Paul Broca (1824-1880), se centraron en la craneometría, mientras que la tradición alemana, dirigida por Rudolf Virchow (1821-1902), enfatizó la influencia del medio ambiente y la enfermedad en el cuerpo humano.
Cambió el enfoque de la tipología racial para concentrarse en el estudio de la evolución humana, alejándose de la clasificación hacia el proceso evolutivo.
Una raza es un grupo de seres humanos basados en cualidades físicas o sociales compartidas en categorías generalmente vistas como distintas por la sociedad.
La ciencia moderna considera la raza como una construcción social, una identidad que se asigna sobre la base de las reglas hechas por la sociedad.
Otros argumentan que, entre los humanos, la raza no tiene importancia taxonómica porque todos los humanos vivos pertenecen a la misma subespecie, Homo sapiens sapiens.
En Sudáfrica, la Ley de Registro de Población de 1950 reconoció solo a los blancos, negros y de color, y los indios se agregaron más tarde.
La Oficina del Censo de los Estados Unidos propuso, pero luego retiró, los planes para agregar una nueva categoría para clasificar a los pueblos de Medio Oriente y África del Norte en el Censo de los Estados Unidos 2020, por una disputa sobre si esta clasificación debería considerarse una etnia blanca o una raza separada.
El establecimiento de límites raciales a menudo implica la subyugación de grupos definidos como racialmente inferiores, como en la regla de una gota usada en los Estados Unidos del 19no siglo para excluir aquellos con cualquier cantidad de la ascendencia africana del grupo racial dominante, definido como "blanco".
Según el genetista David Reich, "si bien la raza puede ser una construcción social, las diferencias en la ascendencia genética que se correlacionan con muchas de las construcciones raciales de hoy en día son reales".
Otras dimensiones de los grupos raciales incluyen la historia compartida, las tradiciones y el lenguaje.
Los factores socioeconómicos, en combinación con puntos de vista tempranos pero duraderos de la raza, han llevado a un sufrimiento considerable dentro de los grupos raciales desfavorecidos.
El racismo ha dado lugar a muchos casos de tragedia, incluida la esclavitud y el genocidio.
Debido a que en algunas sociedades las agrupaciones raciales se corresponden estrechamente con los patrones de estratificación social, para los científicos sociales que estudian la desigualdad social, la raza puede ser una variable significativa.
Por ejemplo, en 2008, John Hartigan, Jr. abogó por una visión de la raza que se centró principalmente en la cultura, pero que no ignora la relevancia potencial de la biología o la genética.
De esta manera, la idea de raza como la entendemos hoy surgió durante el proceso histórico de exploración y conquista que puso a los europeos en contacto con grupos de diferentes continentes, y de la ideología de clasificación y tipología que se encuentra en las ciencias naturales.
Un conjunto de creencias populares se arraigó que vinculaba las diferencias físicas heredadas entre los grupos con las cualidades intelectuales, conductuales y morales heredadas.
La clasificación de 1735 de Carl Linnaeus, inventor de la taxonomía zoológica, dividió la especie humana Homo sapiens en variedades continentales de europaeus, asiaticus, americanus y afer, cada uno asociado con un humor diferente: sanguinario, melancólico, colérico y flemático, respectivamente.
Blumenbach también notó la transición gradual en apariciones de un grupo a grupos contiguos y sugirió que "una variedad de la humanidad tan sensiblemente pasa en el otro, que no se puede marcar los límites entre ellos".
Se argumentó además que algunos grupos pueden ser el resultado de la mezcla entre poblaciones anteriormente distintas, pero que un estudio cuidadoso podría distinguir las razas ancestrales que se habían combinado para producir grupos mixtos.
Nuevos estudios de la cultura y el incipiente campo de la genética de poblaciones socavaron la posición científica del esencialismo racial, lo que llevó a los antropólogos raciales a revisar sus conclusiones sobre las fuentes de la variación fenotípica.
Los estudios de variación genética humana muestran que las poblaciones humanas no están aisladas geográficamente, y sus diferencias genéticas son mucho más pequeñas que las de subespecies comparables.
Andreasen citó diagramas de árboles de distancias genéticas relativas entre poblaciones publicadas por Luigi Cavalli-Sforza como la base para un árbol filogenético de razas humanas (p. 661).
Marks, Templeton y Cavalli-Sforza concluyen que la genética no proporciona evidencia de razas humanas.
Por ejemplo, con respecto al color de la piel en Europa y África, Brace escribe: Hasta el día de hoy, el color de la piel se clasifica por medios imperceptibles desde Europa hacia el sur alrededor del extremo oriental del Mediterráneo y hasta el Nilo en África.
Argumentó además que se podría usar el término raza si se distinguiera entre "diferencias raciales" y "el concepto de raza".
En resumen, Livingstone y Dobzhansky están de acuerdo en que hay diferencias genéticas entre los seres humanos; también están de acuerdo en que el uso del concepto de raza para clasificar a las personas, y cómo se usa el concepto de raza, es una cuestión de convención social.
Como observaron los antropólogos Leonard Lieberman y Fatimah Linda Jackson, "los patrones discordantes de heterogeneidad falsifican cualquier descripción de una población como si fuera genotípica o incluso fenotípicamente homogénea".
El antropólogo de mediados del siglo XX William C. Boyd definió la raza como: "Una población que difiere significativamente de otras poblaciones en lo que respecta a la frecuencia de uno o más de los genes que posee.
Por otra parte, el antropólogo Stephen Molnar ha sugerido que la discordancia de clines inevitablemente resulta en una multiplicación de las razas que hace que el concepto en sí inútil.
Joanna Mountain y Neil Risch advirtieron que si bien algún día se puede demostrar que los grupos genéticos corresponden a variaciones fenotípicas entre grupos, tales suposiciones fueron prematuras ya que la relación entre los genes y los rasgos complejos sigue siendo poco conocida.
Cualquier categoría que se te ocurra será imperfecta, pero eso no te impide usarla o el hecho de que tenga utilidad.
Esto supuso tres grupos de población separados por grandes rangos geográficos (europeo, africano y asiático oriental).
Antropólogos como C. Loring Brace, los filósofos Jonathan Kaplan y Rasmus Winther, y el genetista Joseph Graves, han argumentado que si bien es ciertamente posible encontrar una variación biológica y genética que corresponde aproximadamente a los grupos normalmente definidos como "razas continentales", esto es cierto para casi todas las poblaciones geográficamente distintas.
Weiss y Fullerton han notado que si uno muestreara solo islandeses, mayas y maoríes, se formarían tres grupos distintos y todas las demás poblaciones podrían describirse como compuestas por mezclas de materiales genéticos maoríes, islandeses y mayas.
Además, los datos genómicos no determinan si se desean ver subdivisiones (es decir, divisores) o un continuo (es decir, lumpers).
Junto con los problemas empíricos y conceptuales con la "raza", después de la Segunda Guerra Mundial, los científicos evolutivos y sociales eran muy conscientes de cómo las creencias sobre la raza se habían utilizado para justificar la discriminación, el apartheid, la esclavitud y el genocidio.
Craig Venter y Francis Collins del Instituto Nacional de Salud hicieron conjuntamente el anuncio del mapeo del genoma humano en 2000.
No es científico.
El antropólogo Stephan Palmié ha sostenido que la raza "no es una cosa sino una relación social"; o, en las palabras de Katya Gibel Mevorach, "un metonym", "una invención humana cuyos criterios para la diferenciación no son universales ni fijos, pero siempre se han usado para manejar la diferencia".
Allí, la identidad racial no fue gobernada por la regla de la descendencia rígida, como era en los Estados Unidos.
Estos tipos se clasifican entre sí como los colores del espectro, y ninguna categoría está significativamente aislada del resto.
Nueva Jersey: Prentice Hall Inc, 1984.
En el contexto europeo, la resonancia histórica de la "raza" subraya su naturaleza problemática.
El concepto de origen racial se basa en la noción de que los seres humanos pueden ser separados en "razas" biológicamente distintas, una idea generalmente rechazada por la comunidad científica.
En los Estados Unidos, la mayoría de las personas que se identifican a sí mismas como afroamericanas tienen algunos antepasados europeos, mientras que muchas personas que se identifican como europeoamericanas tienen algunos antepasados africanos o amerindios.
Los criterios para la pertenencia a estas razas divergieron a finales del siglo XIX.
Los amerindios continúan siendo definidos por un cierto porcentaje de "sangre india" (llamado quantum de sangre).
Esta regla significaba que aquellos que eran de raza mixta, pero con alguna ascendencia africana discernible, se definían como negros.
El término "hispano" como un etnonym surgió en el 20mo siglo con la subida de la migración de trabajadores de los países de habla hispana de América Latina a los Estados Unidos.
Se encontró que tres factores, el país de la educación académica, la disciplina y la edad, eran significativos para diferenciar las respuestas.
En 2007, Ann Morning entrevistó a más de 40 biólogos y antropólogos estadounidenses y encontró desacuerdos significativos sobre la naturaleza de la raza, sin que ningún punto de vista tenga una mayoría entre ninguno de los grupos.
Si bien puede ver buenos argumentos para ambas partes, la negación completa de la evidencia opuesta "parece derivarse en gran medida de la motivación sociopolítica y no de la ciencia en absoluto".
En respuesta parcial a la declaración de Gill, el profesor de Antropología Biológica C. Loring Brace argumenta que la razón por la que los laicos y los antropólogos biológicos pueden determinar la ascendencia geográfica de un individuo puede explicarse por el hecho de que las características biológicas están distribuidas por todo el planeta, y eso no se traduce en el concepto de raza.
Los textos de la antropología física sostuvieron que las razas biológicas existen hasta los años 1970, cuando comenzaron a argumentar que las razas no existen.
En febrero de 2001, los editores de Archives of Pediatrics and Adolescent Medicine pidieron a los "autores que no usaran la raza ni el origen étnico cuando no hubiera una razón biológica, científica o sociológica para hacerlo".
Morning (2008) analizó los libros de texto de biología de la escuela secundaria durante el período 1952-2002 e inicialmente encontró un patrón similar con solo el 35% discutiendo directamente la raza en el período 1983-92 desde el 92% inicialmente haciéndolo.
En general, el material sobre la raza ha pasado de los rasgos superficiales a la genética y la historia evolutiva.
En el mejor de los casos, se puede concluir que los biólogos y antropólogos ahora parecen divididos por igual en sus creencias sobre la naturaleza de la raza.
33 investigadores de servicios de salud de diferentes regiones geográficas fueron entrevistados en un estudio de 2008.
Muchos sociólogos se centraron en los afroamericanos, llamados negros en ese momento, y afirmaron que eran inferiores a los blancos.
En 1910, el Journal publicó un artículo de Ulysses G. Weatherly (1865-1940) que pedía la supremacía blanca y la segregación de las razas para proteger la pureza racial.
En su trabajo, sostuvo que la clase social, el colonialismo y el capitalismo dieron forma a las ideas sobre la raza y las categorías raciales.
En 1978, William Julius Wilson (1935-) argumentó que la raza y los sistemas de clasificación racial estaban disminuyendo en importancia, y que en cambio, la clase social describía con mayor precisión lo que los sociólogos habían entendido anteriormente como raza.
Eduardo Bonilla-Silva, profesor de Sociología en la Universidad de Duke, comenta: "Sostengo que el racismo es, más que cualquier otra cosa, una cuestión de poder grupal; se trata de un grupo racial dominante (blancos) que se esfuerza por mantener sus ventajas sistémicas y las minorías que luchan por subvertir el status quo racial.
En entornos clínicos, la raza a veces se ha considerado en el diagnóstico y tratamiento de afecciones médicas.
Existe un debate activo entre los investigadores biomédicos sobre el significado y la importancia de la raza en su investigación.
Los miembros de este último campo a menudo basan sus argumentos en torno al potencial para crear una medicina personalizada basada en el genoma.
Argumentan que exagerar las contribuciones genéticas a las disparidades de salud conlleva varios riesgos, como reforzar los estereotipos, promover el racismo o ignorar la contribución de factores no genéticos a las disparidades de salud.
IC significa "Código de Identificación"; estos elementos también se conocen como clasificaciones de Phoenix.
En muchos países, como Francia, el estado tiene prohibido legalmente mantener datos basados en la raza, lo que a menudo hace que la policía publique avisos que incluyen etiquetas como "piel oscura", etc.
Muchos consideran que el perfil racial de facto es un ejemplo de racismo institucional en la aplicación de la ley.
La encarcelación en masa es también “la red más amplia de leyes, reglas, políticas y costumbres que controlan a los delincuentes etiquetados tanto dentro como fuera de la prisión”.
Muchos hallazgos de la investigación parecen estar de acuerdo en que el impacto de la raza de la víctima en la decisión de arresto de la IPV podría incluir un sesgo racial a favor de las víctimas blancas.
Algunos estudios han informado que las razas se pueden identificar con un alto grado de precisión utilizando ciertos métodos, como el desarrollado por Giles y Elliot.
El estudio concluyó que "el reparto de la diversidad genética en el color de la piel es atípico y no se puede usar para fines de clasificación".
La antropología cultural es una rama de la antropología centrada en el estudio de la variación cultural entre los seres humanos.
Al abordar esta cuestión, los etnólogos en el siglo XIX se dividieron en dos escuelas de pensamiento.
Algunos de los que abogaban por la "invención independiente", como Lewis Henry Morgan, supusieron además que las similitudes significaban que diferentes grupos habían pasado por las mismas etapas de la evolución cultural (véase también el evolucionismo social clásico).
Morgan, al igual que otros evolucionistas sociales del siglo XIX, creía que había una progresión más o menos ordenada de lo primitivo a lo civilizado.
Aunque los etnólogos del siglo XIX vieron la "difusión" y la "invención independiente" como teorías mutuamente excluyentes y competitivas, la mayoría de los etnógrafos rápidamente llegaron a un consenso de que ambos procesos ocurren, y que ambos pueden explicar plausiblemente las similitudes interculturales.
Boas articuló por primera vez la idea en 1887: "... la civilización no es algo absoluto, sino que ... es relativa, y ... nuestras ideas y concepciones son verdaderas solo en lo que respecta a nuestra civilización".
El relativismo cultural implica afirmaciones epistemológicas y metodológicas específicas.
El relativismo cultural fue en parte una respuesta al etnocentrismo occidental.
Esta comprensión de la cultura enfrenta a los antropólogos con dos problemas: primero, cómo escapar de los lazos inconscientes de la propia cultura, que inevitablemente sesgan nuestras percepciones y reacciones ante el mundo, y segundo, cómo dar sentido a una cultura desconocida.
Uno de esos métodos es el de la etnografía: básicamente, abogaban por vivir con personas de otra cultura durante un período prolongado de tiempo, para que pudieran aprender el idioma local y ser inculturados, al menos parcialmente, en esa cultura.
Su enfoque fue empírico, escéptico de las generalizaciones excesivas y evitó los intentos de establecer leyes universales.
Él creía que cada cultura tiene que ser estudiada en su particularidad, y argumentó que las generalizaciones transculturales, como las realizadas en las ciencias naturales, no eran posibles.
Su primera generación de estudiantes incluyó a Alfred Kroeber, Robert Lowie, Edward Sapir y Ruth Benedict, que cada uno produjo estudios ricamente detallados de culturas norteamericanas indígenas.
La publicación del libro de texto de Alfred Kroeber Antropología (1923) marcó un punto de inflexión en la antropología estadounidense.
Influenciados por psicólogos psicoanalíticos como Sigmund Freud y Carl Jung, estos autores trataron de entender la forma en que las personalidades individuales fueron moldeadas por las fuerzas culturales y sociales más amplias en las que crecieron.
La antropología económica, influenciada por Karl Polanyi y practicada por Marshall Sahlins y George Dalton, desafió la economía neoclásica estándar a tener en cuenta los factores culturales y sociales, y empleó el análisis marxista en el estudio antropológico.
De acuerdo con los tiempos, gran parte de la antropología se politizó a través de la Guerra de Independencia de Argelia y la oposición a la Guerra de Vietnam; El marxismo se convirtió en un enfoque teórico cada vez más popular en la disciplina.
En la década de 1980 libros como Antropología y el Encuentro Colonial reflexionaron sobre los vínculos de la antropología con la desigualdad colonial, mientras que la inmensa popularidad de teóricos como Antonio Gramsci y Michel Foucault puso temas de poder y hegemonía en el centro de atención.
Estas interpretaciones deben entonces reflejarse de nuevo a sus creadores, y su adecuación como una traducción afinada de una manera repetida, un proceso llamado el círculo hermenéutico.
El análisis cultural de David Schnieder sobre el parentesco estadounidense ha demostrado ser igualmente influyente.
El método se originó en la investigación de campo de antropólogos sociales, especialmente Bronislaw Malinowski en Gran Bretaña, los estudiantes de Franz Boas en los Estados Unidos, y en la posterior investigación urbana de la Escuela de Sociología de Chicago.
Walnut Creek, CA: Prensa AltaMira.
Para establecer conexiones que eventualmente conduzcan a una mejor comprensión del contexto cultural de una situación, un antropólogo debe estar abierto a formar parte del grupo y dispuesto a desarrollar relaciones significativas con sus miembros.
Antes de que la observación participante pueda comenzar, un antropólogo debe elegir tanto una ubicación como un enfoque de estudio.
Esto permite que el antropólogo se establezca mejor en la comunidad.
La mayor parte de la observación de los participantes se basa en la conversación.
En algunos casos, los etnógrafos también recurren a la observación estructurada, en la que las observaciones de un antropólogo están dirigidas por un conjunto específico de preguntas que él o ella está tratando de responder.
Esto ayuda a estandarizar el método de estudio cuando los datos etnográficos se comparan entre varios grupos o se necesitan para cumplir un propósito específico, como la investigación para una decisión de política gubernamental.
Quién es el etnógrafo tiene mucho que ver con lo que eventualmente escribirá sobre una cultura, porque cada investigador está influenciado por su propia perspectiva.
Sin embargo, estos enfoques generalmente no han tenido éxito, y los etnógrafos modernos a menudo eligen incluir sus experiencias personales y posibles sesgos en su escritura.
Una etnografía es una pieza de escritura sobre un pueblo, en un lugar y momento en particular.
Una etnografía típica también incluirá información sobre geografía física, clima y hábitat.
Los estudiantes de Boas como Alfred L. Kroeber, Ruth Benedict y Margaret Mead dibujaron en su concepción de cultura y relativismo cultural para desarrollar la antropología cultural en los Estados Unidos.
Hoy en día los antropólogos socioculturales atienden a todos estos elementos.
Los "antropólogos culturales" estadounidenses se centraron en las formas en que las personas expresaban su visión de sí mismas y de su mundo, especialmente en formas simbólicas, como el arte y los mitos.
La monogamia, por ejemplo, se promociona con frecuencia como un rasgo humano universal, sin embargo, un estudio comparativo muestra que no lo es.
A través de esta metodología, se puede obtener una mayor comprensión al examinar el impacto de los sistemas mundiales en las comunidades locales y globales.
Por ejemplo, una etnografía multisituada puede seguir a una "cosa", tal como una mercancía particular, a medida que se transporta a través de las redes del capitalismo global.
Un ejemplo de etnografía multisitio es el trabajo de Nancy Scheper-Hughes sobre el mercado negro internacional para el comercio de órganos humanos.
La investigación en estudios de parentesco a menudo se cruza en diferentes subcampos antropológicos, incluyendo la antropología médica, feminista y pública.
Esa es la matriz en la que nacen los niños humanos en la gran mayoría de los casos, y sus primeras palabras son a menudo términos de parentesco.
Hay marcadas diferencias entre las comunidades en términos de práctica matrimonial y valor, dejando mucho espacio para el trabajo de campo antropológico.
La práctica matrimonial que se encuentra en la mayoría de las culturas, sin embargo, es la monogamia, donde una mujer está casada con un hombre.
Hay diferencias fundamentales similares en lo que respecta al acto de procreación.
El cambio se remonta a la década de 1960, con la reevaluación de los principios básicos del parentesco ofrecidos por Edmund Leach, Rodney Neeham, David Schneider y otros.
Este cambio fue avanzado adelante por la aparición del feminismo de la segunda onda a principios de los años 1970, que introdujeron ideas de opresión matrimonial, autonomía sexual y subordinación doméstica.
En este momento, hubo la llegada del "feminismo del Tercer Mundo", un movimiento que argumentaba que los estudios de parentesco no podían examinar las relaciones de género de los países en desarrollo de forma aislada, y también debían respetar los matices raciales y económicos.
En Jamaica, el matrimonio como institución a menudo sustituye a una serie de parejas, ya que las mujeres pobres no pueden depender de las contribuciones financieras regulares en un clima de inestabilidad económica.
Con esta tecnología, han surgido cuestiones de parentesco sobre la diferencia entre la relación biológica y genética, ya que los sustitutos gestacionales pueden proporcionar un entorno biológico para el embrión mientras que los lazos genéticos permanecen con un tercero.
También ha habido problemas de turismo reproductivo y mercantilización corporal, ya que las personas buscan seguridad económica a través de la estimulación hormonal y la recolección de óvulos, que son procedimientos potencialmente dañinos.
Una crítica es que, en su inicio, el marco de los estudios de parentesco era demasiado estructurado y formulado, basándose en un lenguaje denso y reglas estrictas.
Gran parte de este desarrollo se puede atribuir al aumento de antropólogos que trabajan fuera de la academia y la creciente importancia de la globalización tanto en las instituciones como en el campo de la antropología.
Los dos tipos de instituciones definidas en el campo de la antropología son instituciones totales e instituciones sociales.
La antropología de las instituciones puede analizar sindicatos, empresas que van desde pequeñas empresas hasta corporaciones, gobiernos, organizaciones médicas, educación, prisiones e instituciones financieras.
Los antropólogos institucionales pueden estudiar la relación entre organizaciones o entre una organización y otras partes de la sociedad.
Más específicamente, los antropólogos pueden analizar eventos específicos dentro de una institución, realizar investigaciones semióticas o analizar los mecanismos por los cuales el conocimiento y la cultura se organizan y dispersan.
Esta nueva era implicaría muchos nuevos desarrollos tecnológicos, como la grabación mecánica.
Antropología Actual 43(Suplemento):S5-17.Schieffelin, Bambi B. 2006.
Woolard, en su visión general de "cambio de código", o la práctica sistemática de la alternancia de las variedades de lenguaje dentro de una conversación o incluso una sola expresión, se encuentra la pregunta subyacente que los antropólogos hacen de la práctica - ¿Por qué hacen eso? - refleja una ideología de lenguaje dominante.
Otros linguistas han llevado a cabo investigaciones en las áreas de contacto con el idioma, el peligro del idioma y el "inglés como idioma global".
El trabajo de Joel Kuipers desarrolla este tema vis-a-vis la isla de Sumba, Indonesia.
Él siente, de hecho, que la idea ejemplar del centro es uno de los tres hallazgos más importantes de la antropología.
Por lo tanto, después de un par de generaciones estos idiomas ya no se pueden hablar.
Para seguir las mejores prácticas de documentación, estos registros deben anotarse claramente y mantenerse seguros dentro de un archivo de algún tipo.
La revitalización del lenguaje es la práctica de devolver un lenguaje al uso común.
El curso tiene como objetivo educar a los estudiantes indígenas y no indígenas sobre la lengua y la cultura Lenape.
Alentar a aquellos que ya saben el idioma para usarlo, aumentar los dominios de uso y aumentar el prestigio general del idioma son todos componentes de la recuperación.
La antropología social es el estudio de los patrones de comportamiento en las sociedades y culturas humanas.
Antropólogos británicos y estadounidenses, entre ellos Gillian Tett y Karen Ho, que estudiaron Wall Street, proporcionaron una explicación alternativa para la crisis financiera de 2007-2010 a las explicaciones técnicas arraigadas en la teoría económica y política.
Este desarrollo fue reforzado por la introducción de Franz Boas del relativismo cultural argumentando que las culturas se basan en diferentes ideas sobre el mundo y, por lo tanto, solo pueden entenderse adecuadamente en términos de sus propios estándares y valores.
En 1906, el pigmeo congoleño Ota Benga fue puesto por el antropólogo estadounidense Madison Grant en una jaula en el zoológico del Bronx, etiquetado como "el eslabón perdido" entre un orangután y la "raza blanca": Grant, un reconocido eugenista, también fue el autor de The Passing of the Great Race (1916).
La antropología se hizo cada vez más distinta de la historia natural y hacia el final del 19no siglo la disciplina comenzó a cristalizar en su forma moderna - hacia 1935, por ejemplo, era posible para T.K. Penniman escribir una historia de la disciplina titulada Cien años de Antropología.
Por lo tanto, las sociedades no europeas fueron vistas como "fósiles vivientes" evolutivos que podrían estudiarse para comprender el pasado europeo.
Sin embargo, como señala Stocking, Tylor se ocupó principalmente de describir y mapear la distribución de elementos particulares de la cultura, en lugar de la función más grande, y generalmente parecía asumir una idea victoriana de progreso en lugar de la idea de cambio cultural multilineal no direccional propuesta por antropólogos posteriores.
Sus estudios comparativos, más influyentes en las numerosas ediciones de The Golden Bough, analizaron las similitudes en las creencias religiosas y el simbolismo a nivel mundial.
Los hallazgos de la expedición establecieron nuevos estándares para la descripción etnográfica.
Otros fundadores intelectuales incluyen a W. H. R. Rivers y A. C. Haddon, cuya orientación reflejó las parapsicologías contemporáneas de Wilhelm Wundt y Adolf Bastian, y Sir E. B. Tylor, quien definió la antropología como una ciencia positivista después de Auguste Comte.
A. R. Radcliffe-Brown también publicó un trabajo seminal en 1922.
Este fue particularmente el caso de Radcliffe-Brown, quien difundió su agenda para la "Antropología Social" enseñando en universidades de todo el Imperio Británico y la Commonwealth.
Creyó que los términos indígenas usados en datos etnográficos se deberían traducir a términos legales angloamericanos para el beneficio del lector.
Departamentos de Antropología Social en diferentes universidades han tendido a centrarse en aspectos dispares del campo.
Un pueblo es cualquier pluralidad de personas consideradas como un todo.
Cuatro estados, Massachusetts, Virginia, Pensilvania y Kentucky, se refieren a sí mismos como la Commonwealth en casos y procesos legales.
En algunas partes del mundo, la etnología se ha desarrollado a lo largo de caminos independientes de investigación y doctrina pedagógica, con la antropología cultural que se hace dominante sobre todo en los Estados Unidos y la antropología social en Gran Bretaña.
La exploración de América en el siglo XV por parte de los exploradores europeos tuvo un papel importante en la formulación de nuevas nociones de Occidente (el mundo occidental), como la noción del "Otro".
El progreso de la etnología, por ejemplo con la antropología estructural de Claude Lévi-Strauss, llevó a la crítica de concepciones de un progreso lineal o la pseudooposición entre "sociedades con historias" y "sociedades sin historias", juzgadas demasiado dependientes de una visión limitada de la historia como constituida por el crecimiento acumulativo.
Sin embargo, las afirmaciones de tal universalismo cultural han sido criticadas por varios pensadores sociales de los siglos XIX y XX, incluidos Marx, Nietzsche, Foucault, Derrida, Althusser y Deleuze.
Un grupo étnico es un grupo de personas que se identifican entre sí sobre la base de atributos compartidos que los distinguen de otros grupos, como un conjunto común de tradiciones, ascendencia, idioma, historia, sociedad, cultura, nación, religión o tratamiento social dentro de su área de residencia.
La pertenencia a un grupo étnico tiende a ser definida por una herencia cultural compartida, ascendencia, mito de origen, historia, patria, idioma o dialecto, sistemas simbólicos como religión, mitología y ritual, cocina, estilo de vestir, arte o apariencia física.
A través del cambio de idioma, la aculturación, la adopción y la conversión religiosa, los individuos o grupos pueden cambiar con el tiempo de un grupo étnico a otro.
Ya sea a través de la división o la amalgama, la formación de una identidad étnica separada se conoce como etnogénesis.
En inglés Moderno Temprano y hasta mediados del 19no siglo, étnico se usó para significar pagano o pagano (en el sentido de "naciones" dispares que todavía no participaron en oikumene cristiano), ya que la Versión de los sesenta usó ta ethne ("las naciones") para traducir goyim hebreo "las naciones, no hebreos, no judíos".
En el 19no siglo, el término vino para usarse en el sentido de "peculiar a una raza, pueblo o nación", en una vuelta al sentido griego original.
Dependiendo del contexto, el término nacionalidad puede usarse como sinónimo de etnicidad o como sinónimo de ciudadanía (en un estado soberano).
Si la etnicidad califica como universal cultural depende hasta cierto punto de la definición exacta utilizada.
Según Thomas Hylland Eriksen, el estudio de la etnicidad estuvo dominado por dos debates distintos hasta hace poco.
El enfoque instrumentalista, por otro lado, trata la etnicidad principalmente como un elemento ad hoc de una estrategia política, utilizada como un recurso para grupos de interés para lograr objetivos secundarios como, por ejemplo, un aumento en la riqueza, el poder o el estatus.
Los constructivistas ven las identidades nacionales y étnicas como el producto de fuerzas históricas, a menudo recientes, incluso cuando las identidades se presentan como antiguas.
Esto es en el contexto de los debates sobre el multiculturalismo en países, como Estados Unidos y Canadá, que tienen grandes poblaciones de inmigrantes de muchas culturas diferentes, y el poscolonialismo en el Caribe y el sur de Asia.
En tercer lugar, la formación de grupos fue el resultado del impulso de monopolizar el poder y el estatus.
Barth fue más allá que Weber al enfatizar la naturaleza construida de la etnicidad.
Quiso separarse de nociones antropológicas de culturas como entidades limitadas y pertenencia étnica como lazos primordialistas, sustituyéndolo por un foco en la interfaz entre grupos.
Está de acuerdo con la observación de Joan Vincent de que (en la paráfrasis de Cohen) "La etnicidad ... puede reducirse o ampliarse en términos de límites en relación con las necesidades específicas de la movilización política.
Los grupos étnicos llegaron a definirse como entidades sociales más que biológicas.
Ejemplos de varios enfoques son el primordialismo, el esencialismo, el perennialismo, el constructivismo, el modernismo y el instrumentalismo.
El "primordialismo esencialista" sostiene además que la etnicidad es un hecho a priori de la existencia humana, que la etnicidad precede a cualquier interacción social humana y que no cambia por ella.
"Kinship primordialism" sostiene que las comunidades étnicas son extensiones de unidades del parentesco, básicamente derivándose por el parentesco o lazos del clan donde las opciones de signos culturales (lengua, religión, tradiciones) se hacen exactamente para mostrar esta afinidad biológica.
"El primordialismo de Geertz", notablemente apoyado por el antropólogo Clifford Geertz, sostiene que los seres humanos en general atribuyen un poder abrumador a "dados" humanos primordiales como lazos de sangre, lengua, territorio y diferencias culturales.
Smith (1999) distingue dos variantes: "perennialism continuo", que afirma que las naciones particulares han existido durante períodos muy largos, y "perennialism recurrente", que se concentra en la aparición, disolución y reaparición de naciones como un aspecto que se repite de la historia humana.
Esta visión sostiene que el concepto de pertenencia étnica es una herramienta usada por grupos políticos para manipular recursos como riqueza, poder, territorio o estado en los intereses de sus grupos particulares.
El "perennialismo instrumentalista", aunque ve la etnicidad principalmente como una herramienta versátil que identifica diferentes grupos étnicos y límites a través del tiempo, explica la etnicidad como un mecanismo de estratificación social, lo que significa que la etnicidad es la base para un arreglo jerárquico de individuos.
Según Donald Noel, la estratificación étnica surgirá solo cuando grupos étnicos específicos se pongan en contacto entre sí, y solo cuando esos grupos se caractericen por un alto grado de etnocentrismo, competencia y poder diferencial.
Continuando con la teoría de Noel, algún grado de poder diferencial debe estar presente para el surgimiento de la estratificación étnica.
Los diferentes grupos étnicos deben estar compitiendo por algún objetivo común, como el poder o la influencia, o un interés material, como la riqueza o el territorio.
Sostiene que los grupos étnicos son sólo productos de la interacción social humana, mantenida sólo en la medida en que se mantienen como construcciones sociales válidas en las sociedades.
Sostienen que antes de esta homogeneidad étnica no se consideraba un factor ideal o necesario en la forja de sociedades a gran escala.
Los miembros de un grupo étnico, en general, reclaman continuidades culturales a lo largo del tiempo, aunque los historiadores y antropólogos culturales han documentado que muchos de los valores, prácticas y normas que implican continuidad con el pasado son de invención relativamente reciente.
Se basa en la noción de "cultura".
Este punto de vista surgió como una forma de justificar la esclavitud de los afroamericanos y el genocidio de los nativos americanos en una sociedad que se fundó oficialmente en la libertad para todos.
Muchos de los científicos más destacados de la época adoptaron la idea de la diferencia racial y descubrieron que los europeos blancos eran superiores.
En lugar de atribuir el estatus marginado de las personas de color en los Estados Unidos a su inferioridad biológica inherente, lo atribuyó a su incapacidad de asimilarse a la cultura estadounidense.
Argumentan en la Formación Racial en los Estados Unidos que la teoría de la etnicidad se basó exclusivamente en los patrones de inmigración de la población blanca y tuvo en cuenta las experiencias únicas de los no blancos en los Estados Unidos.
Asimilacióndestruir las cualidades particulares de una cultura nativa con el propósito de mezclarse con un huésped cultivado no funciona para algunos grupos como respuesta al racismo y la discriminación, aunque lo hizo para otros.
Culminaron en la subida de "estados-nación" en los cuales los límites presuntos de la nación coincidieron (o idealmente coincidieron) con límites estatales.
Los estados-nación, sin embargo, invariablemente incluyen poblaciones que han sido excluidas de la vida nacional por una razón u otra.
Los estados multiétnicos pueden ser el resultado de dos eventos opuestos, ya sea la reciente creación de fronteras estatales en desacuerdo con los territorios tribales tradicionales, o la reciente inmigración de minorías étnicas a un antiguo estado-nación.
Los estados como el Reino Unido, Francia y Suiza comprendieron grupos étnicos distintos de su formación y también han experimentado la inmigración sustancial, causando lo que se ha llamado sociedades "multiculturales", sobre todo en ciudades grandes.
Aunque estas categorías generalmente se discuten como pertenecientes a la esfera pública y política, se mantienen en gran medida dentro de la esfera privada y familiar.
Antes de Weber (1864-1920), la raza y el origen étnico eran vistos principalmente como dos aspectos de la misma cosa.
Según este punto de vista, el estado no debe reconocer la identidad étnica, nacional o racial, sino más bien hacer cumplir la igualdad política y legal de todos los individuos.
El 19no siglo vio el desarrollo de la ideología política del nacionalismo étnico, cuando el concepto de raza se ató al nacionalismo, primero por teóricos alemanes incluso Johann Gottfried von Herder.
Cada uno promovió la idea panétnica que estos gobiernos adquirían sólo tierras que siempre habían sido habitadas por alemanes étnicos.
La colonización de Asia terminó en gran parte en el siglo XX, con impulsos nacionales por la independencia y la autodeterminación en todo el continente.
Varios países europeos, incluidos Francia y Suiza, no recopilan información sobre la etnia de su población residente.
Durante la colonización europea, los europeos llegaron a América del Norte.
La etnografía digital permite muchas más oportunidades para observar diferentes culturas y sociedades.
La etnografía relacional articula el estudio de campos en lugar de lugares o procesos en lugar de personas procesadas.
El objetivo es recopilar datos de tal manera que el investigador imponga una cantidad mínima de sesgo personal en los datos.
Las entrevistas a menudo se graban y luego se transcriben, lo que permite que la entrevista continúe sin problemas de toma de notas, pero con toda la información disponible más adelante para un análisis completo.
A pesar de estos intentos de reflexividad, ningún investigador puede ser totalmente imparcial.
Por lo general, se les pide a estos informantes que identifiquen a otros informantes que representan a la comunidad, a menudo utilizando bolas de nieve o muestras en cadena.
2010) examinan las presuposiciones ontológicas y epistemológicas subyacentes a la etnografía.
Los investigadores de la teoría crítica abordan "temas de poder dentro de las relaciones investigadas por los investigadores y los vínculos entre el conocimiento y el poder".
Una imagen puede estar contenida dentro del mundo físico a través de la perspectiva de un individuo en particular, basada principalmente en las experiencias pasadas de ese individuo.
La idea de una imagen se basa en la imaginación y se ha visto que es utilizada por los niños de una manera muy espontánea y natural.
Los antropólogos culturales y sociales hoy dan un alto valor a la investigación etnográfica.
Las etnografías a veces también se llaman "estudios de casos".
El trabajo de campo generalmente implica pasar un año o más en otra sociedad, vivir con la gente local y aprender sobre sus formas de vida.
Las experiencias de Benedict con el pueblo Zuni Sudoeste se deben considerar la base de su trabajo de campo formativo.
Una etnografía típica intenta ser holística y típicamente sigue un esquema para incluir una breve historia de la cultura en cuestión, un análisis de la geografía física o el terreno habitado por las personas bajo estudio, incluido el clima, y a menudo incluyendo lo que los antropólogos biológicos llaman hábitat.
El parentesco y la estructura social (incluida la clasificación por edad, los grupos de pares, el género, las asociaciones voluntarias, los clanes, los restos, etc., si existen) se incluyen típicamente.
Los ritos, rituales y otras evidencias de religión han sido durante mucho tiempo un interés y a veces son centrales para las etnografías, especialmente cuando se llevan a cabo en público donde los antropólogos visitantes pueden verlos.
Por ejemplo, si dentro de un grupo de personas, el guiño era un gesto comunicativo, primero trató de determinar qué tipo de cosas podría significar un guiño (podría significar varias cosas).
Geertz, mientras seguía algo así como un esquema etnográfico tradicional, se movió fuera de ese esquema para hablar de "redes" en lugar de "esbozos" de cultura.
Escribir Cultura ayudó a traer cambios tanto a la antropología como a la etnografía a menudo descrita en términos de ser "postmoderna", "reflexiva", "literaria", "deconstructiva" o "postestructural", en el sentido de que el texto ayudó a resaltar los diversos predicamentos epistémicos y políticos que muchos practicantes vieron como plagando representaciones y prácticas etnográficas.
Con respecto a este último punto, Writing Culture se convirtió en un punto focal para observar cómo los etnógrafos podían describir diferentes culturas y sociedades sin negar la subjetividad de esos individuos y grupos que se estudiaban, al mismo tiempo que lo hacían sin reclamar un conocimiento absoluto y una autoridad objetiva.
Como el propósito de la etnografía es describir e interpretar los patrones compartidos y aprendidos de valores, comportamientos, creencias y lenguaje de un grupo de intercambio cultural, Harris, (1968), también Agar (1980) señala que la etnografía es tanto un proceso como un resultado de la investigación.
El sociólogo Sam Ladner argumenta en su libro, que la comprensión de los consumidores y sus deseos requiere un cambio en el "standpoint", uno que sólo la etnografía proporciona.
Al evaluar la experiencia del usuario en un entorno "natural", la etnología proporciona información sobre las aplicaciones prácticas de un producto o servicio.
La conferencia sobre la praxis etnográfica en la industria (EPIC) es una prueba de ello.
La monografía de Jaber F. Gubrium y James A. Holstein (1997), El nuevo lenguaje del método cualitativo, discute formas de etnografía en términos de sus "métodos de conversación".
Esencialmente, Fine sostiene que los investigadores típicamente no son tan éticos como afirman o asumen ser, y que "cada trabajo incluye formas de hacer cosas que serían inapropiadas para que otros sepan".
Sostiene que las "ilusiones" son esenciales para mantener una reputación ocupacional y evitar consecuencias potencialmente más cáusticas.
El código de ética señala que los antropólogos son parte de una red académica y política más amplia, así como del entorno humano y natural, que debe informarse respetuosamente.
Los investigadores toman ficciones cercanas y las convierten en afirmaciones de hechos.
En realidad, un etnógrafo siempre perderá algún aspecto debido a la falta de omnisciencia.
Los pueblos indígenas, también conocidos como primeros pueblos, pueblos aborígenes, pueblos nativos o pueblos autóctonos, son grupos étnicos culturalmente distintos que son nativos de un lugar que ha sido colonizado y colonizado por otro grupo étnico.
Los pueblos generalmente se describen como "indígenas" cuando mantienen tradiciones u otros aspectos de una cultura temprana que se asocia con una región dada.
Los pueblos indígenas continúan enfrentando amenazas a su soberanía, bienestar económico, idiomas, formas de conocimiento y acceso a los recursos de los que dependen sus culturas.
Las estimaciones de la población mundial total de los pueblos indígenas suelen oscilar entre 250 millones y 600 millones.
Como referencia a un grupo de personas, el término indígena entró en uso por primera vez por los europeos que lo utilizaron para diferenciar a los pueblos indígenas de las Américas de los africanos esclavizados.
En la década de 1970, el término se utilizó como una forma de vincular las experiencias, los problemas y las luchas de los grupos de personas colonizadas a través de las fronteras internacionales.
Esta situación puede persistir incluso en el caso de que la población indígena supere en número a la de los otros habitantes de la región o estado; la noción definitoria aquí es la de separación de los procesos de decisión y regulación que tienen cierta influencia, al menos titular, sobre aspectos de su comunidad y derechos a la tierra.
Un informe de las Naciones Unidas de 2009 publicado por la Secretaría del Foro Permanente para las Cuestiones Indígenas declaró: Durante siglos, desde el momento de su colonización, conquista u ocupación, los pueblos indígenas han documentado historias de resistencia, interfaz o cooperación con los estados, demostrando así su convicción y determinación de sobrevivir con sus identidades soberanas distintas.
Esta gente fue vista por escritores antiguos como los antepasados de los griegos, o como un grupo más temprano de la gente que habitó Grecia antes de los griegos.
Las Cruzadas (1096-1271) se basaron en esta ambición de una guerra santa contra quienes la iglesia veía como infieles.
Sin embargo, el consejo sostuvo que las conquistas podrían "legalmente" ocurrir si los no cristianos rechazaran cumplir con Christianization y ley natural europea.
En los 14tos y 15tos siglos, los pueblos Indígenas de lo que se conoce ahora como las Islas Canarias, conocidas como Guanches (quien había vivido en las islas desde la era BCE) se hicieron el sujeto de la atención de colonizadores.
En 1402, los españoles comenzaron los esfuerzos para invadir y colonizar las islas.
Los invasores trajeron destrucción y enfermedades al pueblo guanche, cuya identidad y cultura desaparecieron como resultado.
Como declarado por Robert J. Miller, Jacinta Ruru, Larissa Behrendt y Tracey Lindberg, la doctrina se desarrolló con el tiempo "para justificar la dominación de pueblos no cristianos, no europeos y las confiscaciones de sus tierras y derechos".
El rey español Ferdinand y la reina Isabella contrataron a Cristóbal Colón, que se envió en 1492, para colonizar y traer nuevas tierras bajo la corona española.
Alejandro concedió a España las tierras que descubrió, siempre y cuando no hubieran sido "poseídas previamente por ningún propietario cristiano".
Muchos conquistadores aparentemente temían que, si se les daba la opción, los pueblos indígenas realmente aceptarían el cristianismo, lo que legalmente no permitiría la invasión de sus tierras y el robo de sus pertenencias.
Siendo países católicos en 1493, Inglaterra y Francia trabajaron para "reinterpretar" la Doctrina del Descubrimiento para servir a sus propios intereses coloniales.
Las reclamaciones de la tierra se hicieron a través de "rituales simbólicos del descubrimiento" que se realizaron para ilustrar la reclamación legal de la nación colonizadora de la tierra.
En 1774, el capitán James Cook intentó invalidar reclamaciones de la tierra españolas a Tahití quitando sus marcas de la posesión y luego procediendo a establecer marcas inglesas de la posesión.
Este concepto formalizó la idea de que las tierras que no se estaban utilizando de una manera que los sistemas legales europeos aprobaron estaban abiertas para la colonización europea.
A medida que las "reglas" de colonización se establecieron en la doctrina legal acordada por las potencias coloniales europeas, los métodos para reclamar tierras indígenas continuaron expandiéndose rápidamente.
Las estimaciones precisas para la población total de los pueblos indígenas del mundo son muy difíciles de compilar, dadas las dificultades de identificación y las variaciones e insuficiencias de los datos disponibles del censo.
Esto incluye al menos 5.000 pueblos distintos en más de 72 países.
Algunos también han sido asimilados por otras poblaciones o han sufrido muchos otros cambios.
Los grupos étnicos muy diversos y numerosos que comprenden la mayor parte de estados africanos modernos e independientes contienen dentro de ellos varios pueblos cuya situación, culturas y pastoralist o estilos de vida del cazador-recolector generalmente se marginan y se apartan de las estructuras políticas y económicas dominantes de la nación.
Los impactos de la colonización europea histórica y en curso de las Américas en las comunidades indígenas han sido en general bastante severos, y muchas autoridades estiman que los rangos de disminución significativa de la población se deben principalmente a enfermedades, robo de tierras y violencia.
En los estados sureños de Oaxaca (65.73%) y Yucatán (65.40%), la mayoría de la población es indígena, como se informó en 2015.
Los descriptores "indio" y "esquimal" han caído en desuso en Canadá.
Lo más notable fue el cambio de Asuntos Aborígenes y Desarrollo del Norte de Canadá (AANDC) a Asuntos Indígenas y del Norte de Canadá (INAC) en 2015, que luego se dividió en Servicios Indígenas de Canadá y Relaciones Corona-Indígenas y Desarrollo del Norte de Canadá en 2017.
Los pueblos de las Primeras Naciones firmaron 11 tratados numerados en gran parte de lo que ahora se conoce como Canadá entre 1871 y 1921, excepto en partes de Columbia Británica.
El territorio autónomo de Groenlandia dentro del Reino de Dinamarca también es patria de una población indígena y de la mayoría reconocida de Inuit (aproximadamente el 85%) que colocó el área en el 13er siglo, desplazando a la gente de Dorset indígena y nórdico groenlandés.
En los países de habla hispana o portuguesa, se encuentra el uso de términos como índios, pueblos indígenas, amerindios, povos nativos, povos indígenas y, en Perú, Comunidades Nativas, particularmente entre las sociedades amazónicas como la Urarina y Matsés.
Los pueblos indígenas se encuentran en todo el territorio de Brasil, aunque la mayoría de ellos viven en reservas indias en la parte norte y centro-occidental del país.
Actualmente hay más armenios que viven fuera de su patria ancestral debido al genocidio armenio de 1915.
El argumento entró en el conflicto israelí-palestino en la década de 1990, con los palestinos reclamando el estatus indígena como una población preexistente desplazada por el asentamiento judío, y que actualmente constituye una minoría en el Estado de Israel.
En Rusia, la definición de "pueblos indígenas" se impugna en gran parte refiriéndose a un número de población (menos de 50 000 personas), y descuidando la autoidentificación, el origen de las poblaciones indígenas que habitaron el país o región en la invasión, colonización o establecimiento de fronteras estatales, instituciones sociales, económicas y culturales distintivas.
Los tibetanos son indígenas del Tíbet.
En Hong Kong, los habitantes indígenas de los Nuevos Territorios se definen en la Declaración conjunta sino-británica como la gente descendió a través de la línea masculina de una persona que estaba en 1898, antes de la Convención para la Extensión del Territorio de Hong Kong.
Los Cham son los pueblos indígenas del antiguo estado de Champa que fue conquistado por Vietnam en las guerras Cham-Vietnamese durante Nam tien.
Los Khmer Krom son los pueblos indígenas del Delta del Mekong y Saigón que fueron adquiridos por Vietnam al rey camboyano Chey Chettha II a cambio de una princesa vietnamita.
Este problema es compartido por muchos otros países de la región de la ASEAN.
Los pueblos indígenas de Mindanao son los pueblos Lumad y los Moro (Tausug, Maguindanao Maranao y otros) que también viven en el archipiélago de Sulu.
A menudo se habla de estos grupos como australianos indígenas.
Durante el siglo XX, varias de estas antiguas colonias obtuvieron la independencia y se formaron estados-nación bajo el control local.
Los restos de al menos 25 humanos en miniatura, que vivieron entre 1.000 y 3.000 años atrás, fueron encontrados recientemente en las islas de Palau en Micronesia.
Según el censo de 2013, los maoríes de Nueva Zelanda representan el 14,9% de la población de Nueva Zelanda, con menos de la mitad (46,5%) de todos los residentes maoríes que se identifican únicamente como maoríes.
Muchos líderes nacionales maoríes firmaron un tratado con los británicos, el Tratado de Waitangi (1840), visto en algunos círculos como la formación de la entidad geopolítica moderna que es Nueva Zelanda.
Estas cuestiones incluyen la preservación cultural y linguística, los derechos sobre la tierra, la propiedad y la explotación de los recursos naturales, la determinación política y la autonomía, la degradación ambiental y la incursión, la pobreza, la salud y la discriminación.
La situación puede confundirse aún más cuando hay una historia complicada o controvertida de migración y población de una región determinada, lo que puede dar lugar a disputas sobre la primacía y la propiedad de la tierra y los recursos.
A pesar de la diversidad de los pueblos indígenas, cabe señalar que comparten problemas y cuestiones comunes al tratar con la sociedad predominante o invasora.
Las excepciones notables son los pueblos de Komi y Sakha (dos pueblos indígenas del norte de Rusia), que ahora controlan sus propias repúblicas autónomas dentro del estado ruso, y los inuit canadienses, que forman una mayoría del territorio de Nunavut (creado en 1999).
Este rechazo terminó reconociendo que existía un sistema legal preexistente practicado por el pueblo Meriam.
Consultado el 11 de octubre de 2011.
Los hindúes y los chams han experimentado persecución religiosa y étnica y restricciones a su fe bajo el actual gobierno vietnamita, con el estado vietnamita confiscando la propiedad de los cham y prohibiendo a los cham observar sus creencias religiosas.
En 2012, la policía vietnamita en la aldea de Chau Giang irrumpió en una mezquita Cham, robó el generador eléctrico y también violó a niñas Cham.
En 2012, Indonesia declaró que "El Gobierno de Indonesia apoya la promoción y protección de los pueblos indígenas en todo el mundo ... Indonesia, sin embargo, no reconoce la aplicación del concepto de los pueblos indígenas ... en el país".
Los vietnamitas se centraron originalmente alrededor del Delta del Río Rojo, pero participaron en la conquista y se apoderaron de nuevas tierras como Champa, el Delta del Mekong (de Camboya) y las Tierras Altas Centrales durante Nam Tien.
La tremenda escala de colonos Kinh vietnamitas que inundan las Tierras Altas Centrales ha alterado significativamente la demografía de la región.
Y no hay eliminación de una cultura por otra”.
Los pueblos indígenas se han denotado primitivos, salvajes o incivilizados.
Algunos filósofos, como Thomas Hobbes (1588-1679), consideraban que los indígenas eran simplemente "salvajes".
Consultado el 13 de diciembre de 2013.
La Declaración de las Naciones Unidas sobre los Derechos de los Pueblos Indígenas, adoptada por la Asamblea General en 2007, estableció el derecho de los pueblos indígenas a la libre determinación, lo que implica varios derechos con respecto a la gestión de los recursos naturales.
La perforación petrolera podría destruir miles de años de cultura para los Gwich'in.
Los proyectos de desarrollo como la construcción de represas, los oleoductos y la extracción de recursos han desplazado a un gran número de pueblos indígenas, a menudo sin proporcionar una compensación.
Estas mujeres también se vuelven económicamente dependientes de los hombres cuando pierden sus medios de subsistencia.
Por ejemplo, el pueblo Munduruku en la selva amazónica se opone a la construcción de la presa Tapajós con la ayuda de Greenpeace.
Se proponen dos escenarios principales, una expansión temprana a África Central y un origen único de la dispersión que irradia desde allí, o una separación temprana en una ola de dispersión hacia el este y hacia el sur, con una ola moviéndose a través de la cuenca del Congo hacia África Oriental, y otra moviéndose hacia el sur a lo largo de la costa africana y el sistema del río Congo hacia Angola.
La terminología de ganado en uso entre los relativamente pocos grupos de pastores bantúes modernos sugiere que la adquisición de ganado puede haber sido de los vecinos de habla sudanesa central, Kuliak y Cushitic.
No muy lejos del río Mutirikiwi, los reyes de Monomatapa construyeron el complejo del Gran Zimbabwe, una civilización ancestral del pueblo Kalanga.
La cultura swahili que surgió de estos intercambios evidencia muchas influencias árabes e islámicas que no se ven en la cultura bantú tradicional, al igual que los muchos miembros afroárabes del pueblo bantú swahili.
Después de la Segunda Guerra Mundial, los gobiernos del Partido Nacional adoptaron ese uso oficialmente, mientras que el creciente movimiento nacionalista africano y sus aliados liberales recurrieron al término "africano", por lo que "Bantu" se identificó con las políticas del apartheid.
Una vez más, la asociación con el apartheid desacreditó el término, y el gobierno sudafricano cambió al término políticamente atractivo pero históricamente engañoso "patria étnica".
En Swati el tallo es -ntfu y el sustantivo es buntfu.
No todos los vascos son vascos.
moderno esan vasco) y el sufijo -(k)ara ("manera (de hacer algo)").
Registra el nombre de la lengua vasca como enusquera.
Aunque son genéticamente distintivos en algunos aspectos debido al aislamiento, los vascos siguen siendo muy típicamente europeos en términos de sus secuencias de ADN Y y ADNmt, y en términos de algunos otros loci genéticos.
Sin embargo, los estudios de los haplogrupos Y-ADN encontraron que en sus linajes masculinos directos, la gran mayoría de los vascos modernos tienen un ancestro común con otros europeos occidentales, es decir, un marcado predominio del haplogrupo indoeuropeo R1b-DF27 (70%).
A pesar de su alta frecuencia en vascos, la diversidad interna de Y-STR de R1b-DF27 es más baja allí, y resulta en estimaciones de edad más recientes", implicando que se trajo a la región de otra parte.
La colección de haplogrupos de ADNmt y Y-ADN muestreados allí difería significativamente en comparación con sus frecuencias modernas.
Más bien, hace unos 4.500 años, casi todo el patrimonio de Y-ADN de la mezcla ibérica de cazadores-recolectores mesolíticos y agricultores neolíticos fue reemplazado por el linaje R1b de pastores indoeuropeos de la estepa, y la distinción genética vasca es el resultado de siglos de bajo tamaño de población, deriva genética y endogamia.
Mattias Jakobsson, de la Universidad de Uppsala en Suecia, analizó material genético de ocho esqueletos humanos de la Edad de Piedra encontrados en la Caverna de El Portalón en Atapuerca, al norte de España.
Los hallazgos fueron publicados en Proceedings of the National Academy of Sciences of the United States.
También se encontró que este grupo mezclado era ancestral a otros pueblos ibéricos de nuestros días, pero mientras los vascos permanecieron relativamente aislados durante milenios después de este tiempo, las migraciones posteriores en Iberia llevaron a la mezcla distinta y adicional en todos otros grupos ibéricos.
Hay bastantes pruebas para apoyar la hipótesis que en ese tiempo y más tarde hablaron variedades viejas de la lengua vasca (ver: la lengua de Aquitanian).
El Reino de Pamplona, un reino vasco central, más tarde conocido como Navarre, se sometió a un proceso de feudalization y era sujeto a la influencia de sus vecinos aragoneses, castellanos y franceses mucho más grandes.
Debilitado por la guerra civil navarra, la mayor parte del reino finalmente cayó antes del ataque de los ejércitos españoles (1512-1524).
Sin embargo, los vascos disfrutaron de un gran autogobierno hasta la Revolución Francesa (1790) y las Guerras Carlistas (1839, 1876), cuando los vascos apoyaron al heredero aparente Carlos V y sus descendientes.
La comunidad autónoma (un concepto establecido en la Constitución española de 1978) conocida como Euskal Autonomia Erkidegoa o EAE en euskera y como Comunidad Autónoma Vasca o CAV en español (en inglés: Comunidad Autónoma Vasca o BAC), está formada por las tres provincias españolas de Álava, Vizcaya y Gipuzkoa.
A veces se refiere simplemente como "el País Vasco" (o Euskadi) por escritores y agencias públicas sólo considerando aquellas tres provincias occidentales, sino también en ocasiones simplemente como una abreviatura conveniente cuando esto no lleva a la confusión en el contexto.
En particular en el uso común el término francés paga vasco ("País vasco"), en ausencia de la calificación adicional, se refiere a Euskal Herria entero ("Euskal Herria" en vasco), o no infrecuentemente al norte (o "francés") País vasco expresamente.
Tenga en cuenta que en contextos históricos Navarra puede referirse a un área más amplia, y que la actual provincia vasca del norte de la Baja Navarra también puede denominarse (parte de) Nafarroa, mientras que el término "Navarra Alta" (Nafarroa Garaia en vasco, Alta Navarra en español) también se encuentra como una forma de referirse al territorio de la comunidad autónoma actual.
El conocimiento del español es obligatorio en virtud de la Constitución española (artículo No.
El conocimiento de vasco, después de disminuir durante muchos años durante la dictadura de Franco debido a la persecución oficial, está otra vez en la subida debido a políticas de la lengua oficiales favorables y apoyo popular.
Sólo español es una lengua oficial de Navarre, y la lengua vasca sólo es cooficial en la región del norte de la provincia, donde la mayor parte de Navarrese de habla vasca se concentran.
Gran parte de esta población vive en o cerca del cinturón urbano de Bayona-Anglet-Biarritz (BAB) en la costa (en euskera estos son Baiona, Angelu y Miarritze).
Millones de descendientes vascos (ver americano vasco y canadiense vasco) viven en Norteamérica (los Estados Unidos; Canadá, principalmente en las provincias de Terranova y Quebec), América Latina (en 23 países), Sudáfrica y Australia.
Las estimaciones oscilan entre 2,5 y 5 millones de descendientes vascos viven en Chile; el vasco ha sido una influencia importante, si no la más fuerte, en el desarrollo cultural y económico del país.
Consistía principalmente en el área que hoy es los estados de Chihuahua y Durango.
En Guatemala, la mayoría de los vascos se han concentrado en el departamento de Sacatepéquez, Antigua Guatemala, Jalapa durante seis generaciones, mientras que algunos han emigrado a la ciudad de Guatemala.
Bambuco, una música folclórica colombiana, tiene raíces vascas.
Elko, Nevada, patrocina un festival vasco anual que celebra la danza, la cocina y las culturas de los pueblos vascos de nacionalidades española, francesa y mexicana que han llegado a Nevada desde finales del siglo XIX.
Algunos de los ranchos más grandes de América del Norte, que fueron fundados bajo estas concesiones de tierras coloniales, se pueden encontrar en esta región.
Hay una historia de la cultura vasca en Chino, California.
Son en su mayoría descendientes de colonos de España y México.
Este sentido de identidad vasca ligado a la lengua local no solo existe de forma aislada.
Al igual que con muchos estados europeos, una identidad regional, ya sea derivada linguísticamente o de otro tipo, no es mutuamente excluyente con la identidad nacional más amplia.
Tengo amigos que están involucrados en el lado político de las cosas, pero eso no es para mí.
Hay muy pocos hablantes de euskera: esencialmente todos los hablantes de euskera son bilingues en ambos lados de la frontera.
Se cree que la lengua vasca es una lengua genética aislada en contraste con otras lenguas europeas, casi todas las cuales pertenecen a la amplia familia de lenguas indoeuropeas.
El hogar en este contexto es sinónimo de raíces familiares.
Como en otras culturas, el destino de otros miembros de la familia dependía de los activos de una familia: las familias vascas ricas tendían a mantener a todos los niños de alguna manera, mientras que las familias menos ricas pueden haber tenido solo un activo para proporcionar a un niño.
Sobre todo después del advenimiento de la industrialización, este sistema causó la emigración de muchos vascos rurales a España, Francia o las Américas.
Algunos estudiosos y comentaristas han intentado conciliar estos puntos asumiendo que el parentesco patrilineal representa una innovación.
Surgieron del régimen franquista con una lengua y una cultura revitalizadas.
La región ha sido una fuente de misioneros como Francis Xavier y Michel Garicoats.
Lasuén fue el sucesor del padre franciscano Junípero Serra y fundó 9 de las 21 misiones existentes de California a lo largo de la costa.
Cuando Enrique III de Navarra se convirtió al catolicismo para convertirse en rey de Francia, el protestantismo prácticamente desapareció de la comunidad vasca.
Hoy en día, según una sola encuesta de opinión, solo un poco más del 50% de los vascos profesan algún tipo de creencia en Dios, mientras que el resto son agnósticos o ateos.
Según uno, el cristianismo llegó al País Vasco durante los siglos IV y V, pero según el otro, no tuvo lugar hasta los siglos XII y XIII.
En este sentido, el cristianismo llegó “temprano”.
Según una tradición, viajaba cada siete años entre una cueva en el Monte Anboto y otra en otra montaña (las historias varían); el clima sería húmedo cuando estaba en Anboto, seco cuando estaba en Aloña, o Supelegor, o Gorbea.
Se dice que cuando se reunieron en las altas cuevas de los picos sagrados, engendraron las tormentas.
Las leyendas también hablan de muchos y abundantes genios, como jentilak (equivalente a gigantes), lamiak (equivalente a ninfas), mairuak (constructores de los cromlechs o círculos de piedra, literalmente moros), iratxoak (imps), sorginak (brujas, sacerdotisa de Mari), y así sucesivamente.
Hay un embaucador llamado San Martín Txiki ("San Martín el Menor").
Los jentilak, por otro lado, son un pueblo legendario que explica la desaparición de un pueblo de la cultura de la Edad de Piedra que solía vivir en las tierras altas y sin conocimiento del hierro.
Durante más de un siglo, los estudiosos han discutido ampliamente el alto estatus de las mujeres vascas en los códigos de derecho, así como sus posiciones como jueces, herederos y árbitros a través de los tiempos ante-romanos, medievales y modernos.
Navarra tiene un estatuto separado de la autonomía, un arreglo polémico diseñado durante la transición española a la democracia (el Amejoramiento, una 'actualización' de su estado anterior durante la dictadura).
Las cuestiones de lealtad e identidad política, linguística y cultural son muy complejas en Navarra.
La mayoría de escuelas bajo la jurisdicción del sistema educativo vasco usan vasco como el medio primario de la enseñanza.
En contraste, el deseo de una mayor autonomía o independencia es particularmente común entre los nacionalistas vascos de izquierda.
Se consideran a sí mismos culturalmente y especialmente linguísticamente distintos de sus vecinos circundantes.
Miguel de Unamuno fue un destacado novelista y filósofo de finales del siglo XIX y XX.
También fundó la Asociación Sindical Chilena para promover un movimiento sindical basado en las enseñanzas sociales de la Iglesia Católica.
La presencia histórica de San en Botswana es particularmente evidente en la región de Tsodilo Hills del norte de Botswana.
A partir de los años 1950 hasta los años 1990, las comunidades de San cambiaron a la agricultura debido a programas de modernización mandados por el gobierno.
Ciertos grupos San son uno de los 14 "grupos de población ancestrales" conocidos; es decir, "grupos de poblaciones con ascendencia genética común, que comparten etnia y similitudes tanto en su cultura como en las propiedades de sus lenguas".
En 2003, los representantes de los pueblos san manifestaron su preferencia por el uso de esos nombres de grupos individuales, cuando fuera posible, por encima del uso del término colectivo san.
Seguí usando Bushman, y fui corregido públicamente varias veces por los justos.
En cambio, el representante del Consejo de San fue firme en que no se les causó daño o daño a ellos o a la comunidad de San con la forma en que (Die Burger) publicó la palabra 'boesman'".
El parentesco San es comparable al parentesco esquimal, con el mismo conjunto de términos que en las culturas europeas, pero también utiliza una regla de nombre y una regla de edad.
Los niños no tienen deberes sociales además de jugar, y el ocio es muy importante para San de todas las edades.
Toman importantes decisiones familiares y grupales y reclaman la propiedad de los pozos de agua y las áreas de forrajeo.
Las sequías pueden durar muchos meses y los pozos de agua pueden secarse.
En este agujero se inserta un tallo de hierba largo y hueco.
La primavera temprana es la estación más dura: un período seco y caluroso después del invierno fresco y seco.
Las mujeres recolectan frutas, bayas, tubérculos, cebollas de arbustos y otros materiales vegetales para el consumo de la banda.
Dependiendo de la ubicación, los San consumen de 18 a 104 especies, incluyendo saltamontes, escarabajos, orugas, polillas, mariposas y termitas.
Estos haplogrupos son subgrupos específicos de los haplogrupos A y B, las dos ramas más tempranas en el árbol del cromosoma Y humano.
El haplogrupo mitocondrial más divergente (más antiguo), L0d, se ha identificado en sus frecuencias más altas en los grupos San del sur de África.
Los san se han visto particularmente afectados por la invasión de los pueblos mayoritarios y los agricultores no indígenas a las tierras tradicionalmente ocupadas por los san.
La pérdida de tierras es un importante contribuyente a los problemas que enfrentan los pueblos indígenas de Botswana, incluyendo especialmente el desalojo de los San de la Reserva de Caza del Kalahari Central.
Esto otorgaría regalías a los San por los beneficios de su conocimiento indígena.
Van der Post creció en Sudáfrica y tuvo una respetuosa fascinación de por vida con las culturas nativas africanas.
Impulsado por una fascinación de toda la vida con esta "tribu desaparecida", Van der Post publicó un libro de 1958 sobre esta expedición, titulado El mundo perdido del Kalahari.
Su primera película The Hunters, estrenada en 1957, muestra una caza de jirafas.
Su hermana Elizabeth Marshall Thomas escribió varios libros y numerosos artículos sobre los San, basados en parte en sus experiencias viviendo con estas personas cuando su cultura todavía estaba intacta.
Esto fue revisado por Lawrence Van Gelder para el New York Times, quien dijo que la película "constituye un acto de preservación y un réquiem".
La serie de la BBC The Life of Mammals (2003) incluye imágenes de video de un San indígena del desierto de Kalahari que emprende una búsqueda persistente de un kudu a través de duras condiciones del desierto.
Debido a sus similitudes, las obras de San pueden ilustrar las razones de las pinturas rupestres antiguas.
La película fue dirigida por Jamie Uys, quien regresó a The San una década más tarde con The Gods Must Be Crazy, que resultó ser un éxito internacional.
James A. Michener's The Covenant (1980), es una obra de ficción histórica centrada en Sudáfrica.
La novela de Norman Rush de 1991 Mating presenta un campamento de Basarwa cerca de la ciudad (imaginaria) de Botswana donde se desarrolla la acción principal.
En 2007, David Gilman publicó El aliento del diablo.
El novio del protagonista de The No.
Los pueblos germánicos eran un grupo histórico de personas que vivían en Europa Central y Escandinavia.
En discusiones del período romano, los pueblos germánicos a veces se refieren como Germani o alemanes antiguos, aunque muchos eruditos consideren el segundo término problemático, ya que sugiere la identidad con alemanes modernos.
En contraste, los autores romanos describieron por primera vez a los pueblos germánicos cerca del Rin en el momento en que el Imperio Romano estableció su dominio en esa región.
Los esfuerzos romanos para integrar la gran área entre el Rin y el Elba terminaron alrededor del 16 dC, después de la gran derrota romana en la Batalla del Bosque de Teutoburgo en el 9 dC.
En el 3er siglo los godos de habla germánica dominaron la Estepa de Pontic, fuera de Germania, y lanzaron una serie de expediciones marítimas en los Balcanes y Anatolia por lo que Chipre.
En cambio, la arqueología muestra una sociedad y una economía complejas en toda Germania.
Tradicionalmente, los pueblos germánicos han sido vistos como poseedores de una ley dominada por los conceptos de disputa y compensación de sangre.
Los antiguos pueblos de habla germánica probablemente compartían una tradición poética común, verso aliterativo, y los pueblos germánicos posteriores también compartieron leyendas que se originaron en el Período Migratorio.
Incluso la lengua de la que deriva es un tema de disputa, con propuestas de origen germánico, celta y latino e ilirio.
Independientemente de su idioma de origen, el nombre se transmitió a los romanos a través de hablantes celtas.
A finales de la antiguedad, solo los pueblos cercanos al Rin, especialmente los francos, y a veces los alemanni, eran llamados germanos por los escritores latinos o griegos.
Mientras los autores romanos no excluyeron consecuentemente a la gente de habla celta, o trataron los pueblos germánicos como el nombre de un pueblo, esta nueva definición, usando la lengua germánica como el criterio principal, entendió Germani como una gente o nación con una identidad del grupo estable unida a la lengua.
Algunos eruditos que estudian la Edad media Temprana ahora enfatizan la pregunta de si los pueblos germánicos se vieron como una unidad étnica, mientras los otros señalan la existencia de lenguas germánicas como un hecho histórico que se puede usar para identificar pueblos germánicos, independientemente de si se vieron como "germánicos".
Por tales razones, Goffart sostiene que el término germánico se debería evitar completamente a favor de "bárbaro" excepto en el sentido linguístico, y los historiadores como Walter Pohl también han pedido que el término se evite o se use con la explicación cuidadosa.
En la cuenta de Caesar, la característica que define más clara de la gente de Germani era que vivieron al este del Rin, frente a Galia en el lado de Oeste, una observación que hizo con digresiones históricas en su escritura.
Tácito a veces no estaba seguro de si un pueblo era germánico o no, expresando su incertidumbre sobre los Bastarnae, que dice que se parecían a los sármatas pero hablaban como los alemanes, sobre los Osi y los Cotini, y sobre los Aesti, que eran como suevos pero hablaban un idioma diferente.
El Danubio Superior sirvió de una frontera del sur.
No está claro si estos germánicos hablaban una lengua germánica, y pueden haber sido hablantes celtas en su lugar.
Tacitus sigue mencionando tribus germánicas en la orilla occidental del Rin en el período del Imperio temprano, como Tungri, Nemetes, Ubii y Batavi.
Inspirado por esto, estos tres grupos también a veces se usan en la terminología linguística moderna más vieja, intentando describir las divisiones de lenguas germánicas posteriores.)
Herminones o Hermiones en el interior, incluidos los Suevi, los Hermunduri, los Chatti, los Cherusci según Plinio.
Por otro lado, Tácito escribió en el mismo pasaje que algunos creen que hay otros grupos que son tan antiguos como estos tres, incluyendo "los Marsi, Gambrivii, Suevi, Vandilii".
Strabo, que se concentró principalmente en Germani entre Elbe y Rin, y no menciona a los hijos de Mannus, también aparta los nombres de Germani que no son Suevian, en otros dos grupos, implicando de manera similar tres divisiones principales: "tribus alemanas más pequeñas, como Cherusci, Chatti, Gamabrivi, Chattuarii, y al lado del océano Sicambri, Chaubi, Bructeri, Caimbi".
Durante el período pregermánico (2500-500 A.C.), la proto-lengua ha sido casi con seguridad bajo la influencia de sustratos linguísticos todavía perceptibles en la fonología germánica y el léxico.
También hay mucha influencia en el vocabulario de las lenguas celtas, pero la mayor parte de esto parece ser mucho más tarde, con la mayor parte de préstamos que ocurren antes o durante el cambio sano descrito por la Ley de Grimm.
Aunque el protogermánico se reconstruye sin dialectos a través del método comparativo, es casi seguro que nunca fue un protolenguaje uniforme.
Las inscripciones rúnicas atestiguadas más tempranas (peine de Vimose, punta de lanza de Ovre Stabu), al principio concentradas en Dinamarca moderna y escrita con el sistema de Futhark Mayor, se fechan a la segunda mitad del 2do siglo CE.
Sin embargo, la fusión de vocales protogermánicas no acentuadas, atestiguadas en inscripciones rúnicas de los siglos IV y V, también sugiere que el nórdico primitivo no podría haber sido un predecesor directo de los dialectos germánicos occidentales.
Antes de finales del 3er siglo CE, las divergencias idiomáticas como la pérdida germánica de Oeste de la consonante final ya habían ocurrido dentro del continuo del dialecto de Noroeste "residual".
La inclusión de las lenguas de Burgundian y Vandalic dentro del grupo germánico del Este, mientras plausible, todavía es incierta debido a su atestación escasa.
Una sociedad es un grupo de individuos involucrados en la interacción social persistente, o un grupo social grande que comparte el mismo territorio espacial o social, típicamente sujeto a la misma autoridad política y expectativas culturales dominantes.
Las sociedades construyen patrones de comportamiento al considerar ciertas acciones o discursos como aceptables o inaceptables.
En la medida en que sea colaborativa, una sociedad puede permitir que sus miembros se beneficien de maneras que de otro modo serían difíciles sobre una base individual; Por lo tanto, se pueden distinguir los beneficios individuales y sociales (comunes), o en muchos casos se puede encontrar que se superponen.
Esto era a su vez de la palabra latina societas, que a su vez se sacó del sustantivo socius ("camarada, amigo, aliado"; adjetival forma socialis) solía describir un vínculo o interacción entre partidos que son amistosos, o al menos civil.
En los años 1630 se usó en la referencia a "la gente atada por vecindad y relación consciente de vivir juntos en una comunidad ordenada".
Estas estructuras pueden tener diversos grados de poder político, dependiendo de los entornos culturales, geográficos e históricos con los que estas sociedades deben lidiar.
Sociedades tribales en las que hay algunos casos limitados de rango social y prestigio.
Esta evolución cultural tiene un profundo efecto en los patrones de comunidad.
Las ciudades se convirtieron en ciudades-estado y naciones-estado.
Por el contrario, los miembros de una sociedad también pueden evitar o ser chivos expiatorios de cualquier miembro de la sociedad que viole sus normas.
Algunas sociedades otorgan estatus a un individuo o grupo de personas cuando ese individuo o grupo realiza una acción admirada o deseada.
Aunque los humanos han establecido muchos tipos de sociedades a lo largo de la historia, los antropólogos tienden a clasificar diferentes sociedades de acuerdo con el grado en que los diferentes grupos dentro de una sociedad tienen acceso desigual a ventajas como los recursos, el prestigio o el poder.
Sin embargo, algunas sociedades de caza y recolección en áreas con abundantes recursos (como la gente de Tlingit) vivían en grupos más grandes y formaron complejas estructuras sociales jerárquicas como el chiefdom.
Los estados dentro de la tribu son relativamente iguales, y las decisiones se alcanzan a través de un acuerdo general.
No hay cargos políticos que contengan poder real, y un jefe es simplemente una persona de influencia, una especie de asesor; por lo tanto, las consolidaciones tribales para la acción colectiva no son gubernamentales.
Debido a que su suministro de alimentos es mucho más confiable, las sociedades pastorales pueden apoyar a poblaciones más grandes.
Por ejemplo, algunas personas se convierten en artesanos, produciendo herramientas, armas y joyas, entre otros artículos de valor.
Estas familias a menudo ganan poder a través de su mayor riqueza.
La vegetación silvestre se corta y se quema, y las cenizas se utilizan como fertilizantes.
Pueden regresar a la tierra original varios años más tarde y comenzar el proceso nuevamente.
El tamaño de la población de una aldea depende de la cantidad de tierra disponible para la agricultura; por lo tanto, las aldeas pueden variar desde tan solo 30 personas hasta 2000.
Los sociólogos usan la frase revolución agrícola para referirse a los cambios tecnológicos que ocurrieron hace 8.500 años que llevaron a cultivar cultivos y criar animales de granja.
Mayores grados de estratificación social aparecieron en las sociedades agrarias.
Sin embargo, a medida que las tiendas de alimentos mejoraron y las mujeres asumieron un papel menor en el suministro de alimentos para la familia, se volvieron cada vez más subordinadas a los hombres.
También apareció un sistema de gobernantes con alto estatus social.
La exploración europea de las Américas sirvió como un ímpetu para el desarrollo del capitalismo.
Esto produjo aumentos dramáticos adicionales en la eficiencia.
Este superávit más grande hizo que todos los cambios discutidos anteriormente en la revolución de la domesticación se volvieran aún más pronunciados.
Sin embargo, la desigualdad se hizo aún mayor que antes.
Geográficamente, cubre al menos los países de Europa occidental, América del Norte, Australia y Nueva Zelanda.
Uno de los ámbitos de interés de la Unión Europea es la sociedad de la información.
Algunas asociaciones académicas, profesionales y científicas se describen a sí mismas como sociedades (por ejemplo, la American Mathematical Society, la American Society of Civil Engineers o la Royal Society).
Una comunidad es una unidad social (un grupo de seres vivos) con elementos comunes como normas, religión, valores, costumbres o identidad.
En este sentido, es sinónimo del concepto de un asentamiento antiguo, ya sea una aldea, un pueblo, una ciudad o una ciudad.
La mayoría de las reconstrucciones de las comunidades sociales por parte de los arqueólogos se basan en el principio de que la interacción social en el pasado estaba condicionada por la distancia física.
Ningún grupo es exclusivamente uno u otro.
La socialización está influenciada principalmente por la familia, a través de la cual los niños aprenden primero las normas de la comunidad.
Los profesionales del desarrollo comunitario deben entender cómo trabajar con individuos y cómo afectar las posiciones de las comunidades dentro del contexto de instituciones sociales más grandes.
En la intersección entre el desarrollo comunitario y la construcción de la comunidad hay una serie de programas y organizaciones con herramientas de desarrollo comunitario.
Vacío: Se mueve más allá de los intentos de arreglar, sanar y convertir la etapa de caos, cuando todas las personas se vuelven capaces de reconocer su propia herida y quebrantamiento, común a los seres humanos.
Los tres tipos básicos de organización comunitaria son la organización de base, la construcción de coaliciones y la "organización comunitaria basada en la institución" (también llamada "organización comunitaria de base amplia", un ejemplo de la cual es la organización comunitaria basada en la fe u organización comunitaria basada en la congregación).
Consultado el: 22 de junio de 2008.
La organización comunitaria puede centrarse en algo más que resolver problemas específicos.
Dichos grupos facilitan y fomentan la toma de decisiones de consenso con un enfoque en la salud general de la comunidad en lugar de un grupo de interés específico.
Comunidades basadas en la identidad: van desde la camarilla local, la subcultura, el grupo étnico, la civilización religiosa, multicultural o pluralista, o las culturas comunitarias globales de hoy.
Las relaciones entre los miembros de una comunidad virtual tienden a centrarse en el intercambio de información sobre temas específicos.
Los eruditos en las humanidades son "eruditos de las humanidades" o humanistas.
Las humanidades generalmente estudian tradiciones locales, a través de su historia, literatura, música y artes, con un énfasis en la comprensión de individuos particulares, eventos o épocas.
La antropología (como algunos campos de la historia) no encaja fácilmente en una de estas categorías, y las diferentes ramas de la antropología se basan en uno o más de estos dominios.
La palabra anthropos es de la palabra griega para "ser humano" o "persona".
Esto significa que, aunque los antropólogos generalmente se especializan en un solo subcampo, siempre tienen en cuenta los aspectos biológicos, linguísticos, históricos y culturales de cualquier problema.
La búsqueda del holismo lleva a la mayoría de los antropólogos a estudiar a un pueblo en detalle, utilizando datos biogenéticos, arqueológicos y linguísticos junto con la observación directa de las costumbres contemporáneas.
La arqueología se puede considerar tanto una ciencia social como una rama de las humanidades.
Gran parte de la filosofía del siglo XX y XXI se ha dedicado al análisis del lenguaje y a la cuestión de si, como afirmó Wittgenstein, muchas de nuestras confusiones filosóficas se derivan del vocabulario que usamos; la teoría literaria ha explorado las características retóricas, asociativas y de ordenamiento del lenguaje; y los linguistas históricos han estudiado el desarrollo de las lenguas a través del tiempo.
Se ha definido como un "sistema de reglas", como un "concepto interpretativo" para lograr justicia, como una "autoridad" para mediar los intereses de las personas, e incluso como "el mando de un soberano, respaldado por la amenaza de una sanción".
Las leyes son políticas, porque los políticos las crean.
Como señaló Immanuel Kant, "la filosofía griega antigua se dividió en tres ciencias: física, ética y lógica".)
El sintoísmo, el taoísmo y otras religiones populares o naturales no tienen códigos éticos.
Los sistemas de creencias implican un modelo lógico que las religiones no muestran debido a sus contradicciones internas, falta de evidencia y falsedades.
Son necesarios para comprender el predicamento humano.
Las religiones no fundadoras son el hinduismo, el sintoísmo y las religiones nativas o populares.
Cuando las religiones tradicionales no abordan nuevas preocupaciones, entonces surgirán nuevas religiones.
Las artes escénicas también son apoyadas por trabajadores en campos relacionados, como la composición de canciones y el arte escénico.
Esto se llama Performance Art.
La danza también se usa para describir métodos de comunicación no verbal (ver lenguaje corporal) entre humanos o animales (baile de abejas, baile de apareamiento) y movimiento en objetos inanimados (las hojas bailadas en el viento).
En el arte bizantino y gótico de la Edad Media, el dominio de la iglesia insistía en la expresión de verdades bíblicas y no materiales.
Una característica de este estilo es que el color local a menudo se define por un contorno (un equivalente contemporáneo es la caricatura).
Por lo general, implica hacer marcas en una superficie mediante la aplicación de presión de una herramienta, o mover una herramienta a través de una superficie.
Sin embargo, cuando se usa en un sentido artístico, significa el uso de esta actividad en combinación con el dibujo, la composición y otras consideraciones estéticas para manifestar la intención expresiva y conceptual del practicante.
El negro está asociado con el luto en Occidente, pero en otros lugares puede ser blanco.
La palabra "rojo", por ejemplo, puede cubrir una amplia gama de variaciones en el rojo puro del espectro.
Esto comenzó con el cubismo y no es pintura en sentido estricto.
En consecuencia, muchos pasan los primeros años después de la graduación decidiendo qué hacer a continuación, lo que resulta en menores ingresos al comienzo de su carrera; Mientras tanto, los graduados de programas orientados a la carrera experimentan una entrada más rápida en el mercado laboral.
Sin embargo, la evidencia empírica también muestra que los graduados de humanidades aún obtienen ingresos notablemente más altos que los trabajadores sin educación postsecundaria, y tienen niveles de satisfacción laboral comparables a sus compañeros de otros campos.
Sin embargo, como porcentaje del tipo de títulos otorgados, las humanidades parecen estar disminuyendo.
Los fondos federales representan una fracción mucho menor de los fondos para humanidades que otros campos como STEM o medicina.
Esta comprensión, afirmaron, une a personas de ideas afines de orígenes culturales similares y proporciona un sentido de continuidad cultural con el pasado filosófico.
Además de su aplicación social, la imaginación narrativa es una herramienta importante en la (re)producción del significado entendido en la historia, la cultura y la literatura.
El postestructuralismo ha problematizado un enfoque del estudio humanista basado en cuestiones de significado, intencionalidad y autoría.
Además, el pensamiento crítico, aunque podría decirse que es el resultado de una formación humanista, puede adquirirse en otros contextos.
Tal placer contrasta con la creciente privatización del ocio y la gratificación instantánea característica de la cultura occidental; por lo tanto, satisface los requisitos de Jurgen Habermas para el desprecio del estatus social y la problematización racional de áreas previamente incuestionables necesarias para un esfuerzo que tiene lugar en la esfera pública burguesa.
A pesar de muchos argumentos basados en humanidades contra las humanidades, algunos dentro de las ciencias exactas han pedido su regreso.
Es bueno conocer la historia de la filosofía”.
La comunicación (del latín communicare, que significa "compartir" o "estar en relación con") es "una respuesta aparente a las dolorosas divisiones entre el yo y el otro, privado y público, y el pensamiento interno y la palabra externa".
Composición del mensaje (más elaboración interna o técnica sobre qué expresar exactamente).
Las fuentes de ruido, como las fuerzas naturales y, en algunos casos, la actividad humana (tanto intencional como accidental) comienzan a influir en la calidad de las señales que se propagan desde el remitente a uno o más receptores.
Interpretación y sentido del supuesto mensaje original.
Ejemplos de intención son los movimientos voluntarios e intencionales, como estrechar una mano o guiñar un ojo, así como los involuntarios, como la sudoración.
Del mismo modo, los textos escritos incluyen elementos no verbales como el estilo de escritura, la disposición espacial de las palabras y el uso de emoticonos para transmitir emociones.
Algunas de las funciones de la comunicación no verbal en los humanos son complementar e ilustrar, reforzar y enfatizar, reemplazar y sustituir, controlar y regular, y contradecir el mensaje denotativo.
Para tener una comunicación total, todos los canales no verbales como el cuerpo, la cara, la voz, la apariencia, el tacto, la distancia, el tiempo y otras fuerzas ambientales deben participar durante la interacción cara a cara.
Los comportamientos no verbales pueden formar un sistema de lenguaje universal.
El aprendizaje de idiomas normalmente ocurre más intensamente durante la infancia humana.
Los lenguajes construidos como el esperanto, los lenguajes de programación y varios formalismos matemáticos no están necesariamente restringidos a las propiedades compartidas por los lenguajes humanos.
Las propiedades del lenguaje se rigen por reglas.
Contrariamente a la creencia popular, los lenguajes de señas del mundo (por ejemplo, el lenguaje de señas americano) se consideran comunicación verbal porque su vocabulario de señas, gramática y otras estructuras linguísticas cumplen con todas las clasificaciones necesarias como lenguajes hablados.
La comunicación es, por lo tanto, un proceso por el cual el significado se asigna y se transmite en un intento de crear una comprensión compartida.
Un canal, al que las señales están adaptadas para la transmisión.
Un destino, donde llega el mensaje.
No hay subsidios para diferentes propósitos.
Sin tener en cuenta los contextos situacionales.
Estos actos pueden tomar muchas formas, en una de las diversas formas de comunicación.
Sintácticas (propiedades formales de signos y símbolos).
A la luz de estas debilidades, Barnlund (2008) propuso un modelo transaccional de comunicación.
Esta segunda actitud de comunicación, denominada modelo constitutivo o visión constructivista, se centra en cómo un individuo se comunica como el factor determinante de la forma en que se interpretará el mensaje.
Los filtros personales del remitente y los filtros personales del receptor pueden variar dependiendo de diferentes tradiciones regionales, culturas o género; lo que puede alterar el significado previsto del contenido del mensaje.
Aunque el modelo implica algo como libros de códigos, no están representados en ninguna parte del modelo, lo que crea muchas dificultades conceptuales.
Las empresas con recursos limitados pueden optar por participar solo en algunas de estas actividades, mientras que las organizaciones más grandes pueden emplear un espectro completo de comunicaciones.
El entorno de la información es el conjunto de individuos, organizaciones y sistemas que recopilan, procesan, difunden o actúan sobre la información.
En la comunicación verbal interpersonal se envían dos tipos de mensajes: un mensaje de contenido y un mensaje relacional.
Este es el estudio de cómo los individuos explican qué causa diferentes eventos y comportamientos.
La comunicación abierta y honesta crea una atmósfera que permite a los miembros de la familia expresar sus diferencias, así como el amor y la admiración por los demás.
Los investigadores desarrollan teorías para comprender los comportamientos de comunicación.
Esto también incluye la falta de expresión de comunicación "apropiada para el conocimiento", que ocurre cuando una persona usa palabras legales ambiguas o complejas, jerga médica o descripciones de una situación o entorno que el destinatario no entiende.
Del mismo modo, los equipos deficientes u obsoletos, en particular el fracaso de la administración para introducir nuevas tecnologías, también pueden causar problemas.
Los ejemplos pueden incluir una estructura organizacional que no está clara y, por lo tanto, hace que sea confuso saber con quién comunicarse.
Es mejor evitar tales palabras usando alternativas siempre que sea posible.
Sin embargo, la investigación en comunicación ha demostrado que la confusión puede dar legitimidad a la investigación cuando la persuasión falla.
Es cuando el emisor está expresando un pensamiento o una palabra, pero el receptor le da un significado diferente.
Esto, a su vez, ha llevado a un cambio notable en la forma en que las generaciones más jóvenes se comunican y perciben su propia autoeficacia para comunicarse y conectarse con los demás.
Miedo a ser criticado – Este es un factor importante que impide una buena comunicación.
Esto no solo aumentará su confianza, sino que también mejorará su lenguaje y vocabulario.
Ciertas actitudes también pueden dificultar la comunicación.
El acto de desambiguación se refiere al intento de reducir el ruido y las interpretaciones erróneas, cuando el valor semántico o el significado de un signo pueden estar sujetos al ruido, o en presencia de múltiples significados, lo que dificulta la creación de sentido.
Por ejemplo: las palabras, los colores y los símbolos tienen diferentes significados en diferentes culturas.
La comprensión de los aspectos culturales de la comunicación se refiere a tener conocimiento de diferentes culturas con el fin de comunicarse eficazmente con las personas de culturas cruzadas.
También incluye sonidos de la garganta y todos estos están muy influenciados por las diferencias culturales a través de las fronteras.
Este concepto difiere de una cultura a otra, ya que el espacio permisible varía en diferentes países.
Algunos problemas que explican este concepto son las pausas, los silencios y el retraso de respuesta durante una interacción.
En diferentes países, los mismos gestos y posturas se utilizan para transmitir mensajes diferentes.
Las raíces de las plantas se comunican con las bacterias del rizoma, hongos e insectos dentro del suelo.
En paralelo producen otros volátiles para atraer parásitos que atacan a estos herbívoros.
Los bioquímicos provocan que el organismo fúngico reaccione de una manera específica, mientras que si las mismas moléculas químicas no son parte de los mensajes bióticos, no provocan que el organismo fúngico reaccione.
A través de la detección de quórum, las bacterias pueden detectar la densidad de las células y regular la expresión génica en consecuencia.
La información, en un sentido general, son datos procesados, organizados y estructurados.
La información está asociada con los datos.
La información puede transmitirse en el tiempo, a través del almacenamiento de datos y el espacio, a través de la comunicación y las telecomunicaciones.
La información puede codificarse en varias formas para su transmisión e interpretación (por ejemplo, la información puede codificarse en una secuencia de signos o transmitirse a través de una señal).
La incertidumbre es inversamente proporcional a la probabilidad de ocurrencia.
Por otra parte, el latín en sí ya contenía la palabra ?nf?rm?ti? que significa concepto o idea, pero la medida en que esto puede haber influido en el desarrollo de la palabra información en Inglés no está claro.
En el griego moderno, la palabra ? todavía está en el uso diario y tiene el mismo significado que la información de la palabra en inglés.
El campo fue establecido fundamentalmente por las obras de Harry Nyquist y Ralph Hartley en la década de 1920, y Claude Shannon en la década de 1940.
La entropía cuantifica la cantidad de incertidumbre involucrada en el valor de una variable aleatoria o el resultado de un proceso aleatorio.
Los subcampos importantes de la teoría de la información incluyen la codificación de fuentes, la teoría de la complejidad algorítmica, la teoría de la información algorítmica y la seguridad de la teoría de la información.
En su libro Ecología sensorial biofísico David B. Dusenbery llamó a estas entradas causales.
En la práctica, la información suele ser transportada por estímulos débiles que deben ser detectados por sistemas sensoriales especializados y amplificados por entradas de energía antes de que puedan ser funcionales para el organismo o sistema.
La secuencia de nucleótidos es un patrón que influye en la formación y el desarrollo de un organismo sin necesidad de una mente consciente.
En otras palabras, se puede decir que la información en este sentido es algo potencialmente percibido como representación, aunque no creado o presentado para ese propósito.
Si la respuesta proporciona conocimiento depende de la persona informada.
Este es el equivalente informativo de casi 61 CD-ROM por persona en 2007.
Gestión de registros de sonido asegura que la integridad de los registros se conserva durante el tiempo que se requieren.
Beynon-Davies explica el concepto multifacético de la información en términos de señales y sistemas de señales.
Pragmática se ocupa del propósito de la comunicación.
En otras palabras, la pragmática vincula el lenguaje a la acción.
La semántica es el estudio del significado de los signos: la asociación entre los signos y el comportamiento.
La sintaxis como área estudia la forma de comunicación en términos de la lógica y la gramática de los sistemas de signos.
Él introduce el concepto de costos de información lexicográfica y se refiere al esfuerzo que un usuario de un diccionario debe hacer para encontrar primero, y luego entender los datos para que puedan generar información.
En una situación comunicativa, las intenciones se expresan a través de mensajes que comprenden colecciones de signos interrelacionados tomados de un lenguaje mutuamente entendido por los agentes involucrados en la comunicación.
La visualización de la información (acortada como InfoVis) depende del cálculo y la representación digital de los datos, y ayuda a los usuarios en el reconocimiento de patrones y la detección de anomalías.
El término se emplea generalmente en sociología y las otras ciencias sociales, así como en filosofía y bioética.
En las sociedades en desarrollo puede basarse principalmente en el parentesco y los valores compartidos, mientras que las sociedades más desarrolladas acumulan varias teorías sobre lo que contribuye a un sentido de solidaridad, o más bien, cohesión social.
Durkheim introdujo los términos solidaridad mecánica y orgánica como parte de su teoría del desarrollo de las sociedades en La división del trabajo en la sociedad (1893).
Diccionario Collins de Sociología, p405-6.
Definición: es la cohesión social basada en la dependencia que los individuos tienen unos de otros en sociedades más avanzadas.
Los primeros filósofos antiguos como Sócrates y Aristóteles discuten la solidaridad como un marco de ética de la virtud porque para vivir una buena vida uno debe realizar acciones y comportarse de una manera que sea solidaria con la comunidad.
La práctica moderna de la bioética está significativamente influenciada por el concepto de Immanuel Kant del imperativo categórico.
Los estudios en el extranjero eran prácticamente inexistentes.
Los primeros se convirtieron en defensores de los estudios de área, los últimos defensores de la teoría de la modernización.
De 1953 a 1966 contribuyó con 270 millones de dólares a 34 universidades para estudios de área e idiomas.
Otros programas grandes e importantes siguieron a Ford.
Otros insistieron, sin embargo, que una vez que se establecieron en los campus universitarios, los estudios de área comenzaron a abarcar una agenda intelectual mucho más amplia y profunda que la prevista por las agencias gubernamentales, por lo tanto no centrada en Estados Unidos.
Otros campos de investigación interdisciplinarios como estudios de mujeres, estudios de género, estudios de discapacidad, estudios LGBT y estudios étnicos (incluidos estudios afroamericanos, estudios asiáticoamericanos, estudios latinos, estudios chicanos y estudios nativos americanos) no forman parte de los estudios de área, pero a veces se incluyen en la discusión junto con él.
La demografía (del prefijo demo- del griego antiguo ? (d?mos) que significa 'la gente', y la -grafía de ? (graph?) que significa 'escritura, descripción o medición') es el estudio estadístico de poblaciones, especialmente seres humanos.
Los datos demográficos de los pacientes forman el núcleo de los datos de cualquier institución médica, como la información de contacto de pacientes y emergencias y los datos de registros médicos de pacientes.
El término demografía se refiere al estudio general de la población.
En la Edad Media, los pensadores cristianos dedicaron mucho tiempo a refutar las ideas clásicas sobre la demografía.
Uno de los estudios demográficos más tempranos en el período moderno era Observaciones Naturales y Políticas Hechas sobre las Cuentas de Mortalidad (1662) por John Graunt, que contiene una forma primitiva de la tabla de la vida.
Su trabajo influyó en Thomas Robert Malthus, quien, escribiendo a finales del siglo XVIII, temía que, si no se controlaba, el crecimiento de la población tendería a superar el crecimiento en la producción de alimentos, lo que llevaría a una hambruna y pobreza cada vez mayores.
Un censo es el otro método directo común de recopilar datos demográficos.
Los análisis se llevan a cabo después de un censo para estimar cuánto sobre o undercounting ocurrió.
Otros métodos indirectos en la demografía contemporánea incluyen preguntar a las personas sobre hermanos, padres e hijos.
Incluyen modelos de mortalidad (incluyendo la tabla de vida, modelos de Gompertz, modelos de peligros, modelos de riesgos proporcionales de Cox, tablas de vida de múltiples decrementos, logits relacionales de Brass), fertilidad (modelo de Hernes, modelos de Coale-Trussell, proporciones de progresión de paridad), matrimonio (media única en el matrimonio, modelo de Page), discapacidad (método de Sullivan, tablas de vida multiestatales), proyecciones de población (modelo de Lee-Carter, la matriz de Leslie).
Las tasas de fecundidad específicas por edad, el número anual de nacidos vivos por cada 1,000 mujeres en grupos de edad particulares (generalmente entre 15 y 24 años, etc.)
La expectativa de vida (o esperanza de vida), el número de años que un individuo a una edad dada podría esperar vivir en los niveles de mortalidad actuales.
Una población estacionaria, que es a la vez estable e inmutable en tamaño (la diferencia entre la tasa bruta de natalidad y la tasa bruta de mortalidad es cero).
Tenga en cuenta que la tasa de mortalidad bruta como se define anteriormente y se aplica a toda una población puede dar una impresión engañosa.
Las personas que cambian sus autoetiquetas étnicas o cuya clasificación étnica en las estadísticas del gobierno cambia con el tiempo pueden considerarse como migrando o moviéndose de una subcategoría de población a otra.
La cifra de esta sección muestra las últimas proyecciones (2004) de las Naciones Unidas de la población mundial hasta el año 2150 (rojo + alto, naranja + medio, verde + bajo).
La mortalidad es el estudio de las causas, consecuencias y medición de los procesos que afectan la muerte de los miembros de la población.
Los investigadores de migración no designan a los movimientos como "migraciones" a menos que sean algo permanentes.
Hoy en día, la demografía se enseña ampliamente en muchas universidades de todo el mundo, atrayendo a estudiantes con capacitación inicial en ciencias sociales, estadísticas o estudios de salud.
En este sentido, se puede ver la ciencia de la información como una respuesta al determinismo tecnológico, la creencia de que la tecnología "se desarrolla por sus propias leyes, que se da cuenta de su propio potencial, limitado sólo por los recursos materiales disponibles y la creatividad de sus desarrolladores.
Se ocupa de ese conjunto de conocimientos relacionados con la originación, recopilación, organización, almacenamiento, recuperación, interpretación, transmisión, transformación y utilización de la información.
Esto es especialmente cierto cuando se relaciona con el concepto desarrollado por A. I. Mikhailov y otros autores soviéticos a mediados de la década de 1960.
Las definiciones que dependen de la naturaleza de las herramientas utilizadas para obtener información significativa de los datos están surgiendo en los programas académicos de Informática.
Se puede usar para razonar sobre las entidades dentro de ese dominio y se puede usar para describir el dominio.
Tradicionalmente, su trabajo ha sido con materiales impresos, pero estas habilidades se utilizan cada vez más con materiales electrónicos, visuales, de audio y digitales.
Institucionalmente, la ciencia de la información surgió en el siglo XIX junto con muchas otras disciplinas de ciencias sociales.
En 1731, Benjamin Franklin estableció Library Company de Filadelfia, la primera biblioteca poseída por un grupo de ciudadanos públicos, que rápidamente se ampliaron más allá del reino de libros y se hicieron un centro del experimento científico, y que recibió exposiciones públicas de experimentos científicos.
En 1801, Joseph Marie Jacquard inventó un sistema de tarjetas perforadas para controlar las operaciones del telar de tejido en Francia.
Hacia 1843 Richard Hoe desarrolló la prensa rotatoria, y en 1844 Samuel Morse envió el primer mensaje del telégrafo público.
En 1860 se celebró un congreso en Karlsruhe Technische Hochschule para discutir la viabilidad de establecer una nomenclatura sistemática y racional para la química.
Al año siguiente, la Royal Society comenzó la publicación de su Catálogo de Papeles en Londres.
Muchos historiadores de la ciencia de la información citan a Paul Otlet y Henri La Fontaine como los padres de la ciencia de la información con la fundación del Instituto Internacional de Bibliografía (IIB) en 1895.
Los documentalistas enfatizaron la integración utilitarista de la tecnología y la técnica hacia objetivos sociales específicos.
Otlet y Lafontaine establecieron numerosas organizaciones dedicadas a la normalización, la bibliografía, las asociaciones internacionales y, en consecuencia, la cooperación internacional.
Esta colección incluyó hojas de papel estandarizadas y tarjetas archivadas en gabinetes diseñados a medida de acuerdo con un índice jerárquico (que recogía información de diversas fuentes en todo el mundo) y un servicio comercial de recuperación de información (que respondía a solicitudes escritas copiando información relevante de las tarjetas de índice).
Además, los límites tradicionales entre las disciplinas comenzaron a desvanecerse y muchos estudiosos de la ciencia de la información se unieron a otros programas.
La década de 1980 también vio la aparición de numerosos grupos de interés especial para responder a los cambios.
Zhang, B., Semenov, A., Vos, M. y Veijlainen, J. (2014).
Compartir a través de las redes sociales se ha vuelto tan influyente que los editores deben "jugar bien" si desean tener éxito.
Es por esta razón que estas redes se han realizado por el potencial que proporcionan”.
¿Qué pasa con la asignación de privilegios y la restricción del acceso a usuarios no autorizados?
Es una disciplina emergente y una comunidad de práctica centrada en unir los principios del diseño y la arquitectura al paisaje digital.
Los sistemas automatizados de recuperación de información se utilizan para reducir lo que se ha llamado "sobrecarga de información".
Un proceso de recuperación de información comienza cuando un usuario introduce una consulta en el sistema.
En cambio, varios objetos pueden coincidir con la consulta, tal vez con diferentes grados de relevancia.
Dependiendo de la aplicación, los objetos de datos pueden ser, por ejemplo, documentos de texto, imágenes, audio, mapas mentales o videos.
La búsqueda de información está relacionada con, pero diferente de, la recuperación de información (IR).
La lógica se utiliza para proporcionar semántica formal de cómo las funciones de razonamiento deben aplicarse a los símbolos en el sistema KR.
También era una creencia común que los desastres naturales como el hambre y el diluvio eran represalias divinas que llevaban signos del disgusto del Cielo con el gobernante, por lo que a menudo habría revueltas después de grandes desastres ya que la gente veía estas calamidades como señales de que el Mandato del Cielo había sido retirado.
El concepto es en cierto modo similar al concepto europeo del derecho divino de los reyes; sin embargo, a diferencia del concepto europeo, no confiere un derecho incondicional a gobernar.
El Mandato del Cielo a menudo era invocado por filósofos y eruditos en Chína como una manera de reducir el abuso de poder por el jefe, en un sistema que tenía pocos otros frenos.
En particular, la dinastía duró un tiempo considerable durante el cual 31 reyes gobernaron durante un período prolongado de 17 generaciones.
Con el paso del tiempo, sin embargo, el abuso de los gobernantes de las otras clases sociales llevó a la agitación social y la inestabilidad.
Ellos crearon el Mandato del Cielo para explicar su derecho a asumir el gobierno y presumieron que la única manera de mantener el mandato era gobernar bien a los ojos del Cielo.
Sin embargo, para apaciguar a algunos de los ciudadanos, permitieron que algunos beneficiarios de Shang continuaran gobernando sus pequeños reinos de conformidad con las reglas y regulaciones de Zhou.
También se destacaron en la construcción naval, que, junto con su descubrimiento de la navegación celestial, los convirtió en excelentes navegantes.
La mayoría de estas obras son comentarios sobre el progreso y el movimiento político de la dinastía.
Sus obras enfatizaron principalmente la importancia de la clase dominante, el respeto y su relación con la clase baja.
Dentro de estos distritos había administradores que eran nombrados por el gobierno, a cambio, tenían que mantener su lealtad al gobierno interno principal.
Finalmente, cuando el poder de la dinastía Zhou disminuyó, fue borrado por el estado de Qin, que creyó que Zhou se había hecho débil y su regla injusta.
Durante esta reforma, los cambios administrativos se hicieron y un sistema del legalismo se desarrolló que declaró que la ley es suprema sobre cada individuo, incluso los jefes.
El establecimiento de la dinastía Han marcó un gran período en la historia de Chína marcado por cambios significativos en la estructura política del país.
Un propósito principal era establecer la justificación para la transferencia del Mandato del Cielo a través de estas cinco dinastías y, por lo tanto, a la dinastía Song.
También tenían considerablemente más territorio que cualquiera de los otros estados chinos que habían existido conjuntamente en el sur.
El comportamiento brutal de Zhu Wen y el posterior Liang fue una fuente de considerable bochorno, y por lo tanto hubo presión para excluirlos del Mandato.
Sin embargo, Kublai Khan era el único gobernante indiferente cuando reclamó el Mandato del Cielo sobre la dinastía Yuan, ya que tenía un ejército considerable y era parte del pueblo Khitan, como con muchos otros del mismo origen, ya que no tenían las mismas tradiciones y cultura que sus adversarios chinos.
Era únicamente política de principio a fin y un intento del emperador de mantener un acto favorable hacia el Cielo.
El derecho de rebelión no está codificado en ninguna ley oficial.
Dado que el ganador es el que determina quién ha obtenido el Mandato del Cielo y quién lo ha perdido, algunos eruditos chinos consideran que es una especie de justicia de Víctor, mejor caracterizada en el popular dicho chino "El ganador se convierte en rey, el perdedor se convierte en un proscrito".
También se dice que el reino de Silla se adoptó el Mandato del Cielo, pero los registros más tempranos son de la dinastía Joseon, que hizo el Mandato del Cielo una ideología estatal duradera.
Las dinastías vietnamitas posteriores y más centralizadas adoptaron Confucianism como la ideología estatal, que llevó a la creación de un sistema tributario vietnamita en Sudeste Asiático que se modeló después del sistema sinocéntrico chino en Asia Oriental.
En tiempos posteriores, esta necesidad fue obviada porque la Casa Imperial de Japón afirmó haber descendido en una línea ininterrumpida de la diosa japonesa del sol, Amaterasu.
Incluso después de la Restauración Meiji en 1868, cuando el emperador fue colocado de nuevo en el centro de la burocracia política, el trono en sí tenía muy poco poder frente a la oligarquía Meiji.
Los estudios de medios son una disciplina y campo de estudio que se ocupa del contenido, la historia y los efectos de varios medios; en particular, los medios de comunicación.
Los estudios de medios en Australia se desarrollaron por primera vez como un área de estudio en las universidades victorianas a principios de la década de 1960, y en las escuelas secundarias a mediados de la década de 1960.
En escuelas secundarias, un curso de estudios de la película temprano comenzó a enseñarse como la parte del plan de estudios secundario menor victoriano a mediados de los años 1960.
Desde entonces se ha convertido, y sigue siendo, un fuerte componente de la VCE.
Los estudios de medios no parecen impartirse en el estado de Nueva Gales del Sur en un nivel secundario.
Harold Innis y Marshall McLuhan son eruditos canadienses famosos por sus contribuciones a los campos de ecología de medios y economía política en el 20mo siglo.
La universidad de Carleton y la universidad de Ontario Occidental, 1945 y 1946 prospectivamente, crearon programas específicos del Periodismo o escuelas.
Hoy en día, la mayoría de las universidades ofrecen títulos de pregrado en Estudios de Medios y Comunicación, y muchos académicos canadienses contribuyen activamente al campo, entre los cuales: Brian Massumi (filosofía, estudios culturales), Kim Sawchuk (estudios culturales, feministas, estudios de envejecimiento), Carrie Rentschler (teoría feminista) y Francois Cooren (comunicación organizacional).
Un medio es cualquier cosa que media nuestra interacción con el mundo u otros seres humanos.
McLuhan dice que la "técnica de fragmentación que es la esencia de la tecnología de la máquina" dio forma a la reestructuración del trabajo humano y la asociación y "la esencia de la tecnología de automatización es lo contrario".
La característica de todos los medios significa que el "contenido" de cualquier medio es siempre otro medio.
Si la luz eléctrica se utiliza para el fútbol del viernes por la noche o para iluminar su escritorio, podría argumentar que el contenido de la luz eléctrica es estas actividades.
No es hasta que la luz eléctrica se utiliza para deletrear un nombre de marca que se reconoce como medio.
El efecto del medio se hace fuerte porque se le da otro "contenido" de los medios.
Los medios calientes son bajos en participación y los medios fríos son altos en participación.
Universidad de Comunicación de China, anteriormente conocido como el Instituto de Radiodifusión de Beijing, que se remonta a 1954.
El análisis de Bourdieu es que la televisión proporciona mucha menos autonomía, o libertad, de lo que pensamos.
Dentro del campo de los estudios cinematográficos, tanto Frankfurt como Berlín fueron dominantes en el desarrollo de nuevas perspectivas sobre los medios de imagen en movimiento.
Una de las primeras publicaciones en esta nueva dirección es un volumen editado por Helmut Kreuzer, Literature Studies - Media Studies (Literaturwissenschaft - Medienwissenschaft), que resume las presentaciones dadas en el D ?sseldorfer Germanistentag 1976.
El Instituto Alemán de Política de Medios y Comunicación, fundado en 2005 por el académico de medios Lutz Hachmeister, es una de las pocas instituciones de investigación independientes que se dedica a cuestiones relacionadas con las políticas de medios y comunicaciones.
Medienwissenschaften es actualmente uno de los cursos de estudio más populares en las universidades de Alemania, y muchos solicitantes asumen erróneamente que estudiarlo conducirá automáticamente a una carrera en televisión u otros medios.
Ofrece un programa integrado de cinco años y un programa de dos años en medios electrónicos.
Mientras que las ciencias de la comunicación se centran en la forma en que las personas se comunican, ya sea mediada o no mediada, los estudios de medios tienden a reducir la comunicación a la comunicación mediada.
Las ciencias de la comunicación (o un derivado de las mismas) se pueden estudiar en Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam y Wageningen University and Research Centre.
La Universidad de Punjab Lahore es el departamento más antiguo.
Los estudios de medios ahora se imparten en todo el Reino Unido.
Sin embargo, el enfoque de tales programas a veces excluye ciertos medios: películas, publicaciones de libros, videojuegos, etc.
Esto se debe en parte a la adquisición del profesor Siva Vaidhyanathan, historiador cultural y estudioso de los medios de comunicación, así como a la Conferencia Inaugural de Política y Ética de Medios de Verklin, dotada por el CEO de Canoe Ventures y el ex alumno de UVA David Verklin.
Un estudio de medios de comunicación en Radford todavía significa alguien que se concentra en el periodismo, la radiodifusión, la publicidad o la producción web.
Bergson contrastó una sociedad abierta con lo que llamó una sociedad cerrada, un sistema cerrado de ley, moralidad o religión.
Soros, George, "La edad de falibilidad", Asuntos públicos (2006).
El totalitarismo forzó a que el conocimiento se volviera político, lo que hizo imposible el pensamiento crítico y llevó a la destrucción del conocimiento en los países totalitarios.
En la sociedad cerrada, las afirmaciones de cierto conocimiento y la verdad última conducen al intento de imposición de una versión de la realidad.
Debido a que la percepción de la realidad del electorado puede manipularse fácilmente, el discurso político democrático no conduce necesariamente a una mejor comprensión de la realidad.
Popper, sin embargo, no identificó la sociedad abierta ni con la democracia ni con el capitalismo ni con una economía de laissez-faire, sino más bien con un estado de ánimo crítico por parte del individuo, frente a un grupo comunal que piensa de cualquier tipo.
Los colegios reguladores son entidades legales encargadas de servir al interés público mediante la regulación de la práctica de una profesión.
Por ejemplo, ningún trabajador en Ontario puede trabajar en un oficio obligatorio sin ser miembro del Colegio de Oficios de Ontario.
Para Weber, la sociología es el estudio de la sociedad y el comportamiento y, por lo tanto, debe mirar el corazón de la interacción.
El término es más práctico y abarcador que los "fenómenos sociales" de Florian Znaniecki, ya que el individuo que realiza la acción social no es pasivo, sino más bien activo y reactivo.
Esto también se considera un medio alternativo cuando las consecuencias secundarias han terminado.
Si el estudiante elige no hacerlo bien en la universidad, saben que será difícil ingresar a la escuela de leyes y, en última instancia, lograr el objetivo de ser un abogado.
La relación de valor se divide en los comandos y demandas de los subgrupos.
Estas demandas han planteado varios problemas, incluso el formalismo legal se ha puesto a prueba.
En la medida en que haya muchas empresas religiosas compitiendo entre sí, tenderán a especializarse y satisfacer las necesidades particulares de algunos segmentos de consumidores religiosos.
Es bien sabido que las iglesias estrictas son fuertes y están creciendo en los Estados Unidos contemporáneos, mientras que las liberales están disminuyendo.
Acción afectiva (también conocida como acciones emocionales): acciones que se toman debido a las emociones de uno, para expresar sentimientos personales.
En una reacción incontrolada no hay restricción y hay falta de discreción.
Cuando las aspiraciones no se cumplen, hay malestar interno.
Un ejemplo común son las suposiciones de elección conductuales y racionales.
Estos seis conceptos fueron identificados por Aristóteles y siguen siendo el tema de varias charlas.
Las teorías micrológicas de la economía consideran los actos de un grupo de individuos.
Al hacer esto, hace que los proveedores sean competitivos y, por lo tanto, crea orden en la economía.
La teoría de la elección racional, aunque cada vez más colonizada por economistas, difiere de las concepciones microeconómicas.
Acciones tradicionales: acciones que se llevan a cabo debido a la tradición, porque siempre se llevan a cabo de una manera particular para ciertas situaciones.
Una costumbre es una práctica que descansa entre la familiaridad.
Un hábito es una serie de pasos aprendidos gradualmente y a veces sin conciencia.
La idea del yo de Cooley es que nuestro sentido del yo se desarrolla a medida que observamos y reflexionamos sobre los demás y lo que pueden pensar de nuestras acciones.
El capital social es “las redes de relaciones entre las personas que viven y trabajan en una sociedad particular, permitiendo que esa sociedad funcione de manera efectiva”.
En la primera mitad del 19no siglo, de Tocqueville tenía observaciones sobre la vida americana que pareció perfilar y definir el capital social.
La comunidad en su conjunto se beneficiará de la cooperación de todas sus partes, mientras que el individuo encontrará en sus asociaciones las ventajas de la ayuda, la simpatía y el compañerismo de sus vecinos.
En palabras de Stein (1960:1): “El precio por mantener una sociedad que fomente la diferenciación cultural y la experimentación es, sin duda, la aceptación de una cierta cantidad de desorganización tanto a nivel individual como social”.
Todas estas reflexiones contribuyeron notablemente al desarrollo del concepto de capital social en las décadas siguientes.
Robert D. Putnam (1993) sugirió que el capital social facilitaría la cooperación y las relaciones de apoyo mutuo en las comunidades y naciones y, por lo tanto, sería un medio valioso para combatir muchos de los trastornos sociales inherentes a las sociedades modernas, por ejemplo, el crimen.
El concepto de Nan Lin de capital social tiene un enfoque más individualista: "Invertir en relaciones sociales con los rendimientos esperados en el mercado".
El término capital se usa por analogía con otras formas de capital económico, ya que se argumenta que el capital social tiene beneficios similares (aunque menos medibles).
Robison, Schmid y Siles (2002) revisaron varias definiciones de capital social y concluyeron que muchos no satisfacían el requisito formal de una definición.
Proponen que el capital social se defina como simpatía: el objeto de la simpatía de otro tiene capital social; aquellos que tienen simpatía por otros proporcionan capital social.
El capital social también se distingue de la teoría económica del capitalismo social.
“Crea valor para las personas que están conectadas, y también para los transeúntes”.
Según Robert D. Putnam, el capital social se refiere a "las conexiones entre individuos: las redes sociales y las normas de reciprocidad y confiabilidad que surgen de ellas".
Esto se ve en niveles más bajos de confianza en el gobierno y niveles más bajos de participación cívica.
Putnam también sugiere que una causa fundamental de la disminución del capital social es la entrada de las mujeres en la fuerza laboral, lo que podría correlacionarse con restricciones de tiempo que inhiben la participación de la organización cívica, como las asociaciones de padres y maestros.
Fukuyama sugiere que si bien el capital social es beneficioso para el desarrollo, también impone costos a los miembros que no pertenecen al grupo con consecuencias no deseadas para el bienestar general.
Esta dimensión se centra en las ventajas derivadas de la configuración de la red de un actor, ya sea individual o colectiva.
Esto se caracteriza mejor por la confianza de los demás y su cooperación y la identificación que un individuo tiene dentro de una red.
Las investigaciones de Sheri Berman y Dylan Riley, así como de los economistas Shanker Satyanath, Nico Voigtlander y Hans-Joachim Voth, han vinculado las asociaciones cívicas con el surgimiento de movimientos fascistas.
Las consecuencias negativas del capital social se asocian más a menudo con la vinculación frente a la vinculación.
Vincular y tender puentes entre el capital social puede funcionar de manera productiva si está en equilibrio, o pueden trabajar uno contra el otro.
El fortalecimiento de los lazos insulares puede conducir a una variedad de efectos, como la marginación étnica o el aislamiento social.
Los alemanes se lanzaron a sus clubes, asociaciones voluntarias y organizaciones profesionales por frustración con los fracasos del gobierno nacional y los partidos políticos, ayudando así a socavar la República de Weimar y facilitar el ascenso de Hitler al poder.
Eran muy introvertidos en la República de Weimar.
Robert Putnam, en su trabajo posterior, también sugiere que el capital social y el crecimiento asociado de la confianza pública son inhibidos por la inmigración y el aumento de la diversidad racial en las comunidades.
La falta de homogeneidad llevó a las personas a retirarse incluso de sus grupos y relaciones más cercanas, creando una sociedad atomizada en lugar de una comunidad cohesionada.
Se podía acceder al capital humano, un recurso privado, a través de lo que la generación anterior acumulaba a través del capital social.
Aunque Coleman nunca se dirija realmente a Pierre Bourdieu en su discusión, esto coincide con el argumento de Bourdieu expuesto en la Reproducción en Educación, Sociedad y Cultura.
Por lo tanto, es la plataforma social, en sí misma, la que lo equipa a uno con la realidad social a la que se acostumbran.
Para ilustrar esto, asumimos que un individuo desea mejorar su lugar en la sociedad.
¿Es la sociedad civil una teoría adecuada?
Los ejemplos típicos son que las bandas criminales crean un vínculo de capital social, mientras que los coros y los clubes de bolos (de ahí el título, como Putnam lamentó su declive) crean un puente de capital social.
Aldrich también aplica las ideas de capital social a los principios fundamentales de la recuperación ante desastres, y discute los factores que ayudan o impiden la recuperación, como el alcance del daño, la densidad de población, la calidad del gobierno y la ayuda.
Las personas que viven su vida de esta manera sienten que estas son normas de la sociedad y son capaces de vivir sus vidas libres de preocupaciones por su crédito, hijos, y recibir caridad si es necesario.
Para Marx, todas las formas de "capital" eran poseídas sólo por los capitalistas y él enfatizaba la base del trabajo en la sociedad capitalista, como una clase constituida por individuos obligados a vender su fuerza de trabajo, porque carecían de capital suficiente, en cualquier sentido de la palabra, para hacer lo contrario.
Portes menciona la donación de una beca a un miembro del mismo grupo étnico como un ejemplo de esto.
Se proponen subescalas de vinculación y puente, que han sido adoptadas por más de 300 artículos académicos.
Sin embargo, no hay una forma cuantitativa de determinar el nivel de cohesión, sino más bien una colección de modelos de redes sociales que los investigadores han utilizado durante décadas para operacionalizar el capital social.
Los grupos con mayor membresía (como los partidos políticos) contribuyen más a la cantidad de capital que los grupos con menor membresía, aunque muchos grupos con baja membresía (como las comunidades) todavía se suman para ser significativos.
La forma en que un grupo se relaciona con el resto de la sociedad también afecta al capital social, pero de una manera diferente.
Reconociendo que uno no puede ser capaz de influir en la simpatía de los demás, las personas que buscan pertenecer pueden actuar para aumentar su propia simpatía por los demás y las organizaciones o instituciones que representan.
Según autores como Walzer (1992), Alessandrini (2002), Newtown, Stolle & Rochon, Foley & Edwards (1997), y Walters, es a través de la sociedad civil, o más exactamente, el tercer sector, que los individuos son capaces de establecer y mantener redes relacionales.
No sólo se ha documentado que la sociedad civil produce fuentes de capital social, según el Tercer Sector de Lyon (2001), el capital social no aparece disfrazado bajo los factores que permiten o estimulan el crecimiento del tercer sector.
El objetivo es reintegrar a los marginados de las recompensas del sistema económico en "la comunidad".
Alessandrini está de acuerdo, diciendo que "en Australia en particular, el neoliberalismo ha sido reformulado como racionalismo económico e identificado por varios teóricos y comentaristas como un peligro para la sociedad en general debido al uso al que están poniendo capital social para trabajar".
En el desarrollo internacional, Ben Fine (2001) y John Harriss (2001) han criticado fuertemente la adopción inapropiada del capital social como una supuesta panacea (promoviendo organizaciones de la sociedad civil y ONG, por ejemplo, como agentes del desarrollo) para las desigualdades generadas por el desarrollo económico neoliberal.
Sin embargo, los niveles más altos de capital social llevaron a un mayor apoyo a la democracia.
La evaluación cuidadosa de estos factores fundamentales a menudo sugiere que las mujeres no votan en niveles similares a los hombres.
El capital social ofrece una gran cantidad de recursos y redes que facilitan el compromiso político.
Es más probable que las mujeres se organicen de manera menos jerárquica y se centren en crear consenso.
Por ejemplo, una persona que está enferma de cáncer puede recibir información, dinero o apoyo moral que necesita para soportar el tratamiento y recuperarse.
Además, el capital social de barrio también puede ayudar a amortiguar las desigualdades en salud entre niños y adolescentes.
Las relaciones y redes mantenidas por una población de minorías étnicas en un área geográfica donde un alto porcentaje de residentes pertenecen al mismo grupo étnico pueden conducir a mejores resultados de salud de lo que se esperaría en función de otras características individuales y vecinales.
Por ejemplo, los resultados de una encuesta realizada a estudiantes de 13 a 18 años en Suecia mostraron que el bajo capital social y la baja confianza social se asocian con mayores tasas de síntomas psicosomáticos, dolor musculoesquelético y depresión.
En un estudio, los usos informativos de Internet se correlacionaron positivamente con la producción de capital social de un individuo, y los usos social-recreativos se correlacionaron negativamente (niveles más altos de estos usos se correlacionaron con niveles más bajos de capital social).
Esto significa que los individuos pueden conectarse selectivamente con otros en función de intereses y antecedentes determinados.
Este argumento continúa, aunque la preponderancia de la evidencia muestra una asociación positiva entre el capital social e Internet.
Investigaciones recientes, realizadas en 2006, también muestran que los usuarios de Internet a menudo tienen redes más amplias que aquellos que acceden a Internet de manera irregular o no tienen acceso.
Otra investigación muestra que las personas más jóvenes usan Internet como un medio complementario para la comunicación, en lugar de dejar que la comunicación de Internet reemplace el contacto cara a cara.
Critican a Coleman, que utilizó solo el número de padres presentes en la familia, descuidaron el efecto invisible de dimensiones más discretas, como el de los padrastros y los diferentes tipos de familias monoparentales.
Morgan y Sorensen (1999) desafían directamente a Coleman por su falta de un mecanismo explícito para explicar por qué los estudiantes de las escuelas católicas se desempeñan mejor que los estudiantes de las escuelas públicas en las pruebas estandarizadas de rendimiento.
Se encuentra que mientras que el capital social puede producir un efecto positivo de mantener una comunidad funcional que abarca en las escuelas que hacen cumplir las normas, también produce la consecuencia negativa de la vigilancia excesiva.
Estas escuelas exploran un tipo diferente de capital social, como información sobre oportunidades en las redes sociales extendidas de padres y otros adultos.
La similitud de estos estados es que los padres estaban más asociados con la educación de sus hijos.
Sin capital social en el área de la educación, los maestros y los padres que juegan una responsabilidad en el aprendizaje de los estudiantes, los impactos significativos en el aprendizaje académico de sus hijos pueden depender de estos factores.
Como afirman Tedin y Weiher (2010), "uno de los factores más importantes para promover el éxito de los estudiantes es la participación activa de los padres en la educación de un niño".
Las redes de apoyo, como forma de capital social, son necesarias para activar el capital cultural que poseían los estudiantes recién llegados.
La solidaridad étnica es especialmente importante en el contexto en que los inmigrantes llegan a la sociedad de acogida.
El apoyo étnico proporciona ímpetu al éxito académico.
Su principal argumento para clasificar el capital social como un concepto geográfico es que las relaciones de las personas están formadas y moldeadas por las áreas en las que viven.
En sus estudios, no mira a los participantes individuales de estas estructuras, sino cómo las estructuras y las conexiones sociales que se derivan de ellas se difunden por el espacio.
Otra área donde el capital social puede verse como un área de estudio en geografía es a través del análisis de la participación en el voluntariado y su apoyo a diferentes gobiernos.
Existe una conexión significativa entre el ocio y el capital social democrático.
En un estudio posterior, Kislev (2020) muestra la relación entre el deseo de las relaciones románticas y la soltería.
Resultados similares fueron revelados en un estudio transversal llevado a cabo por Sarker en Bangladesh.
Epo hizo esto comparando los resultados de bienestar de los empresarios que tenían acceso y no tenían acceso.
La cohesión de grupo (también llamada cohesión de grupo y cohesión social) surge cuando los lazos vinculan a los miembros de un grupo social entre sí y con el grupo en su conjunto.
La cohesión se puede definir más específicamente como la tendencia de un grupo a estar en unidad mientras trabaja hacia una meta o para satisfacer las necesidades emocionales de sus miembros.
Su naturaleza dinámica se refiere a cómo cambia gradualmente con el tiempo en su fuerza y forma desde el momento en que se forma un grupo hasta cuando se disuelve un grupo.
Esta definición puede generalizarse a la mayoría de los grupos caracterizados por la definición de grupo analizada anteriormente.
En un estudio, pidieron a los miembros del grupo que identificaran a todos sus buenos amigos y calcularon la relación entre las opciones dentro del grupo y las opciones fuera del grupo.
La cohesión grupal es similar a un tipo de atracción a nivel de grupo que, según Hogg, se conoce como atracción social.
Lott y Lott (1965) que se refieren a la atracción interpersonal como cohesión de grupo realizaron una revisión extensa de la literatura y encontraron que las similitudes de los individuos en el fondo (por ejemplo, raza, etnia, ocupación, edad), actitudes, valores y rasgos de personalidad tienen una asociación generalmente positiva con la cohesión de grupo.
Además, antecedentes similares hacen que sea más probable que los miembros compartan puntos de vista similares sobre diversos temas, incluidos los objetivos del grupo, los métodos de comunicación y el tipo de liderazgo deseado.
Esto a menudo es causado por la holgazanería social, una teoría que dice que los miembros individuales de un grupo en realidad pondrán menos esfuerzo, porque creen que otros miembros compensarán la holgura.
La mayoría de los metanálisis (estudios que han resumido los resultados de muchos estudios) han demostrado que existe una relación entre cohesión y rendimiento.
Cuando se define como compromiso de tarea, también se correlaciona con el rendimiento, aunque en menor medida que la cohesión como atracción.
Sin embargo, algunos grupos pueden tener una relación cohesión-rendimiento más fuerte que otros.
Existe cierta evidencia de que la cohesión puede estar más fuertemente relacionada con el desempeño de los grupos que tienen roles altamente interdependientes que para los grupos en los que los miembros son independientes.
Además, los grupos con objetivos de alto rendimiento eran extremadamente productivos.
Los miembros de los grupos cohesivos también son más optimistas y sufren menos problemas sociales que los de los grupos no cohesivos.
Se encontró que los albañiles y carpinteros estaban más satisfechos cuando trabajaban en grupos cohesivos.
Un estudio mostró que la cohesión como compromiso de la tarea puede mejorar la toma de decisiones del grupo cuando el grupo está bajo estrés, más que cuando no está bajo estrés.
El estudio encontró que los equipos con baja cohesión y alta urgencia obtuvieron peores resultados que los equipos con alta cohesión y alta urgencia.
La teoría del pensamiento grupal sugiere que las presiones impiden que el grupo piense críticamente sobre las decisiones que está tomando.
Otra razón es porque las personas valoran al grupo y, por lo tanto, están más dispuestas a ceder a las presiones de conformidad para mantener o mejorar sus relaciones.
Se supuso que el grado de gusto de los miembros indicaba cohesión de grupo.
Según los informes temáticos del Estado de las Ciudades Inglesas, hay cinco dimensiones diferentes de cohesión social: condiciones materiales, relaciones pasivas, relaciones activas, solidaridad, inclusión e igualdad.
Estas necesidades básicas de la vida son los cimientos de un tejido social fuerte e indicadores importantes del progreso social.
La tercera dimensión se refiere a las interacciones positivas, intercambios y redes entre individuos y comunidades, o "relaciones sociales activas".
También incluye el sentido de pertenencia de las personas a una ciudad y la fuerza de experiencias, identidades y valores compartidos entre personas de diferentes orígenes.
A nivel social, Albrekt Larsen define la cohesión social como “la creencia, sostenida por los ciudadanos en un estado nación determinado, de que comparten una comunidad moral, que les permite confiar el uno en el otro”.
La formación social es un concepto marxista (sinónimo de «sociedad») que se refiere a la articulación concreta e histórica entre el modo de producción capitalista, el mantenimiento de los modos de producción precapitalistas, y el contexto institucional de la economía (desambiguación).
En las ciencias sociales, la estructura social es el patrón de arreglos sociales en la sociedad que son a la vez emergentes y determinantes de las acciones de los individuos.
Contrasta con el "sistema social", que se refiere a la estructura parental en la que están incrustadas estas diversas estructuras.
Determina las normas y los patrones de las relaciones entre las diversas instituciones de la sociedad.
También es importante en el estudio moderno de las organizaciones, ya que la estructura de una organización puede determinar su flexibilidad, capacidad de cambio, etc.
En la escala meso, se refiere a la estructura de las redes sociales entre individuos u organizaciones.
Por ejemplo, John Levi Martin ha teorizado que ciertas estructuras a macroescala son las propiedades emergentes de las instituciones culturales a microescala (es decir, la "estructura" se asemeja a la utilizada por el antropólogo Claude Levi-Strauss).
Alexis de Tocqueville fue supuestamente el primero en usar el término "estructura social".
Una de las cuentas más tempranas y más completas de la estructura social fue proporcionada por Karl Marx, que relacionó la vida política, cultural y religiosa con el modo de producción (una estructura económica subyacente).
Émile Durkheim, basándose en las analogías entre los sistemas biológicos y sociales popularizados por Herbert Spencer y otros, introdujo la idea de que diversas instituciones y prácticas sociales desempeñaron un papel en asegurar la integración funcional de la sociedad a través de la asimilación de diversas partes en un todo unificado y autorreproductor.
Otros siguen a Lévi-Strauss en la búsqueda de un orden lógico en las estructuras culturales.
Los intentos más influyentes de combinar el concepto de estructura social con la agencia son la teoría de la estructuración de Anthony Giddens y la teoría de la práctica de Pierre Bourdieu.
El análisis de Giddens, en este sentido, es muy similar a la deconstrucción de Jacques Derrida de los binarios que subyacen al razonamiento sociológico y antropológico clásico (en particular las tendencias universalizadoras del estructuralismo de Lévi-Strauss).
Esto fue estudiado por Jacob L. Moreno.
La sociobiología es un campo de la biología que tiene como objetivo examinar y explicar el comportamiento social en términos de evolución.
La sociobiología investiga comportamientos sociales como los patrones de apareamiento, las peleas territoriales, la caza de manadas y la sociedad de colmenas de insectos sociales.
Predice que los animales actuarán de maneras que han demostrado ser evolutivamente exitosas con el tiempo.
Por lo tanto, el comportamiento se ve como un esfuerzo por preservar los genes en la población.
Altmann desarrolló su propia marca de sociobiología para estudiar el comportamiento social de los macacos rhesus, utilizando estadísticas, y fue contratado como "sociobiólogo" en el Centro Regional de Investigación de Primates de Yerkes en 1965.
Una vez que un término del especialista, "sociobiología" se hizo extensamente conocido en 1975 cuando Wilson publicó su libro Sociobiología: La Nueva Síntesis, que provocó una controversia intensa.
Sin embargo, la influencia de la evolución en el comportamiento ha sido de interés para los biólogos y filósofos desde poco después del descubrimiento de la evolución misma.
Edward H. Hagen escribe en The Handbook of Evolutionary Psychology que la sociobiología es, a pesar de la controversia pública sobre las aplicaciones a los humanos, "uno de los triunfos científicos del siglo XX".
Por lo tanto, estos rasgos fueron probablemente "adaptativos" en el entorno en el que evolucionó la especie.
Por lo tanto, a menudo están interesados en el comportamiento instintivo o intuitivo, y en explicar las similitudes, en lugar de las diferencias, entre las culturas.
Esta protección parental aumentaría en frecuencia en la población.
E.O. Wilson argumentó que la evolución también puede actuar sobre los grupos.
Si el altruismo está determinado genéticamente, entonces los individuos altruistas deben reproducir sus propios rasgos genéticos altruistas para que el altruismo sobreviva, pero cuando los altruistas prodigan sus recursos a los no altruistas a expensas de su propia especie, los altruistas tienden a morir y los demás tienden a aumentar.
Dentro de la sociobiología, un comportamiento social se explica primero como una hipótesis sociobiológica al encontrar una estrategia evolutivamente estable que coincida con el comportamiento observado.
El altruismo entre insectos sociales y compañeros de camada se ha explicado de esa manera.
En general, las hembras con más oportunidades de porte pueden valorar menos la descendencia, y también pueden organizar oportunidades de porte para maximizar la comida y la protección de las parejas.
Los estudios de la genética del comportamiento humano generalmente han encontrado que los rasgos de comportamiento como la creatividad, la extroversión, la agresividad y el coeficiente intelectual tienen una alta heredabilidad.
Por lo tanto, cuando el VEF se elimina genéticamente del genoma del ratón, los ratones machos atacarán instantáneamente a otros machos, mientras que sus contrapartes de tipo salvaje tardan significativamente más en iniciar un comportamiento violento.
Durante una reunión de 1976 del Grupo de Estudio de Sociobiología, según lo informado por Ullica Segerstr?le, Chomsky abogó por la importancia de una noción sociobiológicamente informada de la naturaleza humana.
Wilson ha afirmado que nunca había tenido la intención de implicar lo que debería ser, solo lo que es el caso.
El negocio es la actividad de ganarse la vida o ganar dinero produciendo o comprando y vendiendo productos (como bienes y servicios).
Si el negocio adquiere deudas, los acreedores pueden ir tras las posesiones personales del propietario.
El término también se usa coloquialmente (pero no por abogados o funcionarios públicos) para referirse a una empresa.
Una corporación privada con fines de lucro es propiedad de sus accionistas, que eligen una junta directiva para dirigir la corporación y contratar a su personal directivo.
Una cooperativa difiere de una corporación en que tiene miembros, no accionistas, y comparten la autoridad de tomar decisiones.
Las sociedades de responsabilidad limitada (LLC), las sociedades de responsabilidad limitada y otros tipos específicos de organizaciones empresariales protegen a sus propietarios o accionistas del fracaso empresarial al hacer negocios bajo una entidad legal separada con ciertas protecciones legales.
Los miembros garantizan el pago de ciertas cantidades (generalmente nominales) si la empresa entra en liquidación insolvente, pero de lo contrario, no tienen derechos económicos en relación con la empresa.
Este tipo de empresa ya no se puede formar en el Reino Unido, aunque todavía existen disposiciones legales para que existan.
Tenga en cuenta que "Ltd después del nombre de la empresa significa sociedad limitada, y PLC (sociedad pública limitada) indica que sus acciones son ampliamente mantenidos."
En una empresa limitada por garantía, estos serán los garantes.
Las empresas privadas no tienen acciones que cotizan en bolsa y, a menudo, contienen restricciones a las transferencias de acciones.
Las compañías de entretenimiento y las agencias de medios de comunicación generan ganancias principalmente de la venta de propiedad intelectual.
Incluyen bienes tangibles como automóviles, autobuses, dispositivos médicos, vidrio o aviones.
La mayoría de las tiendas y empresas de catálogo son distribuidores o minoristas.
Obtienen sus ganancias vendiendo bienes y servicios relacionados con el deporte.
El campo moderno fue establecido por el matemático italiano Luca Pacioli en 1494.
Las finanzas también se pueden definir como la ciencia de la administración del dinero.
Los propietarios pueden administrar sus negocios ellos mismos, o emplear gerentes para hacerlo por ellos.
La gestión de procesos de negocio (BPM) es un enfoque de gestión integral centrado en alinear todos los aspectos de una organización con los deseos y necesidades de los clientes.
Muchas empresas operan a través de una entidad separada, como una corporación o una sociedad (ya sea formada con o sin responsabilidad limitada).
En términos generales, los accionistas de una corporación, los socios limitados en una sociedad de responsabilidad limitada y los miembros de una sociedad de responsabilidad limitada están protegidos de la responsabilidad personal por las deudas y obligaciones de la entidad, que se trata legalmente como una "persona" separada.
Los términos de una asociación se rigen en parte por un acuerdo de asociación si se crea uno, y en parte por la ley de la jurisdicción donde se encuentra la asociación.
En algunos sistemas fiscales, esto puede dar lugar a la llamada doble imposición, porque primero la corporación paga impuestos sobre la ganancia, y luego cuando la corporación distribuye sus ganancias a sus propietarios, los individuos tienen que incluir dividendos en sus ingresos cuando completan sus declaraciones de impuestos personales, momento en el cual se impone una segunda capa de impuesto sobre la renta.
"Hacerse público" a través de un proceso conocido como oferta pública inicial (IPO) significa que parte del negocio será propiedad de miembros del público.
El Código de Hammurabi se remonta a alrededor de 1772 aC, por ejemplo, y contiene disposiciones que se relacionan, entre otros asuntos, con los costos de envío y las transacciones entre comerciantes y corredores.
Las jurisdicciones locales también pueden requerir licencias e impuestos especiales solo para operar un negocio.
La mayoría de los países con mercados de capitales tienen al menos uno.
Otras naciones occidentales tienen organismos reguladores comparables.
La proliferación y la creciente complejidad de las leyes que rigen los negocios han obligado a una creciente especialización en derecho corporativo.
La mayoría de las empresas tienen nombres, logotipos y técnicas de marca similares que podrían beneficiarse de las marcas registradas.
La economía es la ciencia social que estudia cómo las personas interactúan con el valor; en particular, la producción, distribución y consumo de bienes y servicios.
Afirmó que los economistas anteriores generalmente han centrado sus estudios en el análisis de la riqueza: cómo se crea (producción), distribuye y consume la riqueza; y cómo la riqueza puede crecer.
Si la guerra no se puede ganar o si los costos esperados superan los beneficios, los actores decisivos (suponiendo que sean racionales) nunca pueden ir a la guerra (una decisión), sino más bien explorar otras alternativas.
Los preceptos económicos ocurren en todas partes de los escritos del poeta de Beotian Hesiod y varios historiadores económicos han descrito a Hesiod como el "primer economista".
Dos grupos, que más tarde se llamaron "mercantilistas" y "fisiócratas", influyeron más directamente en el desarrollo posterior del sujeto.
Sostuvo que la riqueza de una nación dependía de su acumulación de oro y plata.
Los fisiócratas, un grupo de pensadores y escritores franceses del siglo XVIII, desarrollaron la idea de la economía como un flujo circular de ingresos y producción.
Los fisiócratas abogaron por reemplazar las recaudaciones de impuestos administrativamente costosas con un solo impuesto sobre los ingresos de los propietarios de tierras.
Smith analiza los beneficios potenciales de la especialización por división del trabajo, incluido el aumento de la productividad laboral y los beneficios del comercio, ya sea entre ciudades y países o entre países.
La fuerza de una población en rápido crecimiento contra una cantidad limitada de tierra significó la disminución de los retornos al trabajo.
Mientras Adam Smith hizo hincapié en la producción de ingresos, David Ricardo (1817) se centró en la distribución de los ingresos entre los terratenientes, los trabajadores y los capitalistas.
Ricardo fue el primero en afirmar y probar el principio de ventaja comparativa, según el cual cada país debe especializarse en producir y exportar bienes en el sentido de que tiene un menor costo relativo de producción, en lugar de depender solo de su propia producción.
Mill señaló una clara diferencia entre los dos roles del mercado: asignación de recursos y distribución de ingresos.
Smith escribió que "el precio real de cada cosa... es el trabajo duro y la molestia de adquirirlo".
La definición de Say ha prevalecido hasta nuestro tiempo, al sustituir la palabra "riqueza" por "bienes y servicios", lo que significa que la riqueza también puede incluir objetos no materiales.
Para Robbins, la insuficiencia se resolvió, y su definición nos permite proclamar, con una conciencia tranquila, economía de la educación, economía de la seguridad y la seguridad, economía de la salud, economía de guerra, y por supuesto, economía de la producción, distribución y consumo como sujetos válidos de la ciencia económica.
Aunque lejos de ser unánimes, la mayoría de los economistas aceptarían alguna versión de la definición de Robbins, a pesar de que muchos han planteado serias objeciones al alcance y el método de la economía, que emana de esa definición.
El término "economía" fue popularizado por economistas neoclásicos como Alfred Marshall como un sinónimo conciso de "ciencia económica" y un sustituto de la "economía política" anterior.
Se prescindió de la teoría del valor laboral heredada de la economía clásica en favor de una teoría de la utilidad marginal del valor en el lado de la demanda y una teoría más general de los costos en el lado de la oferta.
Un ejemplo inmediato de esto es la teoría del consumidor de la demanda individual, que aísla cómo los precios (como costos) y los ingresos afectan la cantidad demandada.
La economía convencional moderna se basa en la economía neoclásica, pero con muchos refinamientos que complementan o generalizan el análisis anterior, como la econometría, la teoría de juegos, el análisis del fracaso del mercado y la competencia imperfecta, y el modelo neoclásico de crecimiento económico para analizar variables a largo plazo que afectan el ingreso nacional.
Existe un problema económico, sujeto al estudio de la ciencia económica, cuando una decisión (elección) es tomada por uno o más actores que controlan los recursos para lograr el mejor resultado posible en condiciones racionales limitadas.
El libro se centró en los determinantes del ingreso nacional en el corto plazo cuando los precios son relativamente inflexibles.
La economía keynesiana tiene dos sucesores.
Se asocia generalmente con la Universidad de Cambridge y el trabajo de Joan Robinson.
Ben Bernanke, ex presidente de la Reserva Federal, es uno de los economistas de hoy que generalmente acepta el análisis de Friedman de las causas de la Gran Depresión.
Al crear teorías, el objetivo es encontrar las que son al menos tan simples en los requisitos de información, más precisas en las predicciones y más fructíferas en la generación de investigaciones adicionales que las teorías anteriores.
Los primeros modelos macroeconómicos se centraron en modelar las relaciones entre las variables agregadas, pero a medida que las relaciones parecían cambiar con el tiempo, los macroeconomistas, incluidos los nuevos keynesianos, reformularon sus modelos en microfundaciones.
A veces una hipótesis económica es sólo cualitativa, no cuantitativa.
Sin embargo, el campo de la economía experimental está creciendo, y se está haciendo un uso cada vez mayor de experimentos naturales.
Por tales medios, una hipótesis puede ganar la aceptación, aunque en un probabilístico, más bien que seguro, sentido.
Las críticas basadas en estándares profesionales y la no replicabilidad de los resultados sirven como controles adicionales contra el sesgo, los errores y la generalización excesiva, aunque se ha acusado a muchas investigaciones económicas de no ser replicables, y se ha acusado a revistas prestigiosas de no facilitar la replicación a través de la provisión del código y los datos.
En economía aplicada, los modelos input-output que emplean métodos de programación lineal son bastante comunes.
Esto ha reducido la distinción largamente señalada de economía de ciencias naturales porque permite pruebas directas de lo que antes se tomaba como axiomas.
Pruebas empíricas similares ocurren en neuroeconomía.
En mercados perfectamente competitivos, ningún participante es lo suficientemente grande como para tener el poder de mercado para establecer el precio de un producto homogéneo.
La microeconomía estudia los mercados individuales simplificando el sistema económico asumiendo que la actividad en el mercado que se analiza no afecta a otros mercados.
La teoría del equilibrio general estudia varios mercados y su comportamiento.
Hay que elegir entre acciones deseables pero mutuamente excluyentes.
Parte del costo de hacer pretzels es que ni la harina ni la mañana ya están disponibles, para su uso de alguna otra manera.
Los insumos utilizados en el proceso de producción incluyen factores primarios de producción como los servicios laborales, el capital (bienes duraderos producidos utilizados en la producción, como una fábrica existente) y la tierra (incluidos los recursos naturales).
La eficiencia se mejora si se genera más salida sin cambiar las entradas, o en otras palabras, se reduce la cantidad de "desperdicio".
En el caso más simple, una economía puede producir solo dos bienes (por ejemplo, "armas" y "mantequilla").
La escasez está representada en la figura por personas que están dispuestas pero no pueden en el agregado consumir más allá de la PPF (como en X) y por la pendiente negativa de la curva.
La pendiente de la curva en un punto sobre ella da la compensación entre los dos bienes.
A lo largo del PPF, la escasez implica que elegir más de un bien en conjunto implica hacer con menos del otro bien.
Un punto dentro de la curva (como en A), es factible pero representa la ineficiencia de producción (uso derrochador de entradas), en que la salida de uno o ambos bienes podría aumentar moviéndose en una dirección noreste a un punto en la curva.
Se ha observado que se produce un gran volumen de comercio entre las regiones, incluso con acceso a una tecnología similar y una combinación de factores, incluidos los países de altos ingresos.
Entre cada uno de estos sistemas de producción, puede haber una división del trabajo correspondiente con diferentes grupos de trabajo especializados, o correspondientemente diferentes tipos de equipos de capital y usos diferenciados de la tierra.
La teoría y la observación establecen las condiciones para que los precios de mercado de los productos y los insumos productivos seleccionen una asignación de insumos de factores por ventaja comparativa, de modo que los insumos (relativamente) de bajo costo se destinen a producir productos de bajo costo.
En microeconomía, se aplica a la determinación de precios y producción para un mercado con competencia perfecta, que incluye la condición de que no haya compradores o vendedores lo suficientemente grandes como para tener poder de fijación de precios.
La teoría de la demanda describe a los consumidores individuales como la elección racional de la cantidad más preferida de cada bien, dado el ingreso, los precios, los gustos, etc.
La ley de la demanda establece que, en general, el precio y la cantidad demandada en un mercado determinado están inversamente relacionados.
Además, el poder adquisitivo de la caída de los precios aumenta la capacidad de compra (el efecto ingreso).
La oferta es la relación entre el precio de un bien y la cantidad disponible para la venta a ese precio.
La oferta se representa típicamente como una función que relaciona el precio y la cantidad, si otros factores no cambian.
Al igual que en el lado de la demanda, la posición de la oferta puede cambiar, por ejemplo, de un cambio en el precio de un insumo productivo o una mejora técnica.
El equilibrio del mercado ocurre cuando la cantidad suministrada es igual a la cantidad demandada, la intersección de las curvas de oferta y demanda en la figura anterior.
A un precio por encima del equilibrio, hay un excedente de cantidad ofrecida en comparación con la cantidad demandada.
Los tipos más obvios de empresas son corporaciones, asociaciones y fideicomisos.
En los mercados perfectamente competitivos estudiados en la teoría de la oferta y la demanda, hay muchos productores, ninguno de los cuales influye significativamente en el precio.
Las estructuras de mercado comunes estudiadas además de la competencia perfecta incluyen la competencia monopolística, varias formas de oligopolio y monopolio.
Dadas sus diferentes formas, existen diversas formas de representar la incertidumbre y modelar las respuestas de los agentes económicos a ella.
En economía conductual, se ha utilizado para modelar las estrategias que los agentes eligen al interactuar con otros cuyos intereses son al menos parcialmente adversos a los suyos.
Tiene aplicaciones significativas aparentemente fuera de la economía en temas tan diversos como la formulación de estrategias nucleares, la ética, la ciencia política y la biología evolutiva.
También analiza la fijación de precios de los instrumentos financieros, la estructura financiera de las empresas, la eficiencia y la fragilidad de los mercados financieros, las crisis financieras y las políticas o regulaciones gubernamentales relacionadas.
Los clientes sin conocimiento de si un coche es un "limón" rebajan su precio por debajo de lo que sería un coche de segunda mano de calidad.
Ambos problemas pueden aumentar los costos de seguro y reducir la eficiencia al expulsar del mercado a los transactores dispuestos de otro modo ("mercados incompletos").
Las asimetrías de información y los mercados incompletos pueden resultar en ineficiencia económica, pero también en una posibilidad de mejorar la eficiencia a través de remedios legales, regulatorios y de mercado, como se discutió anteriormente.
Los bienes públicos son bienes que están insuficientemente suministrados en un mercado típico.
Por ejemplo, la contaminación del aire puede generar una externalidad negativa, y la educación puede generar una externalidad positiva (menos delincuencia, etc.).
En muchas áreas, se postula alguna forma de rigidez de precios para tener en cuenta las cantidades, en lugar de los precios, ajustándose a corto plazo a los cambios en el lado de la demanda o en el lado de la oferta.
Ejemplos de esta rigidez de precios en mercados particulares incluyen las tasas salariales en los mercados laborales y los precios publicados en los mercados que se desvían de la competencia perfecta.
Tales agregados incluyen el ingreso y la producción nacional, la tasa de desempleo y la inflación de precios y subagregados como el consumo total y el gasto en inversión y sus componentes.
Esto ha abordado una preocupación de larga data sobre los desarrollos inconsistentes del mismo tema.
Keynes sostuvo que la demanda agregada de bienes podría ser insuficiente durante las recesiones económicas, lo que llevaría a un desempleo innecesariamente alto y a pérdidas de producción potencial.
La nueva macroeconomía clásica, a diferencia de la visión keynesiana del ciclo económico, postula la compensación del mercado con información imperfecta.
La fuerza laboral solo incluye a los trabajadores que buscan trabajo activamente.
Los modelos clásicos de desempleo ocurren cuando los salarios son demasiado altos para que los empleadores estén dispuestos a contratar más trabajadores.
Grandes cantidades de desempleo estructural pueden ocurrir cuando una economía está en transición industrias y los trabajadores encuentran su conjunto anterior de habilidades ya no están en demanda.
El dinero tiene aceptabilidad general, consistencia relativa en el valor, divisibilidad, durabilidad, portabilidad, elasticidad en la oferta y longevidad con confianza pública masiva.
En palabras de Francis Amasa Walker, un conocido economista del siglo XIX, "El dinero es lo que hace el dinero" ("El dinero es lo que hace el dinero" en el original).
Su función económica se puede contrastar con el trueque (intercambio no monetario).
Cuando la demanda agregada cae por debajo de la producción potencial de la economía, hay una brecha de producción donde parte de la capacidad productiva queda desempleada.
Por ejemplo, los constructores de viviendas desempleados pueden ser contratados para expandir las carreteras.
Los efectos de la política fiscal pueden verse limitados por el desplazamiento.
Algunos economistas piensan que el desplazamiento es siempre un problema, mientras que otros no piensan que es un problema importante cuando la producción está deprimida.
Este último, un aspecto de la teoría de la elección pública, modela el comportamiento del sector público de manera análoga a la microeconomía, involucrando interacciones de votantes, políticos y burócratas interesados.
También se refiere al tamaño y la distribución de las ganancias del comercio.
A menudo se afirma que Carlyle le dio a la economía el apodo de "la ciencia sombría" como respuesta a los escritos de finales del siglo XVIII del reverendo Thomas Robert Malthus, quien predijo sombríamente que se produciría el hambre, ya que el crecimiento proyectado de la población excedió la tasa de aumento en el suministro de alimentos.
La estrecha relación de la teoría y la práctica económica con la política es un foco de contención que puede sombrear o distorsionar los principios originales más modestos de la economía, y a menudo se confunde con agendas sociales específicas y sistemas de valores.
Algunas revistas económicas académicas han aumentado sus esfuerzos para medir el consenso de los economistas con respecto a ciertas cuestiones de política con la esperanza de lograr un entorno político más informado.
Temas como la independencia del banco central, las políticas del banco central y la retórica en el discurso de los gobernadores de los bancos centrales o las premisas de las políticas macroeconómicas (política monetaria y fiscal) del estado, son foco de contención y crítica.
El campo de la economía de la información incluye tanto la investigación matemático-económica como la economía del comportamiento, similar a los estudios en psicología del comportamiento, y los factores de confusión de los supuestos neoclásicos son objeto de un estudio sustancial en muchas áreas de la economía.
Joskow tenía una fuerte sensación de que el importante trabajo en el oligopolio se realizaba a través de observaciones informales, mientras que los modelos formales se "desplegaban ex post".
Otro tema importante es la evolución, que explica la unidad y la diversidad de la vida.
Sus obras como Historia de los animales fueron especialmente importantes porque revelaron sus inclinaciones naturalistas, y más tarde trabajos empíricos que se centraron en la causalidad biológica y la diversidad de la vida.
La medicina fue especialmente bien estudiada por los eruditos islámicos que trabajan en las tradiciones filosóficas griegas, mientras que la historia natural se basó en gran medida en el pensamiento aristotélico, especialmente en la defensa de una jerarquía fija de la vida.
Las investigaciones de Jan Swammerdam condujeron a un nuevo interés en la entomología y ayudaron a desarrollar las técnicas básicas de disección microscópica y tinción.
Luego, en 1838, Schleiden y Schwann comenzaron a promover las ideas ahora universales de que (1) la unidad básica de los organismos es la célula y (2) que las células individuales tienen todas las características de la vida, aunque se opusieron a la idea de que (3) todas las células provienen de la división de otras células.
Carl Linnaeus publicó una taxonomía básica para el mundo natural en 1735 (variaciones de las cuales han estado en uso desde entonces), y en la década de 1750 introdujo nombres científicos para todas sus especies.
Lamarck creía que estos rasgos adquiridos podrían transmitirse a la descendencia del animal, que los desarrollaría y perfeccionaría aún más.
La base para la genética moderna comenzó con el trabajo de Gregor Mendel, quien presentó su artículo, "Versuche Ober Pflanzenhybriden" ("Experimentos sobre la hibridación de plantas"), en 1865, que describía los principios de la herencia biológica, sirviendo como base para la genética moderna.
Un enfoque en nuevos tipos de organismos modelo como virus y bacterias, junto con el descubrimiento de la estructura de doble hélice del ADN por James Watson y Francis Crick en 1953, marcó la transición a la era de la genética molecular.
Finalmente, el Proyecto Genoma Humano se lanzó en 1990 con el objetivo de mapear el genoma humano en general.
La vida en la Tierra comenzó a partir del agua y permaneció allí durante unos tres mil millones de años antes de migrar a la tierra.
El núcleo está formado por uno o más protones y un número de neutrones.
El átomo de cada elemento específico contiene un número único de protones, que se conoce como su número atómico, y la suma de sus protones y neutrones es el número de masa de un átomo.
El carbono, por ejemplo, puede existir como un isótopo estable (carbono-12 o carbono-13) o como un isótopo radiactivo (carbono-14), el último de los cuales puede usarse en la datación radiométrica (específicamente la datación por radiocarbono) para determinar la edad de los materiales orgánicos.
El enlace iónico implica la atracción electrostática entre iones con carga opuesta, o entre dos átomos con electronegatividades muy diferentes, y es la interacción primaria que ocurre en los compuestos iónicos.
A diferencia de los enlaces iónicos, un enlace covalente implica el intercambio de pares de electrones entre átomos.
Un ejemplo ubicuo de un enlace de hidrógeno se encuentra entre las moléculas de agua.
El agua es importante para la vida porque es un disolvente eficaz, capaz de disolver solutos tales como iones de sodio y cloruro u otras moléculas pequeñas para formar una solución acuosa.
Debido a que los enlaces O-H son polares, el átomo de oxígeno tiene una ligera carga negativa y los dos átomos de hidrógeno tienen una ligera carga positiva.
El agua también es adhesiva, ya que es capaz de adherirse a la superficie de cualquier molécula polar o cargada que no sea agua.
La menor densidad de hielo en comparación con el agua líquida se debe al menor número de moléculas de agua que forman la estructura de la red cristalina del hielo, lo que deja una gran cantidad de espacio entre las moléculas de agua.
Por lo tanto, se necesita una gran cantidad de energía para romper los enlaces de hidrógeno entre las moléculas de agua para convertir el agua líquida en gas (o vapor de agua).
Con la excepción del agua, casi todas las moléculas que componen cada organismo contienen carbono.
Por ejemplo, un solo átomo de carbono puede formar cuatro enlaces covalentes simples tales como en metano, dos enlaces covalentes dobles tales como en dióxido de carbono, o un enlace covalente triple tal como en monóxido de carbono (CO).
Una cadena principal hidrocarbonada puede sustituirse por otros elementos tales como oxígeno (O), hidrógeno (H), fósforo (P) y azufre (S), que pueden cambiar el comportamiento químico de ese compuesto.
Cuando dos monosacáridos como la glucosa y la fructosa se unen entre sí, pueden formar un disacárido como la sacarosa.
Estos lípidos son compuestos orgánicos que son en gran medida no polares e hidrófobos.
El grupo glicerol y fosfato juntos constituyen la región polar e hidrófila (o cabeza) de la molécula, mientras que los ácidos grasos constituyen la región no polar e hidrófoba (o cola).
Las proteínas son las más diversas de las macromoléculas, que incluyen enzimas, proteínas de transporte, moléculas de señalización grandes, anticuerpos y proteínas estructurales.
La polaridad y la carga de las cadenas laterales afectan la solubilidad de los aminoácidos.
La estructura primaria consiste en una secuencia única de aminoácidos que están unidos covalentemente entre sí por enlaces peptídicos.
El plegamiento de hélices alfa y láminas beta da a una proteína su estructura tridimensional o terciaria.
Las purinas incluyen guanina (G) y adenina (A), mientras que las pirimidinas consisten en citosina (T), uracilo (U) y timina (T).
Una membrana celular consiste en una bicapa lipídica, incluidos los colesteroles que se encuentran entre los fosfolípidos para mantener su fluidez a diversas temperaturas.
Las membranas celulares están implicadas en diversos procesos celulares tales como adhesión celular, almacenamiento de energía eléctrica y señalización celular y sirven como superficie de unión para varias estructuras extracelulares tales como una pared celular, glicocáliz y citoesqueleto.
El texto de Alberts discute cómo los "bloques de construcción celulares" se mueven para dar forma a los embriones en desarrollo.
Las células vegetales tienen orgánulos adicionales que las distinguen de las células animales, como una pared celular que proporciona soporte para la célula vegetal, cloroplastos que recogen la energía de la luz solar para producir azúcar y vacuolas que proporcionan almacenamiento y soporte estructural, además de estar involucradas en la reproducción y descomposición de las semillas de las plantas.
Según la primera ley de la termodinámica, la energía se conserva, es decir, no se puede crear o destruir.
Como resultado, un organismo requiere una entrada continua de energía para mantener un bajo estado de entropía.
Por lo general, el catabolismo libera energía, y el anabolismo consume energía.
La reacción general se produce en una serie de pasos bioquímicos, algunos de los cuales son reacciones redox.
Acetil-Coa entra en el ciclo del ácido cítrico, que tiene lugar dentro de la matriz mitocondrial.
La fosforilación oxidativa comprende la cadena de transporte de electrones, que es una serie de cuatro complejos de proteínas que transfieren electrones de un complejo a otro, liberando así energía de NADH y FADH2 que está acoplada al bombeo de protones (iones de hidrógeno) a través de la membrana mitocondrial interna (quimiosmosis), que genera una fuerza motriz de protones.
Si el oxígeno no estuviera presente, el piruvato no sería metabolizado por la respiración celular, sino que sufriría un proceso de fermentación.
La fermentación oxida NADH a NAD + para que pueda ser reutilizado en la glucólisis.
En los músculos esqueléticos, el producto de desecho es el ácido láctico.
Durante la glucólisis anaeróbica, NAD+ se regenera cuando los pares de hidrógeno se combinan con piruvato para formar lactato.
Durante la recuperación, cuando el oxígeno está disponible, el NAD+ se une al hidrógeno del lactato para formar ATP.
En la mayoría de los casos, el oxígeno también se libera como un producto de desecho.
Esto es análogo a la fuerza motriz de protones generada a través de la membrana mitocondrial interna en la respiración aeróbica.
En la señalización autocrina, el ligando afecta a la misma célula que lo libera.
En eucariotas (es decir, células animales, vegetales, fúngicas y protistas), hay dos tipos distintos de división celular: mitosis y meiosis.
Después de la división celular, cada una de las células hijas comienza la interfase de un nuevo ciclo.
Ambos ciclos de división celular se utilizan en el proceso de reproducción sexual en algún momento de su ciclo de vida.
A diferencia de los procesos de mitosis y meiosis en eucariotas, la fisión binaria tiene lugar sin la formación de un aparato de huso en la célula.
La herencia mendeliana, específicamente, es el proceso por el cual los genes y los rasgos se transmiten de padres a hijos.
La primera es que las características genéticas, que ahora se llaman alelos, son discretas y tienen formas alternativas (por ejemplo, púrpura vs. blanco o alto vs. enano), cada una heredada de uno de dos padres.
Mendel señaló que durante la formación de gametos, los alelos para cada gen se segregan entre sí, de modo que cada gameto lleva solo un alelo para cada gen, lo que se establece en su ley de segregación.
Los nucleótidos se unen entre sí en una cadena mediante enlaces covalentes entre el azúcar de un nucleótido y el fosfato del siguiente, dando como resultado una cadena principal alterna de azúcar-fosfato.
Las bases se dividen en dos grupos: pirimidinas y purinas.
El ADN se replica una vez que las dos cadenas se separan.
Un cromosoma es una estructura organizada que consiste en ADN e histonas.
En los procariotas, el ADN se mantiene dentro de un cuerpo de forma irregular en el citoplasma llamado nucleoide.
La información genética almacenada en el ADN representa el genotipo, mientras que el fenotipo resulta de la síntesis de proteínas que controlan la estructura y el desarrollo de un organismo, o que actúan como enzimas que catalizan vías metabólicas específicas.
Bajo el código genético, estas hebras de ARNm especifican la secuencia de aminoácidos dentro de las proteínas en un proceso llamado traducción, que ocurre en los ribosomas.
La secuenciación y el análisis de genomas se pueden hacer usando secuenciación de ADN de alto rendimiento y bioinformática para ensamblar y analizar la función y la estructura de genomas completos.
Los genomas de los procariotas son pequeños, compactos y diversos.
Hay cuatro procesos clave que subyacen al desarrollo: Determinación, diferenciación, morfogénesis y crecimiento.
Las células madre son células indiferenciadas o parcialmente diferenciadas que pueden diferenciarse en varios tipos de células y proliferar indefinidamente para producir más de la misma célula madre.
La apoptosis, o muerte celular programada, también ocurre durante la morfogénesis, como la muerte de las células entre los dedos en el desarrollo embrionario humano, que libera los dedos de manos y pies individuales.
Estos genes de kit de herramientas están altamente conservados entre los phyla, lo que significa que son antiguos y muy similares en grupos de animales ampliamente separados.
Los genes Hox determinan dónde las partes repetitivas, como las muchas vértebras de las serpientes, crecerán en un embrión o larva en desarrollo.
Un gen de kit de herramientas se puede expresar en un patrón diferente, como cuando el pico del gran pinzón de tierra de Darwin fue agrandado por el gen BMP, o cuando las serpientes perdieron sus piernas como genes Distal-less (Dlx) se expresaron insuficientemente o no se expresaron en absoluto en los lugares donde otros reptiles continuaron formando sus extremidades.
Esta perspectiva sostiene que la evolución ocurre cuando hay cambios en las frecuencias de los alelos dentro de una población de organismos que se entrecruzan.
Cuando las fuerzas selectivas están ausentes o son relativamente débiles, es igualmente probable que las frecuencias de los alelos se desplacen hacia arriba o hacia abajo en cada generación sucesiva debido a que los alelos están sujetos a error de muestreo.
El aislamiento reproductivo también tiende a aumentar con la divergencia genética.
Cuando un linaje se divide en dos, se representa como un nodo (o división) en el árbol filogenético.
Dentro de un árbol, cualquier grupo de especies designadas con un nombre es un taxón (por ejemplo, humanos, primates, mamíferos o vertebrados) y un taxón que consiste en todos sus descendientes evolutivos es un clado, también conocido como un taxón monofilético.
Una especie o grupo que está estrechamente relacionado con el grupo interno pero que está filogenéticamente fuera de él se llama grupo externo, que sirve como punto de referencia en el árbol.
Basado en el principio de Parsimonia (o la navaja de Occam), el árbol que se ve favorecido es el que tiene la menor cantidad de cambios evolutivos necesarios para ser asumido sobre todos los rasgos en todos los grupos.
Sobre la base de este sistema, a cada especie se le da dos nombres, uno para su género y otro para su especie.
Los biólogos consideran la ubicuidad del código genético como evidencia de descendencia común universal para todas las bacterias, arqueas y eucariotas.
Más tarde, hace unos 1.700 millones de años, comenzaron a aparecer organismos multicelulares, con células diferenciadas que realizaban funciones especializadas.
Las plantas terrestres tuvieron tanto éxito que se cree que contribuyeron al evento de extinción del Devónico Tardío.
Durante la recuperación de esta catástrofe, los arcosaurios se convirtieron en los vertebrados terrestres más abundantes; un grupo de arcosaurios, los dinosaurios, dominaron los períodos Jurásico y Cretácico.
Las bacterias habitan el suelo, el agua, las aguas termales ácidas, los desechos radiactivos y la biosfera profunda de la corteza terrestre.
Las arqueas constituyen el otro dominio de las células procariotas y se clasificaron inicialmente como bacterias, recibiendo el nombre de arqueobacterias (en el reino de las arqueobacterias), un término que ha caído en desuso.
Las arqueas y las bacterias son generalmente similares en tamaño y forma, aunque algunas arqueas tienen formas muy diferentes, como las células planas y cuadradas de Haloquadratum walsbyi.
Las arqueas usan más fuentes de energía que los eucariotas: estos van desde compuestos orgánicos, como azúcares, hasta amoníaco, iones metálicos o incluso gas hidrógeno.
Las primeras arqueas observadas eran extremófilos, que vivían en ambientes extremos, como aguas termales y lagos salados sin otros organismos.
Las arqueas son una parte importante de la vida de la Tierra.
Cinco de estos clados también se conocen colectivamente como protistas, que son en su mayoría organismos eucariotas microscópicos que no son plantas, hongos o animales.
La mayoría de los protistas son unicelulares, que también se conocen como eucariotas microbianas.
Los dinoflagelados son fotosintéticos y se pueden encontrar en el océano, donde desempeñan un papel como productores primarios de materia orgánica.
Los ciliados son alveolados que poseen numerosas estructuras similares al cabello llamadas cilios.
Las excavaciones son grupos de protistas que comenzaron a diversificarse hace aproximadamente 1.500 millones de años poco después del origen de los eucariotas.
Los estramenopiles, la mayoría de los cuales se pueden caracterizar por la presencia de pelos tubulares en el más largo de sus dos flagelos, incluyen diatomeas y algas pardas.
Los rizarianos comprenden tres grupos principales: cercozoos, foraminíferos y radiolarios.
Las algas comprenden varios clados distintos, como los glaucofitos, que son algas microscópicas de agua dulce que pueden haberse parecido en forma al ancestro unicelular temprano de Plantae.
Las plantas terrestres (embriofitas) aparecieron por primera vez en ambientes terrestres hace aproximadamente 450 a 500 millones de años.
Por el contrario, los otros tres clados son plantas no vasculares, ya que no tienen traqueidas.
Tienden a encontrarse en áreas donde el agua está fácilmente disponible.
La mayoría de las plantas no vasculares son terrestres, con algunas que viven en ambientes de agua dulce y ninguna que vive en los océanos.
Las gimnospermas incluyen coníferas, cícadas, ginkgo y gnetofitos.
Lo hacen a través de un proceso llamado heterotrofia de absorción por el cual primero secretarían enzimas digestivas que descomponen grandes moléculas de alimentos antes de absorberlas a través de sus membranas celulares.
Los hongos, junto con otros dos linajes, los coanoflagelados y los animales, se pueden agrupar como opisthokonts.
Los hongos multicelulares, por otro lado, tienen un cuerpo llamado micelio, que se compone de una masa de filamentos tubulares individuales llamados hifas que permite que se produzca la absorción de nutrientes.
Con pocas excepciones, los animales consumen materia orgánica, respiran oxígeno, son capaces de moverse, pueden reproducirse sexualmente y crecer a partir de una esfera hueca de células, la blástula, durante el desarrollo embrionario.
Los animales se pueden distinguir en dos grupos en función de sus características de desarrollo.
En los protostomas, el blastoporo da lugar a la boca, que es seguida por la formación del ano.
Los cuerpos de la mayoría de los animales son simétricos, con simetría radial o bilateral.
Finalmente, los animales pueden distinguirse en función del tipo y la ubicación de sus apéndices, como las antenas para detectar el entorno o las garras para capturar presas.
La mayoría (97%) de las especies animales son invertebrados, que son animales que no poseen ni desarrollan una columna vertebral (comúnmente conocida como columna vertebral o columna vertebral), derivada de la notocorda.
Muchos taxones de invertebrados tienen un mayor número y variedad de especies que todo el subfilo de los vertebrados.
Se han descrito más de 6.000 especies de virus en detalle.
Cuando no está dentro de una célula infectada o en el proceso de infectar una célula, los virus existen en forma de partículas independientes, o viriones, que consisten en el material genético (ADN o ARN), una cubierta proteica llamada cápside y, en algunos casos, una envoltura externa de lípidos.
Los orígenes de los virus en la historia evolutiva de la vida no están claros: algunos pueden haber evolucionado a partir de plásmidos, piezas de ADN que pueden moverse entre las células, mientras que otros pueden haber evolucionado a partir de bacterias.
Los virus pueden propagarse de muchas maneras.
El norovirus y el rotavirus, causas comunes de gastroenteritis viral, se transmiten por la vía fecal-oral, se pasan por contacto mano a boca o en alimentos o agua.
El sistema de brotes se compone de tallo, hojas y flores.
La dirección del movimiento del agua a través de una membrana semipermeable está determinada por el potencial de agua a través de esa membrana.
La mayoría de las semillas de plantas generalmente están inactivas, una condición en la que se suspende la actividad normal de la semilla.
La imbibición es el primer paso en la germinación, por lo que el agua es absorbida por la semilla.
Estos monómeros se obtienen a partir de la hidrólisis de almidón, proteínas y lípidos que se almacenan en los cotiledones o el endospermo.
Sus flores son órganos que facilitan la reproducción, generalmente al proporcionar un mecanismo para la unión de los espermatozoides con los óvulos.
La polinización cruzada es la transferencia de polen de la antera de una flor al estigma de otra flor en un individuo diferente de la misma especie.
Estos cambios pueden verse afectados por factores genéticos, químicos y físicos.
Las proteínas fotorreceptoras transmiten información como si es de día o de noche, la duración del día, la intensidad de la luz disponible y la fuente de luz.
Muchas plantas con flores florecen en el momento adecuado debido a los compuestos sensibles a la luz que responden a la duración de la noche, un fenómeno conocido como fotoperiodismo.
Los animales pueden clasificarse como reguladores o confórmeros.
Por el contrario, los animales como los peces y las ranas son conformes, ya que adaptan su entorno interno (por ejemplo, la temperatura corporal) para que coincida con sus entornos externos.
Los ratones, por ejemplo, son capaces de consumir tres veces más alimentos que los conejos en proporción a sus pesos, ya que la tasa metabólica basal por unidad de peso en ratones es mayor que en conejos.
Sin embargo, la relación no es lineal en los animales que nadan o vuelan.
A bajas velocidades de vuelo, un ave debe mantener una alta tasa metabólica para permanecer en el aire.
Finalmente, los animales de agua dulce tienen fluidos corporales que son hiperosmóticos para el agua dulce.
Si un animal consumiera alimentos que contienen una cantidad excesiva de energía química, almacenaría la mayor parte de esa energía en forma de lípidos para su uso futuro y parte de esa energía como glucógeno para un uso más inmediato (por ejemplo, satisfaciendo las necesidades de energía del cerebro).
Además de sus tractos digestivos, los animales vertebrados tienen glándulas accesorias como el hígado y el páncreas como parte de sus sistemas digestivos.
Al salir del estómago, el alimento entra en el intestino medio, que es la primera parte del intestino (o intestino delgado en los mamíferos) y es el sitio principal de digestión y absorción.
El intercambio de gases en los pulmones ocurre en millones de pequeños sacos de aire; en los mamíferos y reptiles estos se llaman alvéolos, y en las aves se les conoce como aurículas.
Estos entran en los pulmones donde se ramifican en bronquios secundarios y terciarios progresivamente más estrechos que se ramifican en numerosos tubos más pequeños, los bronquiolos.
Existen dos tipos de sistemas circulatorios: abiertos y cerrados.
La circulación en animales ocurre entre dos tipos de tejidos: tejidos sistémicos y órganos respiratorios (u pulmonares).
En aves y mamíferos, los sistemas sistémico y pulmonar están conectados en serie.
Las contracciones del músculo esquelético son neurogénicas, ya que requieren la entrada sináptica de las neuronas motoras.
La contracción producida puede describirse como una contracción, suma o tétanos, dependiendo de la frecuencia de los potenciales de acción.
Los mecanismos de contracción son similares en los tres tejidos musculares.
Otros animales, como los moluscos y los nematodos, poseen músculos estriados oblicuamente, que contienen bandas de filamentos gruesos y delgados que están dispuestos helicoidalmente en lugar de transversalmente, como en los músculos esqueléticos o cardíacos de los vertebrados.
Pueden transmitir o recibir información en sitios de contactos llamados sinapsis.
Las células tales como neuronas o células musculares pueden excitarse o inhibirse al recibir una señal de otra neurona.
En los vertebrados, el sistema nervioso consiste en el sistema nervioso central (SNC), que incluye el cerebro y la médula espinal, y el sistema nervioso periférico (SNP), que consiste en nervios que conectan el SNC a todas las demás partes del cuerpo.
El SNP se divide en tres subsistemas separados, los sistemas nerviosos somático, autónomo y entérico.
El sistema nervioso simpático se activa en casos de emergencia para movilizar energía, mientras que el sistema nervioso parasimpático se activa cuando los organismos están en un estado relajado.
Los nervios que salen directamente del cerebro se llaman nervios craneales, mientras que los que salen de la médula espinal se llaman nervios espinales.
En los seres humanos específicamente, las principales glándulas endocrinas son la glándula tiroides y las glándulas suprarrenales.
Las hormonas pueden ser complejos de aminoácidos, esteroides, eicosanoides, leucotrienos o prostaglandinas.
Producen gametos haploides por meiosis.
En la mayoría de los casos, una tercera capa germinal, el mesodermo, también se desarrolla entre ellos.
La gastrulación ocurre, por lo que los movimientos morfogenéticos convierten la masa celular en tres capas germinales que comprenden el ectodermo, el mesodermo y el endodermo.
La diferenciación celular está influenciada por señales extracelulares tales como factores de crecimiento que se intercambian con células adyacentes, lo que se denomina señalización yuxtracrina, o con células vecinas a distancias cortas, lo que se denomina señalización paracrina.
El sistema inmune adaptativo proporciona una respuesta adaptada a cada estímulo al aprender a reconocer las moléculas que ha encontrado anteriormente.
Las bacterias tienen un sistema inmunológico rudimentario en forma de enzimas que protegen contra las infecciones por virus.
Los vertebrados con mandíbula, incluidos los humanos, tienen mecanismos de defensa aún más sofisticados, incluida la capacidad de adaptarse para reconocer patógenos de manera más eficiente.
Los patrones de acción fijos, por ejemplo, son comportamientos genéticamente determinados y estereotipados que ocurren sin aprender.
La comunidad de organismos vivos (bióticos) junto con los componentes no vivos (abióticos) (por ejemplo, agua, luz, radiación, temperatura, humedad, atmósfera, acidez y suelo) de su entorno se llama ecosistema.
Al alimentarse de las plantas y entre sí, los animales juegan un papel importante en el movimiento de la materia y la energía a través del sistema.
El entorno físico de la Tierra está formado por la energía solar y la topografía.
El clima es la temperatura del día a día y la actividad de precipitación, mientras que el clima es el promedio a largo plazo del clima, típicamente promediado durante un período de 30 años.
Como resultado, los ambientes húmedos permiten que crezca la vegetación exuberante.
El crecimiento de la población durante los intervalos a corto plazo se puede determinar utilizando la ecuación de la tasa de crecimiento de la población, que tiene en cuenta las tasas de nacimiento, muerte e inmigración.
Una interacción biológica es el efecto que un par de organismos que viven juntos en una comunidad tienen el uno sobre el otro.
Una interacción a largo plazo se llama simbiosis.
Hay diferentes niveles tróficos dentro de cualquier red alimentaria, siendo el nivel más bajo los productores primarios (o autótrofos) como las plantas y las algas que convierten la energía y el material inorgánico en compuestos orgánicos, que luego pueden ser utilizados por el resto de la comunidad.
Y aquellos que comen consumidores secundarios son consumidores terciarios y así sucesivamente.
En algunos ciclos hay reservorios donde una sustancia permanece o es secuestrada por un largo período de tiempo.
El mayor impulsor del calentamiento es la emisión de gases de efecto invernadero, de los cuales más del 90% son dióxido de carbono y metano.
La biodiversidad afecta el funcionamiento de los ecosistemas, que proporcionan una variedad de servicios de los que dependen las personas.
Tradicionalmente, la botánica también ha incluido el estudio de hongos y algas por micólogos y phycologists respectivamente, con el estudio de estos tres grupos de organismos que permanecen dentro de la esfera de interés del Congreso Botánico Internacional.
Los jardines físicos medievales, a menudo unidos a monasterios, contenían plantas de importancia médica.
Estos jardines facilitaron el estudio académico de las plantas.
En las últimas dos décadas del siglo XX, los botánicos explotaron las técnicas de análisis genético molecular, incluida la genómica y la proteómica y las secuencias de ADN para clasificar las plantas con mayor precisión.
La botánica moderna tiene sus raíces en la antigua Grecia específicamente en Teofrasto (c. 371-287 aC), un estudiante de Aristóteles que inventó y describió muchos de sus principios y es ampliamente considerado en la comunidad científica como el "Padre de la botánica".
De Materia Medica fue ampliamente leído durante más de 1.500 años.
A mediados del siglo XVI, se fundaron jardines botánicos en varias universidades italianas.
Apoyaron el crecimiento de la botánica como materia académica.
A lo largo de este período, la botánica permaneció firmemente subordinada a la medicina.
Bock creó su propio sistema de clasificación de plantas.
La elección y la secuencia de los caracteres pueden ser artificiales en claves diseñadas puramente para la identificación (claves de diagnóstico) o más estrechamente relacionadas con el orden natural o filético de los taxones en claves sinópticas.
Esto estableció un binomio estandarizado o esquema de nomenclatura de dos partes donde el primer nombre representaba el género y el segundo identificaba las especies dentro del género.
El aumento del conocimiento de la anatomía de las plantas, la morfología y los ciclos de vida llevó a la comprensión de que había más afinidades naturales entre las plantas que el sistema sexual artificial de Linneo.
El trabajo de Katherine Esau (1898-1997) sobre la anatomía de las plantas sigue siendo una base importante de la botánica moderna.
El concepto de que la composición de las comunidades de plantas como el bosque templado de hoja ancha cambia por un proceso de sucesión ecológica fue desarrollado por Henry Chandler Cowles, Arthur Tansley y Frederic Clements.
El descubrimiento e identificación de las hormonas vegetales de auxina por Kenneth V. Thimann en 1948 permitió la regulación del crecimiento de las plantas mediante productos químicos aplicados externamente.
Los desarrollos del siglo XX en bioquímica vegetal han sido impulsados por técnicas modernas de análisis químico orgánico, como espectroscopia, cromatografía y electroforesis.
Estas tecnologías permiten el uso biotecnológico de plantas enteras o cultivos de células vegetales cultivados en biorreactores para sintetizar pesticidas, antibióticos u otros productos farmacéuticos, así como la aplicación práctica de cultivos modificados genéticamente diseñados para rasgos tales como rendimiento mejorado.
La sistemática moderna tiene como objetivo reflejar y descubrir las relaciones filogenéticas entre las plantas.
Como un subproducto de la fotosíntesis, las plantas liberan oxígeno a la atmósfera, un gas que es requerido por casi todos los seres vivos para llevar a cabo la respiración celular.
Históricamente, todos los seres vivos se clasificaron como animales o plantas y la botánica cubrió el estudio de todos los organismos no considerados animales.
La definición más estricta de "planta" incluye solo las "plantas terrestres" o embriofitos, que incluyen plantas de semillas (gimnospermas, incluidos los pinos y las plantas con flores) y los criptogams de esporas libres que incluyen helechos, clubmoses, hepáticas, hornworts y musgos.
La fase haploide sexual de los embriofitos, conocida como el gametofito, nutre al embrión diploide en desarrollo dentro de sus tejidos durante al menos parte de su vida, incluso en las plantas de semillas, donde el propio gametofito es nutrido por su esporofito padre.
Los paleobotánicos estudian plantas antiguas en el registro fósil para proporcionar información sobre la historia evolutiva de las plantas.
Esto es lo que los ecologistas llaman el primer nivel trófico.
Los botánicos también estudian las malas hierbas, que son un problema considerable en la agricultura, y la biología y el control de los patógenos de las plantas en la agricultura y los ecosistemas naturales.
La energía luminosa capturada por la clorofila a está inicialmente en forma de electrones (y más tarde un gradiente de protones) que se utiliza para hacer moléculas de ATP y NADPH que almacenan y transportan energía temporalmente.
Parte de la glucosa se convierte en almidón que se almacena en el cloroplasto.
A diferencia de los animales (que carecen de cloroplastos), las plantas y sus parientes eucariotas han delegado muchas funciones bioquímicas a sus cloroplastos, incluida la síntesis de todos sus ácidos grasos y la mayoría de los aminoácidos.
Las plantas terrestres vasculares producen lignina, un polímero utilizado para fortalecer las paredes celulares secundarias de las traqueidas del xilema y los vasos para evitar que se colapsen cuando una planta succiona agua a través de ellas bajo estrés hídrico.
Otros, como los aceites esenciales aceite de menta y aceite de limón son útiles para su aroma, como aromatizantes y especias (por ejemplo, capsaicina), y en la medicina como productos farmacéuticos como en el opio de las amapolas de opio.
Por ejemplo, la aspirina analgésica es el éster acetilo del ácido salicílico, originalmente aislado de la corteza de los sauces, y se obtiene una amplia gama de analgésicos opiáceos como la heroína por modificación química de la morfina obtenida de la amapola del opio.
Los nativos americanos han utilizado varias plantas como formas de tratar enfermedades durante miles de años.
Azúcar, almidón, algodón, lino, cáñamo, algunos tipos de cuerda, madera y tableros de partículas, papiro y papel, aceites vegetales, cera y caucho natural son ejemplos de materiales comercialmente importantes hechos de tejidos vegetales o sus productos secundarios.
Los productos hechos de celulosa incluyen rayón y celofán, pasta de papel tapiz, biobutanol y algodón con pistola.
Algunos ecologistas incluso se basan en datos empíricos de los pueblos indígenas recopilados por etnobotánicos.
Las plantas dependen de ciertos factores edáficos (suelo) y climáticos en su entorno, pero también pueden modificar estos factores.
Interactúan con sus vecinos en una variedad de escalas espaciales en grupos, poblaciones y comunidades que colectivamente constituyen vegetación.
Gregor Mendel descubrió las leyes genéticas de la herencia estudiando rasgos heredados como la forma en Pisum sativum (guisantes).
Sin embargo, hay algunas diferencias genéticas distintivas entre las plantas y otros organismos.
Las muchas variedades cultivadas de trigo son el resultado de múltiples cruces inter e intraespecíficos entre especies silvestres y sus híbridos.
En muchas plantas terrestres, los gametos masculinos y femeninos son producidos por individuos separados.
La formación de tubérculos de tallo en la patata es un ejemplo.
La apomixis también puede ocurrir en una semilla, produciendo una semilla que contiene un embrión genéticamente idéntico al padre.
Una planta alopoliploide puede resultar de un evento de hibridación entre dos especies diferentes.
Algunos poliploides vegetales de otro modo estériles todavía pueden reproducirse vegetativamente o por apomixis de semillas, formando poblaciones clonales de individuos idénticos.
El diente de león común es un triploide que produce semillas viables por semilla apomíctica.
La secuenciación de algunos otros genomas relativamente pequeños, de arroz (Oryza sativa) y distachyon de Brachypodium, los ha convertido en especies modelo importantes para comprender la genética, la biología celular y molecular de cereales, gramíneas y monocotiledóneas en general.
Espinacas, guisantes, soja y un musgo Physcomitrella patens se utilizan comúnmente para estudiar la biología de las células vegetales.
La expresión génica también puede controlarse mediante proteínas represoras que se unen a regiones silenciadoras del ADN y evitan que se exprese esa región del código de ADN.
Algunos cambios epigenéticos han demostrado ser hereditarios, mientras que otros se restablecen en las células germinales.
A diferencia de los animales, muchas células vegetales, particularmente las del parénquima, no se diferencian terminalmente, permaneciendo totipotentes con la capacidad de dar lugar a una nueva planta individual.
Las algas son un grupo polifilético y se colocan en varias divisiones, algunas más estrechamente relacionadas con las plantas que otras.
La clase Charophyte Charophyceae y el sub-reino de la planta terrestre Embryophyta forman juntos el grupo monofilético o clado Streptophytina.
Las plantas vasculares pteridofíticas con verdadero xilema y floema que se reproducían por esporas que germinaban en gametofitos de vida libre evolucionaron durante el período Silúrico y se diversificaron en varios linajes durante el Silúrico tardío y el Devónico temprano.
Sus gametofitos reducidos se desarrollaron a partir de megasporas retenidas dentro de los órganos productores de esporas (megasporangios) del esporofito, una condición conocida como endosporia.
Las primeras plantas de semillas conocidas datan de la última etapa devónica fameniana.
Los productos químicos obtenidos del aire, el suelo y el agua forman la base de todo el metabolismo de las plantas.
Los heterótrofos, incluidos todos los animales, todos los hongos, todas las plantas completamente parasitarias y las bacterias no fotosintéticas, absorben moléculas orgánicas producidas por los fotoautótrofos y las respiran o las usan en la construcción de células y tejidos.
El transporte subcelular de iones, electrones y moléculas como el agua y las enzimas se produce a través de las membranas celulares.
Ejemplos de elementos que las plantas necesitan transportar son nitrógeno, fósforo, potasio, calcio, magnesio y azufre.
Este compuesto media las respuestas tropicales de los brotes y las raíces hacia la luz y la gravedad.
La citoquinina zeatina natural se descubrió en el maíz, Zea mays, y es un derivado de la purina adenina.
Están involucrados en la promoción de la germinación y la ruptura de la latencia en las semillas, en la regulación de la altura de la planta mediante el control de la elongación del tallo y el control de la floración.
Se llamaba así porque originalmente se pensaba que controlaba la abscisión.
Otra clase de fitohormonas son los jasmonatos, aislados primero del aceite de Jasminum grandiflorum que regula las respuestas de las heridas en las plantas desbloqueando la expresión de genes requeridos en la respuesta de resistencia adquirida sistémica al ataque de patógenos.
Las plantas no vasculares, las hepáticas, las hornworts y los musgos no producen raíces vasculares penetrantes en el suelo y la mayor parte de la planta participa en la fotosíntesis.
Las células en cada sistema son capaces de crear células del otro y producir brotes o raíces adventicias.
En el caso de que uno de los sistemas se pierda, el otro a menudo puede volver a crecer.
En las plantas vasculares, el xilema y el floema son los tejidos conductores que transportan los recursos entre los brotes y las raíces.
Las hojas recogen la luz solar y realizan la fotosíntesis.
Las angiospermas son plantas productoras de semillas que producen flores y tienen semillas cerradas.
Algunas plantas se reproducen sexualmente, algunas asexualmente y otras a través de ambos medios.
La clasificación biológica es una forma de taxonomía científica.
Si bien los científicos no siempre están de acuerdo sobre cómo clasificar los organismos, la filogenética molecular, que utiliza secuencias de ADN como datos, ha impulsado muchas revisiones recientes a lo largo de las líneas evolutivas y es probable que continúe haciéndolo.
La nomenclatura de los organismos botánicos está codificada en el Código Internacional de Nomenclatura de algas, hongos y plantas (ICN) y es administrada por el Congreso Botánico Internacional.
El nombre científico de una planta representa su género y sus especies dentro del género, lo que resulta en un solo nombre mundial para cada organismo.
La combinación es el nombre de la especie.
Las relaciones evolutivas y la herencia de un grupo de organismos se llama filogenia.
Como ejemplo, las especies de Pereskia son árboles o arbustos con hojas prominentes.
Juzgar las relaciones basadas en personajes compartidos requiere cuidado, ya que las plantas pueden parecerse entre sí a través de la evolución convergente en la que los personajes han surgido de forma independiente.
Solo los caracteres derivados, como las areolas productoras de espina dorsal de los cactus, proporcionan evidencia de descendencia de un ancestro común.
La diferencia es que el código genético en sí mismo se utiliza para decidir las relaciones evolutivas, en lugar de ser utilizado indirectamente a través de los personajes que da lugar a.
La evidencia genética sugiere que la verdadera relación evolutiva de los organismos multicelulares es como se muestra en el cladograma a continuación: los hongos están más estrechamente relacionados con los animales que con las plantas.
Investigar cómo se relacionan las especies de plantas entre sí permite a los botánicos comprender mejor el proceso de evolución en las plantas.
Aunque los humanos siempre han estado interesados en la historia natural de los animales que veían a su alrededor, y han hecho uso de este conocimiento para domesticar ciertas especies, se puede decir que el estudio formal de la zoología se originó con Aristóteles.
La zoología moderna tiene sus orígenes durante el Renacimiento y el período moderno temprano, con Carl Linnaeus, Antonie van Leeuwenhoek, Robert Hooke, Charles Darwin, Gregor Mendel y muchos otros.
Hay pinturas rupestres, grabados y esculturas en Francia que datan de 15.000 años que muestran bisontes, caballos y ciervos en detalle cuidadosamente prestados.
El conocimiento antiguo de la vida silvestre se ilustra por las representaciones realistas de animales salvajes y domésticos en el Cercano Oriente, Mesopotamia y Egipto, incluidas las prácticas y técnicas de cría, caza y pesca.
Aristóteles, en el siglo IV a.C., consideraba a los animales como organismos vivos, estudiando su estructura, desarrollo y fenómenos vitales.
Cuatrocientos años más tarde, el médico romano Galeno disecó animales para estudiar su anatomía y la función de las diferentes partes, porque la disección de cadáveres humanos estaba prohibida en ese momento.
En Europa, el trabajo de Galen en la anatomía permaneció en gran parte insuperable e incontestado hasta el 16to siglo.
Habiendo sido previamente el reino de caballeros naturalistas, durante los siglos XVIII, XIX y XX, la zoología se convirtió en una disciplina científica cada vez más profesional.
Estos desarrollos, así como los resultados de la embriología y la paleontología, fueron sintetizados en la publicación de 1859 de la teoría de la evolución de Charles Darwin por selección natural; en este Darwin colocó la teoría de la evolución orgánica en una nueva base, explicando los procesos por los cuales puede ocurrir, y proporcionando evidencia observacional de que lo había hecho.
Darwin dio una nueva dirección a la morfología y la fisiología, uniéndolos en una teoría biológica común: la teoría de la evolución orgánica.
Una necesidad temprana era identificar los organismos y agruparlos de acuerdo con sus características, diferencias y relaciones, y este es el campo del taxonomista.
Sus ideas se centraron en la morfología de los animales.
Desde entonces, estas agrupaciones han sido revisadas para mejorar la coherencia con el principio darwiniano de la descendencia común.
Homo es el género, y sapiens el epíteto específico, ambos combinados forman el nombre de la especie.
El sistema de clasificación dominante se llama taxonomía de Linneo.
Comprender la estructura y función de las células es fundamental para todas las ciencias biológicas.
Se centra en cómo los órganos y los sistemas de órganos trabajan juntos en los cuerpos de los seres humanos y los animales, además de cómo funcionan de forma independiente.
Los estudios fisiológicos se han dividido tradicionalmente en fisiología vegetal y fisiología animal, pero algunos principios de la fisiología son universales, sin importar qué organismo en particular se esté estudiando.
Por ejemplo, generalmente involucra a científicos que tienen un entrenamiento especial en organismos particulares como la mamíferosogia, la ornitología, la herpetología o la entomología, pero usan esos organismos como sistemas para responder preguntas generales sobre la evolución.
Los etólogos se han preocupado particularmente por la evolución del comportamiento y la comprensión del comportamiento en términos de la teoría de la selección natural.
Mientras que los investigadores practican técnicas específicas de biología molecular, es común combinarlas con métodos de genética y bioquímica.
La sistemática biológica es el estudio de la diversificación de las formas de vida, tanto pasadas como presentes, y las relaciones entre los seres vivos a través del tiempo.
Los árboles filogenéticos de especies y taxones superiores se utilizan para estudiar la evolución de rasgos (por ejemplo, características anatómicas o moleculares) y la distribución de organismos (biogeografía).
La sistemática biológica clasifica las especies mediante el uso de tres ramas específicas.
La sistemática experimental identifica y clasifica a los animales en función de las unidades evolutivas que componen una especie, así como su importancia en la evolución misma.
Explicar la biodiversidad del planeta y sus organismos.
La taxonomía es la parte de la Sistemática que se ocupa de los temas (a) a (d) anteriores.
Sin embargo, en el uso moderno, todos se pueden considerar sinónimos el uno del otro.
Algunos afirman que la sistemática por sí sola se ocupa específicamente de las relaciones a través del tiempo, y que puede ser sinónimo de filogenética, en términos generales, se trata de la jerarquía inferida de los organismos.
Las clasificaciones científicas son ayudas para registrar y reportar información a otros científicos y laicos.
En biología, una especie es la unidad básica de clasificación y un rango taxonómico de un organismo, así como una unidad de biodiversidad.
Además, los paleontólogos utilizan el concepto de la cronoespecie ya que la reproducción fósil no puede ser examinada.
Todas las especies (excepto los virus) reciben un nombre de dos partes, un "binomial".
Por ejemplo, Boa constrictor es una de las cuatro especies del género Boa, siendo constrictor el epíteto de la especie.
Además, entre los organismos que se reproducen solo asexualmente, el concepto de una especie reproductiva se descompone, y cada clon es potencialmente una microespecie.
Las especies fueron vistas desde la época de Aristóteles hasta el siglo XVIII como categorías fijas que podían organizarse en una jerarquía, la gran cadena del ser.
Esa comprensión se extendió en gran medida en el siglo XX a través de la genética y la ecología de la población.
Ernst Mayr enfatizó el aislamiento reproductivo, pero esto, al igual que otros conceptos de especies, es difícil o incluso imposible de probar.
Este método se usó como un método "clásico" de determinar especies, como con Linnaeus temprano en la teoría evolutiva.
Como regla general, los microbiólogos han asumido que los tipos de bacterias o arqueas con secuencias de genes de ARN ribosómico 16S más similares entre sí deben verificarse mediante hibridación de ADN-ADN para decidir si pertenecen a la misma especie o no.
Los enfoques modernos comparan la similitud de secuencia usando métodos computacionales.
Una base de datos, Barcode of Life Data Systems (BOLD), contiene secuencias de códigos de barras de ADN de más de 190.000 especies.
Por ejemplo, en un estudio realizado sobre hongos, el estudio de los caracteres de nucleótidos utilizando especies cladísticas produjo los resultados más precisos en el reconocimiento de las numerosas especies de hongos de todos los conceptos estudiados.
Sin embargo, otros defienden este enfoque, considerando la "inflación taxonómica" como peyorativa y etiquetando la visión opuesta como "conservadurismo taxonómico"; afirmando que es políticamente conveniente dividir las especies y reconocer poblaciones más pequeñas a nivel de especie, porque esto significa que pueden incluirse más fácilmente como en peligro de extinción en la lista roja de la UICN y pueden atraer legislación y fondos para la conservación.
Si los científicos quieren decir que algo se aplica a todas las especies dentro de un género, usan el nombre del género sin el nombre específico o epíteto.
A medida que más información llega a la mano, la hipótesis puede ser corroborada o refutada.
Dividir un taxón en múltiples taxones, a menudo nuevos, se llama división.
El término cuasiespecie a veces se usa para mutar rápidamente entidades como virus.
En las especies de anillos, cuando los miembros de poblaciones adyacentes en un rango de distribución ampliamente continuo se cruzan con éxito, pero los miembros de poblaciones más distantes no lo hacen.
Por lo tanto, las especies en anillo presentan una dificultad para cualquier concepto de especie que se base en el aislamiento reproductivo.
La especiación depende de una medida de aislamiento reproductivo, un flujo genético reducido.
Las bacterias pueden intercambiar plásmidos con bacterias de otras especies, incluidas algunas aparentemente lejanas en diferentes dominios filogenéticos, lo que dificulta el análisis de sus relaciones y debilita el concepto de una especie bacteriana.
Las extinciones masivas tuvieron una variedad de causas, incluida la actividad volcánica, el cambio climático y los cambios en la química oceánica y atmosférica, y a su vez tuvieron efectos importantes en la ecología, la atmósfera, la superficie terrestre y las aguas de la Tierra.
Algunos observadores afirman que existe un conflicto inherente entre el deseo de comprender los procesos de especiación y la necesidad de identificar y categorizar.
Uno de los casos clásicos en América del Norte es el del búho moteado del norte protegido que se hibrida con el búho moteado de California sin protección y el búho barrado; Esto ha llevado a debates legales.
Una forma se distinguía por ser compartida por todos sus miembros, los jóvenes heredaban cualquier variación que pudieran tener de sus padres.
Estableció la idea de una jerarquía taxonómica de clasificación basada en características observables y destinada a reflejar las relaciones naturales.
Jean-Baptiste Lamarck, en su Filosofía Zoológica de 1809, describió la transmutación de especies, proponiendo que una especie podría cambiar con el tiempo, en una desviación radical del pensamiento aristotélico.
El género (géneros plurales) es un rango taxonómico utilizado en la clasificación biológica de organismos vivos y fósiles, así como virus.
Panthera leo (león) y Panthera onca (jaguar) son dos especies dentro del género Panthera.
Un ejemplo botánico sería Hibiscus arnottianus, una especie particular del género Hibiscus nativa de Hawai.
Los nombres disponibles son los publicados de acuerdo con el Código Internacional de Nomenclatura Zoológica y no suprimidos por decisiones posteriores de la Comisión Internacional de Nomenclatura Zoológica (ICZN); el nombre más antiguo para cualquier taxón (por ejemplo, un género) debe seleccionarse como el nombre "válido" (es decir, actual o aceptado) para el taxón en cuestión.
En botánica, existen conceptos similares pero con diferentes etiquetas.
Sin embargo, muchos nombres se han asignado (por lo general involuntariamente) a dos o más géneros diferentes.
Un nombre que significa dos cosas diferentes es un homónimo.
Sin embargo, un género en un reino se permite llevar un nombre científico que está en el uso como un nombre genérico (o el nombre de un taxón en otra fila) en un reino que es gobernado por un código de la nomenclatura diferente.
Por ejemplo, entre los reptiles (no aviares), que tienen aproximadamente 1180 géneros, la mayoría (300) tienen solo 1 especie, 360 tienen entre 2 y 4 especies, 260 tienen 5-10 especies, 200 tienen 11-50 especies, y solo 27 géneros tienen más de 50 especies.
¿Qué especies se asignan a un género es algo arbitrario.
Lo que pertenece a una familia, o si una familia descrita debe ser reconocida, es propuesta y determinada por taxónomos practicantes.
A menudo no hay un acuerdo exacto, con diferentes taxonomistas cada uno tomando una posición diferente.
Michael Novacek (1986) los insertó en la misma posición.
No hay reglas objetivas para describir una clase, pero para animales bien conocidos es probable que haya consenso.
En botánica, las clases rara vez son discutidas.
Informalmente, los phyla se pueden considerar como agrupaciones de organismos basadas en la especialización general del plan corporal.
Por lo tanto, los phyla pueden fusionarse o dividirse si se hace evidente que están relacionados entre sí o no.
Según la definición de Budd y Jensen, un phylum se define por un conjunto de personajes compartidos por todos sus representantes vivos.
Sin embargo, como se basa en caracteres, es fácil de aplicar al registro fósil.
Sin embargo, probar que un fósil pertenece al grupo de la corona de un phylum es difícil, ya que debe mostrar un carácter único para un subconjunto del grupo de la corona.
La siguiente tabla sigue el influyente (aunque polémico) sistema Cavalier-Smith al igualar "Plantae" con Archaeplastida, un grupo que contiene Viridiplantae y las divisiones de algas Rhodophyta y Glaucophyta.
La división Pinophyta se puede usar para todas las gimnospermas (es decir, incluidas las cícadas, los ginkgos y los gnetofitos), o para las coníferas solas como se indica a continuación.
Protista es un taxón polifilético, que es menos aceptable para los biólogos actuales que en el pasado.
Carl Linnaeus (1707-1778) sentó las bases de la nomenclatura biológica moderna, ahora regulada por los Códigos de Nomenclatura, en 1735.
En 1937 Édouard Chatton introdujo los términos "procariota" y "eucariota" para diferenciar estos organismos.
Robert Whittaker reconoció un reino adicional para los hongos.
Los dos reinos restantes, Protista y Monera, incluyeron colonias unicelulares y celulares simples.
En otros sistemas, como el sistema de Lynn Margulis de cinco reinos, las plantas incluían solo las plantas terrestres (Embryophyta), y Protoctista tiene una definición más amplia.
Los avances tecnológicos en microscopía electrónica permitieron la separación del reino Chromista del reino Plantae.
Finalmente, se descubrieron algunos protistas que carecían de mitocondrias.
Este superreino se oponía al superreino de Metakaryota, agrupando a los otros cinco reinos eucariotas (Animalia, Protozoa, Fungi, Plantae y Chromista).
Cavalier-Smith ya no aceptó la importancia de la división fundamental Eubacteria-Archaebacteria presentada por Woese y otros y respaldada por investigaciones recientes.
Cavalier-Smith no acepta el requisito de que los taxones sean monofiléticos ("holofiléticos" en su terminología) para ser válidos.
Los avances de los estudios filogenéticos permitieron a Cavalier-Smith darse cuenta de que todos los phyla que se pensaba que eran arquezoanos (es decir, eucariotas primitivamente amitocondriados) habían perdido secundariamente sus mitocondrias, típicamente transformándolas en nuevos orgánulos: Hidrogenosomas.
Sobre la base de tales estudios de ARN, Carl Woese pensó que la vida podría dividirse en tres grandes divisiones y se refirió a ellas como el modelo de "tres reinos primarios" o modelo de "urkingdom".
Woese dividió a los procariotas (anteriormente clasificados como Kingdom Monera) en dos grupos, llamados Eubacteria y Archaebacteria, enfatizando que había tanta diferencia genética entre estos dos grupos como entre cualquiera de ellos y todos los eucariotas.
Sostuvieron que sólo los grupos monofiléticos se deberían aceptar como filas formales en una clasificación y que – mientras este enfoque había sido impracticable antes (necesitando "literalmente docenas de 'reinos eucarióticos') – se había hecho posible ahora dividir los eucariotas en "sólo unos grupos principales que son probablemente todos monofiléticos".
Dividió a los eucariotas en los mismos seis "supergrupos".
Se cree que las plantas están más distantemente relacionadas con los animales y los hongos.
Los diez argumentos en contra incluyen el hecho de que son parásitos intracelulares obligados que carecen de metabolismo y no son capaces de replicación fuera de una célula huésped.
Los dos primeros son todos microorganismos procariotas, o en su mayoría organismos unicelulares cuyas células tienen un núcleo distorsionado o no unido a la membrana.
Los halófilos, organismos que prosperan en ambientes altamente salados, y los hipertermófilos, organismos que prosperan en ambientes extremadamente calientes, son ejemplos de Archaea.
Las cianobacterias y los micoplasmas son dos ejemplos de bacterias.
La evolución es el cambio en las características heredables de las poblaciones biológicas a lo largo de generaciones sucesivas.
La evolución ocurre cuando los procesos evolutivos como la selección natural (incluida la selección sexual) y la deriva genética actúan sobre esta variación, lo que hace que ciertas características se vuelvan más comunes o raras dentro de una población.
La teoría científica de la evolución por selección natural fue concebida independientemente por Charles Darwin y Alfred Russel Wallace a mediados del siglo XIX y se expuso en detalle en el libro de Darwin Sobre el origen de las especies.
Por lo tanto, en generaciones sucesivas, los miembros de una población tienen más probabilidades de ser reemplazados por las progenies de padres con características favorables que les han permitido sobrevivir y reproducirse en sus respectivos entornos.
El registro fósil incluye una progresión de grafito biogénico temprano, a fósiles de esteras microbianas, a organismos multicelulares fosilizados.
Buscó explicaciones de los fenómenos naturales en términos de leyes físicas que eran las mismas para todas las cosas visibles y que no requerían la existencia de ninguna categoría natural fija u orden cósmico divino.
La clasificación biológica introducida por Carl Linnaeus en 1735 explícitamente reconoció la naturaleza jerárquica de relaciones de la especie, pero todavía veía especies como fijadas según un plan divino.
Estas ideas fueron condenadas por naturalistas establecidos como especulación carente de apoyo empírico.
En parte bajo la influencia de Un Ensayo sobre el Principio de la Población (1798) por Thomas Robert Malthus, Darwin notó que el crecimiento demográfico llevaría a una "lucha por la existencia" en la cual las variaciones favorables prevalecieron ya que los otros perecieron.
Darwin desarrolló su teoría de la "selección natural" a partir de 1838 y estaba escribiendo su "gran libro" sobre el tema cuando Alfred Russel Wallace le envió una versión de prácticamente la misma teoría en 1858.
Con este fin, Darwin desarrolló su teoría provisional de la pangénesis.
Para explicar cómo se originan las nuevas variantes, de Vries desarrolló una teoría de mutación que condujo a una brecha temporal entre aquellos que aceptaron la evolución darwiniana y los biométricos que se aliaron con de Vries.
La publicación de la estructura del ADN por James Watson y Francis Crick con la contribución de Rosalind Franklin en 1953 demostró un mecanismo físico para la herencia.
En 1973, el biólogo evolutivo Theodosius Dobzhansky escribió que "nada en biología tiene sentido excepto a la luz de la evolución", porque ha sacado a la luz las relaciones de lo que primero parecían hechos inconexos en la historia natural en un cuerpo explicativo coherente de conocimiento que describe y predice muchos hechos observables sobre la vida en este planeta.
El conjunto completo de rasgos observables que componen la estructura y el comportamiento de un organismo se llama fenotipo.
Por ejemplo, la piel bronceada proviene de la interacción entre el genotipo de una persona y la luz solar; por lo tanto, los bronceados no se transmiten a los hijos de las personas.
El ADN es un biopolímero largo compuesto de cuatro tipos de bases.
Las porciones de una molécula de ADN que especifican una sola unidad funcional se llaman genes; diferentes genes tienen diferentes secuencias de bases.
Si la secuencia de ADN en un locus varía entre individuos, las diferentes formas de esta secuencia se denominan alelos.
Sin embargo, aunque esta simple correspondencia entre un alelo y un rasgo funciona en algunos casos, la mayoría de los rasgos son más complejos y están controlados por loci de rasgos cuantitativos (múltiples genes que interactúan).
La metilación del ADN que marca la cromatina, los bucles metabólicos autosostenibles, el silenciamiento génico por interferencia del ARN y la conformación tridimensional de las proteínas (como los priones) son áreas donde se han descubierto sistemas de herencia epigenética a nivel organísmico.
Por ejemplo, la herencia ecológica a través del proceso de construcción de nichos se define por las actividades regulares y repetidas de los organismos en su entorno.
A pesar de la introducción constante de nuevas variaciones a través de la mutación y el flujo de genes, la mayor parte del genoma de una especie es idéntica en todos los individuos de esa especie.
Una parte sustancial de la variación fenotípica en una población está causada por la variación genotípica.
La variación desaparece cuando un nuevo alelo alcanza el punto de fijación, cuando desaparece de la población o reemplaza por completo al alelo ancestral.
Cuando se producen mutaciones, pueden alterar el producto de un gen, o impedir que el gen funcione, o no tener ningún efecto.
Las copias adicionales de genes son una fuente importante de la materia prima necesaria para que evolucionen los nuevos genes.
Se pueden generar nuevos genes a partir de un gen ancestral cuando una copia duplicada muta y adquiere una nueva función.
La generación de nuevos genes también puede implicar la duplicación de pequeñas partes de varios genes, con estos fragmentos que luego se recombinan para formar nuevas combinaciones con nuevas funciones.
La recombinación y la redistribución no alteran las frecuencias de los alelos, sino que cambian qué alelos están asociados entre sí, produciendo descendencia con nuevas combinaciones de alelos.
El primer costo es que en las especies sexualmente dimórficas solo uno de los dos sexos puede tener crías.
Sin embargo, la reproducción sexual es el medio más común de reproducción entre los eucariotas y los organismos multicelulares.
La transferencia génica entre especies incluye la formación de organismos híbridos y la transferencia génica horizontal.
Se ha producido la transferencia horizontal de genes de bacterias a eucariotas tales como la levadura Saccharomyces cerevisiae y el gorgojo de frijol adzuki Callosobruchus chinensis.
Diferentes rasgos confieren diferentes tasas de supervivencia y reproducción (aptitud diferencial).
En consecuencia, los organismos con rasgos que les dan una ventaja sobre sus competidores tienen más probabilidades de transmitir sus rasgos a la próxima generación que aquellos con rasgos que no les confieren una ventaja.
El concepto central de la selección natural es la aptitud evolutiva de un organismo.
Por ejemplo, si un organismo pudiera sobrevivir bien y reproducirse rápidamente, pero su descendencia fuera demasiado pequeña y débil para sobrevivir, este organismo haría poca contribución genética a las generaciones futuras y, por lo tanto, tendría una baja aptitud.
Ejemplos de rasgos que pueden aumentar la condición física son la supervivencia mejorada y el aumento de la fecundidad.
Sin embargo, incluso si la dirección de selección se invierte de esta manera, los rasgos que se perdieron en el pasado pueden no volver a evolucionar en una forma idéntica (ver la ley de Dollo).
La primera es la selección direccional, que es un cambio en el valor promedio de un rasgo a lo largo del tiempo, por ejemplo, los organismos se hacen más altos lentamente.
Finalmente, en la selección estabilizadora hay selección contra valores de rasgos extremos en ambos extremos, lo que causa una disminución en la varianza alrededor del valor promedio y menos diversidad.
Esta amplia comprensión de la naturaleza permite a los científicos delinear fuerzas específicas que, en conjunto, comprenden la selección natural.
Sin embargo, la tasa de recombinación es baja (aproximadamente dos eventos por cromosoma por generación).
Un conjunto de alelos que generalmente se hereda en un grupo se llama haplotipo.
Esta deriva se detiene cuando un alelo finalmente se fija, ya sea desapareciendo de la población o reemplazando los otros alelos por completo.
La teoría neutral de la evolución molecular propuso que la mayoría de los cambios evolutivos son el resultado de la fijación de mutaciones neutras por deriva genética.
Sin embargo, una versión más reciente y mejor apoyada de este modelo es la teoría casi neutral, donde una mutación que sería efectivamente neutral en una población pequeña no es necesariamente neutral en una población grande.
El número de individuos en una población no es crítico, sino una medida conocida como el tamaño efectivo de la población.
La presencia o ausencia de flujo genético cambia fundamentalmente el curso de la evolución.
Este argumento de presiones opuestas fue utilizado durante mucho tiempo para descartar la posibilidad de tendencias internas en la evolución, hasta que la era molecular provocó un renovado interés en la evolución neutral.
Por ejemplo, los sesgos de mutación se invocan frecuentemente en modelos de uso de codones.
Diferentes sesgos de inserción frente a deleción en diferentes taxones pueden conducir a la evolución de diferentes tamaños de genoma.
El pensamiento contemporáneo sobre el papel de los sesgos de mutación refleja una teoría diferente de la de Haldane y Fisher.
Los organismos también pueden responder a la selección cooperando entre sí, generalmente ayudando a sus familiares o participando en simbiosis mutuamente beneficiosa.
La macroevolución se refiere a la evolución que ocurre en o por encima del nivel de la especie, en particular la especiación y la extinción; mientras que la microevolución se refiere a cambios evolutivos más pequeños dentro de una especie o población, en particular cambios en la frecuencia y adaptación de los alelos.
Sin embargo, en la macroevolución, los rasgos de toda la especie pueden ser importantes.
Un error común es que la evolución tiene metas, planes a largo plazo, o una tendencia innata para el "progreso", como se expresa en creencias como la ortogénesis y el evolucionismo; sin embargo, de manera realista, la evolución no tiene un objetivo a largo plazo y no necesariamente produce una mayor complejidad.
Además, el término adaptación puede referirse a un rasgo que es importante para la supervivencia de un organismo.
Un rasgo adaptativo es un aspecto del patrón de desarrollo del organismo que permite o mejora la probabilidad de que ese organismo sobreviva y se reproduzca.
Otros ejemplos llamativos son la bacteria Escherichia coli que desarrolla la capacidad de usar ácido cítrico como nutriente en un experimento de laboratorio a largo plazo, Flavobacterium que desarrolla una nueva enzima que permite que estas bacterias crezcan en los subproductos de la fabricación de nylon y la bacteria del suelo Sphingobium que desarrolla una vía metabólica completamente nueva que degrada el pesticida sintético pentaclorofenol.
En consecuencia, las estructuras con organización interna similar pueden tener diferentes funciones en organismos relacionados.
Sin embargo, dado que todos los organismos vivos están relacionados en cierta medida, incluso los órganos que parecen tener poca o ninguna similitud estructural, como los ojos de artrópodos, calamares y vertebrados, o las extremidades y alas de artrópodos y vertebrados, pueden depender de un conjunto común de genes homólogos que controlan su ensamblaje y función; esto se denomina homología profunda.
Los ejemplos incluyen pseudogenes, los restos no funcionales de ojos en peces ciegos que habitan en cuevas, alas en aves no voladoras, la presencia de huesos de cadera en ballenas y serpientes, y rasgos sexuales en organismos que se reproducen a través de la reproducción asexual.
Un ejemplo es el lagarto africano Holaspis guentheri, que desarrolló una cabeza extremadamente plana para esconderse en grietas, como se puede ver al observar a sus parientes cercanos.
Otro ejemplo es el reclutamiento de enzimas de la glucólisis y el metabolismo xenobiótico para servir como proteínas estructurales llamadas cristalinas dentro de las lentes de los ojos de los organismos.
Estos estudios han demostrado que la evolución puede alterar el desarrollo para producir nuevas estructuras, como las estructuras óseas embrionarias que se desarrollan en la mandíbula en otros animales, en lugar de formar parte del oído medio en los mamíferos.
Estos cambios en la segunda especie, a su vez, causan nuevas adaptaciones en la primera especie.
Por ejemplo, existe una cooperación extrema entre las plantas y los hongos micorrícicos que crecen en sus raíces y ayudan a la planta a absorber los nutrientes del suelo.
Las coaliciones entre organismos de la misma especie también han evolucionado.
Aquí, las células somáticas responden a señales específicas que les indican si crecer, permanecer como están o morir.
Existen múltiples formas de definir el concepto de “especie”.
A pesar de la diversidad de varios conceptos de especies, estos diversos conceptos se pueden colocar en uno de tres enfoques filosóficos amplios: mestizaje, ecológico y filogenético.
A pesar de su uso amplio y a largo plazo, el BSC como otros no está exento de controversia, por ejemplo, porque estos conceptos no se pueden aplicar a los procariotas, y esto se llama el problema de las especies.
El flujo de genes puede ralentizar este proceso mediante la difusión de las nuevas variantes genéticas también a las otras poblaciones.
En este caso, las especies estrechamente relacionadas pueden cruzarse regularmente, pero los híbridos se seleccionarán y las especies permanecerán distintas.
La especiación se ha observado varias veces tanto en condiciones de laboratorio controladas (ver experimentos de laboratorio de especiación) como en la naturaleza.
La más común en los animales es la especiación alopátrica, que se produce en poblaciones inicialmente aisladas geográficamente, como por fragmentación del hábitat o migración.
El segundo modo de especiación es la especiación peripátrica, que ocurre cuando pequeñas poblaciones de organismos se aíslan en un nuevo entorno.
El tercer modo es la especiación parapátrica.
En general, esto ocurre cuando ha habido un cambio drástico en el medio ambiente dentro del hábitat de la especie parental.
La selección contra el cruzamiento con la población parental sensible a los metales produjo un cambio gradual en el tiempo de floración de las plantas resistentes a los metales, lo que finalmente produjo un aislamiento reproductivo completo.
Esta forma es rara, ya que incluso una pequeña cantidad de flujo genético puede eliminar las diferencias genéticas entre partes de una población.
Esto no es común en los animales, ya que los híbridos animales suelen ser estériles.
Esto permite que los cromosomas de cada especie parental formen pares coincidentes durante la meiosis, ya que los cromosomas de cada padre ya están representados por un par.
De hecho, la duplicación cromosómica dentro de una especie puede ser una causa común de aislamiento reproductivo, ya que la mitad de los cromosomas duplicados no tendrán igual cuando se reproduzcan con organismos no duplicados.
Casi todas las especies animales y vegetales que han vivido en la Tierra ahora están extintas, y la extinción parece ser el destino final de todas las especies.
A pesar de la extinción estimada de más del 99 por ciento de todas las especies que alguna vez vivieron en la Tierra, se estima que alrededor de 1 billón de especies están en la Tierra actualmente con solo una milésima parte del uno por ciento descrito.
La primera evidencia indiscutible de vida en la Tierra data de hace al menos 3.500 millones de años, durante la Era Eoarquea después de que una corteza geológica comenzara a solidificarse después del Eón de Hadeo fundido anterior.
Al comentar sobre los hallazgos australianos, Stephen Blair Hedges escribió: "Si la vida surgió relativamente rápido en la Tierra, entonces podría ser común en el universo".
Las estimaciones sobre el número de especies actuales de la Tierra oscilan entre 10 millones y 14 millones, de los cuales se estima que alrededor de 1,9 millones han sido nombrados y 1,6 millones documentados en una base de datos central hasta la fecha, dejando al menos el 80 por ciento aún no descrito.
La descendencia común de los organismos se dedujo por primera vez de cuatro hechos simples sobre los organismos: Primero, tienen distribuciones geográficas que no pueden explicarse por la adaptación local.
En cuarto lugar, los organismos se pueden clasificar utilizando estas similitudes en una jerarquía de grupos anidados, similar a un árbol genealógico.
Esta visión se remonta a una idea brevemente mencionada por Darwin, pero más tarde abandonada.
Al comparar las anatomías de especies modernas y extintas, los paleontólogos pueden inferir los linajes de esas especies.
Más recientemente, la evidencia de la descendencia común ha venido del estudio de las similitudes bioquímicas entre los organismos.
Las células eucariotas surgieron hace entre 1.600 y 2.700 millones de años.
Otro engullimiento de organismos similares a las cianobacterias condujo a la formación de cloroplastos en algas y plantas.
En enero de 2016, los científicos informaron que, hace unos 800 millones de años, un cambio genético menor en una sola molécula llamada GK-PID pudo haber permitido que los organismos pasaran de un solo organismo celular a una de muchas células.
Se han propuesto varios factores desencadenantes de la explosión cámbrica, incluida la acumulación de oxígeno en la atmósfera a partir de la fotosíntesis.
La selección artificial es la selección intencional de rasgos en una población de organismos.
Las proteínas con propiedades valiosas han evolucionado por rondas repetidas de mutación y selección (por ejemplo, enzimas modificadas y nuevos anticuerpos) en un proceso llamado evolución dirigida.
La cría conjunta de diferentes poblaciones de este pez ciego produjo algunas crías con ojos funcionales, ya que se habían producido diferentes mutaciones en las poblaciones aisladas que habían evolucionado en diferentes cuevas.
Muchas enfermedades humanas no son fenómenos estáticos, sino capaces de evolucionar.
Es posible que estemos ante el final de la vida efectiva de la mayoría de los antibióticos disponibles y predigamos la evolución de nuestros patógenos y diseñemos estrategias para ralentizarlos o eludirlos, lo que requiere un conocimiento más profundo de las complejas fuerzas que impulsan la evolución a nivel molecular.
Utilizó estrategias de evolución para resolver problemas complejos de ingeniería.
En algunos países, notablemente los Estados Unidos, estas relaciones tensas entre ciencia y religión han abastecido de combustible la controversia de la evolución de la creación corriente, un conflicto religioso que se concentra en política y educación pública.
La decisión del Juicio de Scopes de 1925 hizo que el tema se volviera muy raro en los libros de texto de biología secundaria estadounidenses durante una generación, pero se volvió a presentar gradualmente más tarde y se protegió legalmente con la decisión de 1968 Epperson v.
La selección natural es la supervivencia diferencial y la reproducción de los individuos debido a las diferencias en el fenotipo.
La variación existe dentro de todas las poblaciones de organismos.
El entorno de un genoma incluye la biología molecular en la célula, otras células, otros individuos, poblaciones, especies, así como el entorno abiótico.
La selección natural es una piedra angular de la biología moderna.
El concepto de selección natural se desarrolló originalmente en ausencia de una teoría válida de la herencia; en el momento de la escritura de Darwin, la ciencia aún no había desarrollado las teorías modernas de la genética.
Los argumentos clásicos fueron introducidos de nuevo en el 18vo siglo por Pierre Louis Maupertuis y otros, incluso el abuelo de Darwin, Erasmo Darwin.
El éxito de esta teoría aumentó la conciencia de la gran escala del tiempo geológico e hizo plausible la idea de que los pequeños cambios, prácticamente imperceptibles en las generaciones sucesivas podrían producir consecuencias en la escala de las diferencias entre especies.
Estaba en el proceso de escribir su "gran libro" para presentar su investigación cuando el naturalista Alfred Russel Wallace concibió independientemente el principio y lo describió en un ensayo que envió a Darwin para remitirlo a Charles Lyell.
En la tercera edición de 1861 Darwin reconoció que otros, como William Charles Wells en 1813 y Patrick Matthew en 1831, habían propuesto ideas similares, pero no las habían desarrollado ni presentado en publicaciones científicas notables.
En una carta a Charles Lyell en el septiembre de 1860, Darwin lamentó el uso del término "Selección Natural", prefiriendo el término "Preservación Natural".
Sin embargo, la selección natural siguió siendo controvertida como mecanismo, en parte porque se percibía que era demasiado débil para explicar el rango de características observadas de los organismos vivos, y en parte porque incluso los partidarios de la evolución se resistieron a su naturaleza "no guiada" y no progresiva, una respuesta que se ha caracterizado como el impedimento más significativo para la aceptación de la idea.
Con la integración de principios del siglo XX de la evolución con las leyes de la herencia de Mendel, la llamada síntesis moderna, los científicos generalmente llegaron a aceptar la selección natural.
J. B. S. Haldane introdujo el concepto del "costo" de la selección natural.
Sin embargo, la selección natural es "ciega" en el sentido de que los cambios en el fenotipo pueden dar una ventaja reproductiva independientemente de si el rasgo es hereditario o no.
Si los rasgos que dan a estos individuos una ventaja reproductiva también son heredables, es decir, transmitidos de padres a hijos, entonces habrá una reproducción diferencial, es decir, una proporción ligeramente mayor de conejos rápidos o algas eficientes en la próxima generación.
Esto da la apariencia de un propósito, pero en la selección natural no hay una elección intencional.
Esto dio a las polillas de color oscuro una mejor oportunidad de sobrevivir para producir descendencia de color oscuro, y en solo cincuenta años desde que se atrapó la primera polilla oscura, casi todas las polillas en Manchester industrial eran oscuras.
Si un organismo vive la mitad de tiempo que otros de su especie, pero tiene el doble de descendientes que sobreviven hasta la edad adulta, sus genes se vuelven más comunes en la población adulta de la próxima generación.
Se debe hacer una distinción entre el concepto de "supervivencia del más apto" y "mejora en la aptitud".
Haldane llamó a este proceso "sustitución" o más comúnmente en biología, esto se llama "fijación".
La probabilidad de que ocurra una mutación beneficiosa en algún miembro de una población depende del número total de replicaciones de esa variante.
En este experimento, la "mejora en la aptitud" depende del número de replicaciones de la variante particular para que aparezca una nueva variante que sea capaz de crecer en la siguiente región de concentración de fármaco más alta.
El experimento clásico de evolución a largo plazo de E. coli de Richard Lenski es un ejemplo de adaptación en un entorno competitivo, ("mejora en la condición física" durante la "supervivencia del más apto").
La selección disruptiva poco común también actúa durante los períodos de transición cuando el modo actual es subóptimo, pero altera el rasgo en más de una dirección.
Algunos biólogos reconocen solo dos tipos: selección de viabilidad (o supervivencia), que actúa para aumentar la probabilidad de supervivencia de un organismo, y selección de fecundidad (o fertilidad o reproducción), que actúa para aumentar la tasa de reproducción, dada la supervivencia.
En la selección de parentesco y el conflicto intragenómico, la selección a nivel de gen proporciona una explicación más adecuada del proceso subyacente.
La selección ecológica es la selección natural a través de cualquier medio que no sea la selección sexual, como la selección de parentesco, la competencia y el infanticidio.
Sin embargo, en algunas especies, la elección de pareja es principalmente por los machos, como en algunos peces de la familia Syngnathidae.
Desde el descubrimiento de la penicilina en 1928, los antibióticos se han utilizado para combatir enfermedades bacterianas.
La variación genética es el resultado de mutaciones, recombinaciones genéticas y alteraciones en el cariotipo (el número, forma, tamaño y disposición interna de los cromosomas).
Sin embargo, muchas mutaciones en el ADN no codificante tienen efectos perjudiciales.
Los cambios en estos a menudo tienen grandes efectos en el fenotipo del individuo porque regulan la función de muchos otros genes.
Cuando tales mutaciones dan como resultado una mayor aptitud, la selección natural favorece estos fenotipos y el rasgo novedoso se extiende en la población.
Sin embargo, es intrínseco al concepto de una especie contra la que se seleccionan los híbridos, oponiéndose a la evolución del aislamiento reproductivo, un problema que fue reconocido por Darwin.
El fenotipo está determinado por la composición genética de un organismo (genotipo) y el entorno en el que vive el organismo.
Un ejemplo son los antígenos del tipo de sangre ABO en seres humanos, donde tres alelos gobiernan el fenotipo.
Este proceso puede continuar hasta que el alelo se fije y toda la población comparta el fenotipo de ajuste.
La selección estabilizadora conserva características genéticas funcionales, tales como genes codificantes de proteínas o secuencias reguladoras, a lo largo del tiempo mediante presión selectiva contra variantes perjudiciales.
Algunas formas de selección de equilibrio no dan como resultado la fijación, sino que mantienen un alelo a frecuencias intermedias en una población.
El mantenimiento de la variación alélica también puede ocurrir a través de la selección disruptiva o diversificadora, que favorece genotipos que se alejan del promedio en cualquier dirección (es decir, lo opuesto a la sobredominancia), y puede dar como resultado una distribución bimodal de los valores de los rasgos.
Sin embargo, después de un período sin nuevas mutaciones, la variación genética en estos sitios se elimina debido a la deriva genética.
El resultado exacto de los dos procesos depende tanto de la velocidad a la que se producen nuevas mutaciones como de la fuerza de la selección natural, que es una función de cuán desfavorable resulta ser la mutación.
La posibilidad de que se produzca una reorganización de este tipo entre dos alelos está inversamente relacionada con la distancia entre ellos.
Un fuerte barrido selectivo da como resultado una región del genoma donde el haplotipo seleccionado positivamente (el alelo y sus vecinos) son en esencia los únicos que existen en la población.
La selección de fondo es lo opuesto a un barrido selectivo.
En palabras del filósofo Daniel Dennett, "la peligrosa idea de Darwin" de la evolución por selección natural es un "ácido universal", que no puede mantenerse restringido a ningún recipiente o contenedor, ya que pronto se escapa, abriéndose camino en un entorno cada vez más amplio.
Estas condiciones son: heredabilidad, variación de tipo y competencia por recursos limitados.
Herbert Spencer y la eugenesia defienden la interpretación de Francis Galton de la selección natural como necesariamente progresiva, lo que lleva a supuestos avances en la inteligencia y la civilización, se convirtió en una justificación para el colonialismo, la eugenesia y el darwinismo social.
La idea racial como base de nuestro estado ya ha logrado mucho en este sentido.
El ejemplo más destacado de la psicología evolutiva, notablemente avanzado en los primeros trabajos de Noam Chomsky y más tarde de Steven Pinker, es la hipótesis de que el cerebro humano se ha adaptado para adquirir las reglas gramaticales del lenguaje natural.
Observó que los organismos (plantas de guisante) heredan rasgos a través de "unidades de herencia" discretas.
La estructura y función génica, la variación y la distribución se estudian dentro del contexto de la célula, el organismo (por ejemplo, dominancia) y dentro del contexto de una población.
Los procesos genéticos trabajan en combinación con el entorno y las experiencias de un organismo para influir en el desarrollo y el comportamiento, a menudo denominados naturaleza versus crianza.
La ciencia moderna de la genética, tratando de entender este proceso, comenzó con el trabajo del fraile agustino Gregor Mendel a mediados del siglo XIX.
Su segunda ley es la misma que Mendel publicó.
Una teoría popular durante el 19no siglo, e implicado por Charles Darwin 1859 En el Origen de Especies, mezclaba la herencia: la idea que los individuos heredan una mezcla lisa de rasgos de sus padres.
En su artículo "Versuche Aber Pflanzenhybriden" ("Experimentos sobre la hibridación de plantas"), presentado en 1865 a la Naturforschender Verein (Sociedad para la Investigación en la Naturaleza) en Brunn, Mendel rastreó los patrones de herencia de ciertos rasgos en las plantas de guisantes y los describió matemáticamente.
William Bateson, un defensor del trabajo de Mendel, acuñó la palabra genética en 1905 (el adjetivo genético, derivado de la palabra griega génesis, "origen", es anterior al sustantivo y se usó por primera vez en un sentido biológico en 1860).
Durante los siguientes 11 años, descubrió que las mujeres solo tenían el cromosoma X y los hombres tenían los cromosomas X e Y.
James Watson y Francis Crick determinaron la estructura del ADN en 1953, utilizando el trabajo de cristalografía de rayos X de Rosalind Franklin y Maurice Wilkins que indicaba que el ADN tenía una estructura helicoidal (es decir, con forma de sacacorchos).
La estructura también sugirió un método simple para la replicación: si las cadenas se separan, se pueden reconstruir nuevas cadenas asociadas para cada una en función de la secuencia de la cadena antigua.
En los años siguientes, los científicos trataron de entender cómo el ADN controla el proceso de producción de proteínas.
Con la nueva comprensión molecular de la herencia llegó una explosión de la investigación.
Un desarrollo importante fue la secuenciación de ADN de terminación de cadena en 1977 por Frederick Sanger.
En sus experimentos que estudian el rasgo para el color de la flor, Mendel observó que las flores de cada planta de guisante eran púrpura o blanca, pero nunca un intermedio entre los dos colores.
Muchas especies, incluidos los humanos, tienen este patrón de herencia.
Cuando los organismos son heterocigotos en un gen, a menudo un alelo se llama dominante ya que sus cualidades dominan el fenotipo del organismo, mientras que el otro alelo se llama recesivo ya que sus cualidades retroceden y no se observan.
A menudo se usa un símbolo "+" para marcar el alelo no mutante habitual para un gen.
Uno de los diagramas comunes usados para predecir el resultado del cruzamiento es el cuadrado de Punnett.
Algunos genes no se clasifican de forma independiente, lo que demuestra la vinculación genética, un tema discutido más adelante en este artículo.)
Otro gen, sin embargo, controla si las flores tienen color o son blancas.
Muchos rasgos no son rasgos discretos (por ejemplo, flores moradas o blancas), sino que son rasgos continuos (por ejemplo, la altura humana y el color de la piel).
El grado en que los genes de un organismo contribuyen a un rasgo complejo se llama heredabilidad.
El ADN está compuesto por una cadena de nucleótidos, de los cuales hay cuatro tipos: adenina (A), citosina (C), guanina (G) y timina (T).
Los virus no pueden reproducirse sin un huésped y no se ven afectados por muchos procesos genéticos, por lo que tienden a no considerarse organismos vivos.
Esta estructura del ADN es la base física para la herencia: la replicación del ADN duplica la información genética dividiendo las cadenas y utilizando cada cadena como plantilla para la síntesis de una nueva cadena asociada.
Estas cadenas de ADN a menudo son extremadamente largas; el cromosoma humano más grande, por ejemplo, tiene aproximadamente 247 millones de pares de bases de longitud.
El ADN se encuentra con mayor frecuencia en el núcleo de las células, pero Ruth Sager ayudó en el descubrimiento de genes no cromosómicos que se encuentran fuera del núcleo.
Mientras que los organismos haploides tienen solo una copia de cada cromosoma, la mayoría de los animales y muchas plantas son diploides, que contienen dos de cada cromosoma y, por lo tanto, dos copias de cada gen.
En los seres humanos y muchos otros animales, el cromosoma Y contiene el gen que desencadena el desarrollo de las características específicamente masculinas.
Este proceso, llamado mitosis, es la forma más simple de reproducción y es la base para la reproducción asexual.
Los organismos eucariotas a menudo usan la reproducción sexual para generar descendencia que contiene una mezcla de material genético heredado de dos padres diferentes.
Algunas bacterias pueden experimentar conjugación, transfiriendo una pequeña pieza circular de ADN a otra bacteria.
De esta manera, pueden ocurrir nuevas combinaciones de genes en la descendencia de un par de apareamiento.
Durante el cruce, los cromosomas intercambian tramos de ADN, barajando efectivamente los alelos del gen entre los cromosomas.
La primera demostración citológica del cruce fue realizada por Harriet Creighton y Barbara McClintock en 1931.
Para una distancia arbitrariamente larga, la probabilidad de cruce es lo suficientemente alta como para que la herencia de los genes no esté correlacionada de manera efectiva.
La secuencia específica de aminoácidos da como resultado una estructura tridimensional única para esa proteína, y las estructuras tridimensionales de las proteínas están relacionadas con sus funciones.
La estructura de la proteína es dinámica; la proteína hemoglobina se dobla en formas ligeramente diferentes, ya que facilita la captura, el transporte y la liberación de moléculas de oxígeno dentro de la sangre de los mamíferos.
Por ejemplo, la anemia de células falciformes es una enfermedad genética humana que resulta de una diferencia de base única dentro de la región codificante para la sección de p-globina de la hemoglobina, causando un cambio de aminoácido único que cambia las propiedades físicas de la hemoglobina.
Algunas secuencias de ADN se transcriben en ARN pero no se traducen en productos proteicos, tales moléculas de ARN se denominan ARN no codificante.
Un ejemplo interesante es la coloración del pelaje del gato siamés.
Pero estas proteínas oscuras que producen cabello son sensibles a la temperatura (es decir, tienen una mutación que causa sensibilidad a la temperatura) y se desnaturalizan en entornos de mayor temperatura, no produciendo pigmento oscuro en áreas donde el gato tiene una temperatura corporal más alta.
Después de la caída del Imperio Romano de Occidente, el conocimiento de las concepciones griegas del mundo se deterioró en Europa occidental durante los primeros siglos (400 a 1000 dC) de la Edad Media, pero se conservó en el mundo musulmán durante la Edad de Oro islámica.
La ciencia moderna típicamente se divide en tres ramas principales que consisten en las ciencias naturales (p.ej., biología, química y física), que estudian la naturaleza en el sentido más amplio; las ciencias sociales (p.ej., economía, psicología y sociología), que estudian a individuos y sociedades; y las ciencias formales (p.ej., lógica, matemáticas y ciencias informáticas teóricas), que tratan con símbolos gobernados por reglas.
El nuevo conocimiento en la ciencia es avanzado por la investigación de científicos que están motivados por la curiosidad sobre el mundo y el deseo de resolver problemas.
En particular, era el tipo de conocimiento que las personas pueden comunicarse entre sí y compartir.
Sin embargo, no se hizo una distinción consciente consistente entre el conocimiento de tales cosas, que son ciertas en todas las comunidades, y otros tipos de conocimiento comunitario, como las mitologías y los sistemas legales.
Incluso desarrollaron un calendario oficial que contenía doce meses, treinta días cada uno y cinco días al final del año.
Por esta razón, se afirma que estos hombres fueron los primeros filósofos en el sentido estricto, y también los primeros en distinguir claramente la "naturaleza" y la "convención".
Por el contrario, tratar de usar el conocimiento de la naturaleza para imitar a la naturaleza (artificio o tecnología, techné griego) fue visto por los científicos clásicos como un interés más apropiado para los artesanos de clase social inferior.
La teoría de los átomos fue desarrollada por el filósofo griego Leucipo y su estudiante Demócrito.
El método socrático documentado por los diálogos de Platón es un método dialéctico de eliminación de hipótesis: las mejores hipótesis se encuentran identificando y eliminando constantemente aquellas que conducen a contradicciones.
Sócrates criticó el tipo más antiguo de estudio de la física como demasiado puramente especulativo y carente de autocrítica.
Aristóteles más tarde creó un programa sistemático de la filosofía teleológica: el movimiento y el cambio se describe como la actualización de los potenciales ya en las cosas, de acuerdo con qué tipo de cosas son.
Los socráticos también insistieron en que la filosofía debería usarse para considerar la cuestión práctica de la mejor manera de vivir para un ser humano (un estudio de Aristóteles dividido en ética y filosofía política).
El modelo de Aristarco fue ampliamente rechazado porque se creía que violaba las leyes de la física.
John Philoponus, un erudito Bizantino en los años 500, puso la enseñanza de Aristóteles en duda de la física, notando sus defectos.
Las cuatro causas de Aristóteles prescribían que la pregunta "por qué" debía ser respondida de cuatro maneras para explicar las cosas científicamente.
Sin embargo, los textos originales de Aristóteles finalmente se perdieron en Europa Occidental, y sólo un texto por Platón extensamente se conocía, Timeo, que era el único diálogo Platónico y uno de los pocos trabajos originales de la filosofía natural clásica, disponible para lectores latinos en la Edad media temprana.
Muchas traducciones sirias fueron hechas por grupos como Nestorians y Monophysites.
p. 465: "solo cuando la influencia de ibn al-Haytam y otros en la corriente principal de los escritos físicos medievales posteriores ha sido seriamente investigada puede ser evaluada la afirmación de Schramm de que ibn al-Haytam fue el verdadero fundador de la física moderna".
El canon de Avicena se considera una de las publicaciones más importantes en medicina y ambos contribuyeron significativamente a la práctica de la medicina experimental, utilizando ensayos clínicos y experimentos para respaldar sus afirmaciones.
Además, los textos griegos clásicos comenzaron a traducirse del árabe y griego al latín, dando un nivel más alto de la discusión científica en Europa Occidental.
Las copias del manuscrito de Alhazen del libro de la óptica también se propagaron a través de Europa antes de 1240, como lo demuestra su incorporación en la Perspectiva de Vitello.
La afluencia de textos antiguos causó el Renacimiento del siglo XII y el florecimiento de una síntesis del catolicismo y el aristotelismo conocida como escolástica en Europa occidental, que se convirtió en un nuevo centro geográfico de la ciencia.
Un modelo de visión más tarde conocido como perspectivismo fue explotado y estudiado por los artistas del Renacimiento.
Esto estaba basado en un teorema que los períodos orbitales de los planetas son más largos ya que sus orbes están más lejos del centro del movimiento, que encontró no estar de acuerdo con el modelo de Ptolemeo.
Descubrió que toda la luz de un solo punto de la escena fue fotografiada en un solo punto en la parte posterior de la esfera de vidrio.
Kepler no rechazó la metafísica aristotélica y describió su trabajo como una búsqueda de la Armonía de las Esferas.
Galileo había usado argumentos del Papa y los había puesto en la voz del simplón en el trabajo "Diálogo Acerca de los Dos Sistemas mundiales Principales", que enormemente ofendió a Urban VIII.
Descartes enfatizó el pensamiento individual y argumentó que las matemáticas en lugar de la geometría deberían usarse para estudiar la naturaleza.
Esta nueva ciencia comenzó a verse a sí misma como una descripción de las "leyes de la naturaleza".
En el estilo de Francis Bacon, Leibniz asumió que los diferentes tipos de cosas funcionan de acuerdo con las mismas leyes generales de la naturaleza, sin causas formales o finales especiales para cada tipo de cosa.
En palabras de Bacon, "el objetivo real y legítimo de las ciencias es la dotación de la vida humana con nuevas invenciones y riquezas", y desalentó a los científicos de perseguir ideas filosóficas o espirituales intangibles, que creía que contribuían poco a la felicidad humana más allá de "el humo de la especulación sutil, sublime o agradable".
Otro desarrollo importante fue la popularización de la ciencia entre una población cada vez más alfabetizada.
Los filósofos de la Ilustración eligieron una breve historia de los predecesores científicos, Galileo, Boyle y Newton principalmente, como guías y garantes de sus aplicaciones del concepto singular de la naturaleza y la ley natural a todos los campos físicos y sociales de la época.
Hume y otros pensadores de la Ilustración escocesa desarrollaron una "ciencia del hombre", que se expresó históricamente en obras de autores como James Burnett, Adam Ferguson, John Millar y William Robertson, todos los cuales fusionaron un estudio científico de cómo los humanos se comportaron en culturas antiguas y primitivas con una fuerte conciencia de las fuerzas determinantes de la modernidad.
Tanto John Herschel como William Whewell sistematizaron la metodología: este último acuñó el término científico.
Por separado, Gregor Mendel presentó su artículo, "Versuche Ober Pflanzenhybriden" ("Experimentos sobre la hibridación de plantas"), en 1865, que delineó los principios de la herencia biológica, que sirve como base para la genética moderna.
Los fenómenos que permitirían la deconstrucción del átomo fueron descubiertos en la última década del siglo XIX: el descubrimiento de los rayos X inspiró el descubrimiento de la radiactividad.
Además, el uso extensivo de la innovación tecnológica estimulada por las guerras de este siglo llevó a revoluciones en el transporte (automóviles y aviones), el desarrollo de ICBMs, una carrera espacial y una carrera de armamentos nucleares.
El descubrimiento de la radiación cósmica de fondo de microondas en 1964 llevó a un rechazo de la teoría del estado estacionario del universo a favor de la teoría del Big Bang de Georges Lemaetre.
El uso generalizado de circuitos integrados en el último cuarto del siglo XX combinado con satélites de comunicaciones condujo a una revolución en la tecnología de la información y al surgimiento de Internet global y la informática móvil, incluidos los teléfonos inteligentes.
Tanto las ciencias naturales como las sociales son ciencias empíricas, ya que su conocimiento se basa en observaciones empíricas y es capaz de ser probado para su validez por otros investigadores que trabajan en las mismas condiciones.
Por ejemplo, la ciencia física se puede subdividir en física, química, astronomía y ciencias de la tierra.
Aún así, las perspectivas filosóficas, las conjeturas y las presuposiciones, a menudo pasadas por alto, siguen siendo necesarias en las ciencias naturales.
Incluye matemáticas, teoría de sistemas y ciencias de la computación teóricas.
Por lo tanto, las ciencias formales son disciplinas a priori y, debido a esto, hay desacuerdo sobre si realmente constituyen una ciencia.
La ingeniería en sí abarca una gama de campos más especializados de la ingeniería, cada uno con un énfasis más específico en áreas particulares de las matemáticas aplicadas, la ciencia y los tipos de aplicación.
Respondió: "Señor, ¿de qué sirve un niño recién nacido?".
Esta nueva explicación se utiliza para hacer predicciones falsables que son comprobables por experimento u observación.
Esto se hace en parte a través de la observación de fenómenos naturales, pero también a través de la experimentación que intenta simular eventos naturales en condiciones controladas según sea apropiado para la disciplina (en las ciencias observacionales, como la astronomía o la geología, una observación predicha podría tomar el lugar de un experimento controlado).
Si la hipótesis sobreviviera a las pruebas, podría adoptarse en el marco de una teoría científica, un modelo o marco lógicamente razonado y autoconsistente para describir el comportamiento de ciertos fenómenos naturales.
En ese sentido, las teorías se formulan de acuerdo con la mayoría de los mismos principios científicos que las hipótesis.
Esto se puede lograr mediante un cuidadoso diseño experimental, transparencia y un exhaustivo proceso de revisión por pares de los resultados experimentales, así como de cualquier conclusión.
La estadística, una rama de las matemáticas, se utiliza para resumir y analizar los datos, lo que permite a los científicos evaluar la fiabilidad y la variabilidad de sus resultados experimentales.
Se puede contrastar con el antirrealismo, la opinión de que el éxito de la ciencia no depende de que sea precisa sobre entidades no observables como los electrones.
Hay diferentes escuelas de pensamiento en la filosofía de la ciencia.
Esto es necesario porque el número de predicciones que hacen esas teorías es infinito, lo que significa que no se pueden conocer a partir de la cantidad finita de evidencia utilizando solo la lógica deductiva.
El racionalismo crítico es un enfoque contrastante de la ciencia del siglo XX, definido por primera vez por el filósofo austriaco-británico Karl Popper.
Popper propuso reemplazar la verificabilidad con la falsabilidad como el hito de las teorías científicas y reemplazar la inducción con la falsificación como el método empírico.
Otro enfoque, el instrumentalismo, enfatiza la utilidad de las teorías como instrumentos para explicar y predecir fenómenos.
Cerca del instrumentalismo está el empirismo constructivo, según el cual el criterio principal para el éxito de una teoría científica es si lo que dice sobre entidades observables es cierto.
Cada paradigma tiene sus propias preguntas, objetivos e interpretaciones.
Es decir, la elección de un nuevo paradigma se basa en observaciones, a pesar de que esas observaciones se hacen en el contexto del viejo paradigma.
Su punto principal es que se debe hacer una diferencia entre las explicaciones naturales y sobrenaturales y que la ciencia debe restringirse metodológicamente a las explicaciones naturales.
Es decir, ninguna teoría se considera estrictamente cierta ya que la ciencia acepta el concepto de falibilismo.
El nuevo conocimiento científico rara vez resulta en grandes cambios en nuestra comprensión.
El conocimiento en la ciencia se obtiene mediante una síntesis gradual de información de diferentes experimentos de varios investigadores en diferentes ramas de la ciencia; es más como una escalada que un salto.
El filósofo Barry Stroud agrega que, aunque la mejor definición para "conocimiento" es impugnada, ser escéptico y entretener la posibilidad de que uno sea incorrecto es compatible con ser correcto.
Este es especialmente el caso en los campos más macroscópicos de la ciencia (por ejemplo, la psicología, la cosmología física).
Desde entonces, el número total de publicaciones periódicas activas ha aumentado constantemente.
Aunque las revistas están en 39 idiomas, el 91 por ciento de los artículos indexados se publican en inglés.
Las revistas científicas como New Scientist, Science & Vie y Scientific American satisfacen las necesidades de un número de lectores mucho más amplio y proporcionan un resumen no técnico de áreas populares de investigación, incluidos descubrimientos notables y avances en ciertos campos de investigación.
Varios tipos de publicidad comercial, que van desde el bombo hasta el fraude, pueden caer en estas categorías.
Muchos científicos siguen carreras en diversos sectores de la economía, como la academia, la industria, el gobierno y las organizaciones sin fines de lucro.
Por ejemplo, Christine Ladd (1847-1930) pudo ingresar a un programa de doctorado como "C. Ladd"; Christine "Kitty" Ladd completó los requisitos en 1882, pero recibió su título solo en 1926, después de una carrera que abarcó el álgebra de la lógica (ver tabla de la verdad), la visión del color y la psicología.
A finales del siglo XX, el reclutamiento activo de mujeres y la eliminación de la discriminación institucional basada en el sexo aumentaron en gran medida el número de mujeres científicas, pero siguen existiendo grandes disparidades de género en algunos campos; a principios del siglo XXI, más de la mitad de los nuevos biólogos eran mujeres, mientras que el 80% de los doctorados en física se otorgan a los hombres.
La membresía puede estar abierta a todos, puede requerir la posesión de algunas credenciales científicas o puede ser un honor conferido por elección.
Por lo tanto, la política científica se ocupa de todo el dominio de las cuestiones que involucran a las ciencias naturales.
Los ejemplos históricos prominentes incluyen la Gran Muralla de Chína, completada en el transcurso de dos milenios a través del apoyo estatal de varias dinastías, y el Gran Canal del río Yangtze, una inmensa hazaña de ingeniería hidráulica iniciada por Sunshu Ao (el 7mo siglo.
Tales procesos, que son administrados por el gobierno, corporaciones o fundaciones, asignan fondos escasos.
La proporción de financiación del gobierno en ciertas industrias es mayor, y domina la investigación en ciencias sociales y humanidades.
Muchos factores pueden actuar como facetas de la politización de la ciencia, como el antiintelectualismo populista, las amenazas percibidas a las creencias religiosas, el subjetivismo posmodernista y el miedo a los intereses comerciales.
Un experimento es un procedimiento llevado a cabo para apoyar o refutar una hipótesis.
Los experimentos pueden aumentar los puntajes de las pruebas y ayudar a un estudiante a estar más comprometido e interesado en el material que está aprendiendo, especialmente cuando se usa con el tiempo.
Los experimentos típicamente incluyen controles, que están diseñados para minimizar los efectos de variables distintas de la variable independiente única.
Los investigadores también utilizan la experimentación para probar teorías existentes o nuevas hipótesis para apoyarlas o refutarlas.
Si un experimento se lleva a cabo cuidadosamente, los resultados generalmente apoyan o refutan la hipótesis.
En medicina y ciencias sociales, la prevalencia de la investigación experimental varía ampliamente entre disciplinas.
Un solo estudio normalmente no implica repeticiones del experimento, pero los estudios separados pueden agregarse a través de la revisión sistemática y el metanálisis.
Podemos llegar así a la verdad que gratifica el corazón y llegar gradual y cuidadosamente al final en el que aparece la certeza; mientras que a través de la crítica y la cautela podemos aprovechar la verdad que disipa el desacuerdo y resuelve asuntos dudosos.
En este proceso de consideración crítica, el hombre mismo no debe olvidar que tiende a las opiniones subjetivas, a través de "prejuicios" y "cleniencia", y por lo tanto tiene que ser crítico sobre su propia forma de construir hipótesis.
Bacon quería un método que se basara en observaciones o experimentos repetibles.
Por ejemplo, Galileo Galilei (1564-1642) midió con precisión el tiempo y experimentó para hacer mediciones precisas y conclusiones sobre la velocidad de un cuerpo que caía.
En algunas disciplinas (p. ej., psicología o ciencias políticas), un “verdadero experimento” es un método de investigación social en el que hay dos tipos de variables.
Un buen ejemplo sería un ensayo de drogas.
Los resultados de las muestras replicadas a menudo pueden promediarse, o si una de las réplicas es obviamente inconsistente con los resultados de las otras muestras, puede descartarse como el resultado de un error experimental (alguna etapa del procedimiento de prueba puede haberse omitido erróneamente para esa muestra).
Se sabe que un control negativo da un resultado negativo.
Muy a menudo, el valor del control negativo se trata como un valor de "fondo" para restar de los resultados de la muestra de prueba.
Los estudiantes pueden recibir una muestra de líquido que contiene una cantidad desconocida (para el estudiante) de proteína.
Los estudiantes podrían hacer varias muestras de control positivo que contienen varias diluciones del estándar de proteína.
El ensayo es un ensayo colorimétrico en el que un espectrofotómetro puede medir la cantidad de proteína en muestras detectando un complejo coloreado formado por la interacción de moléculas de proteína y moléculas de un colorante añadido.
En este caso, el experimento comienza creando dos o más grupos de muestra que son probabilísticamente equivalentes, lo que significa que las mediciones de rasgos deben ser similares entre los grupos y que los grupos deben responder de la misma manera si se les da el mismo tratamiento.
Una vez que se han formado grupos equivalentes, el experimentador intenta tratarlos de manera idéntica, excepto por la única variable que desea aislar.
Esto asegura que cualquier efecto en el voluntario se deba al tratamiento en sí y no sea una respuesta al conocimiento de que está siendo tratado.
Estas hipótesis sugieren razones para explicar un fenómeno o predecir los resultados de una acción.
La hipótesis nula es que no hay explicación o poder predictivo del fenómeno a través del razonamiento que se está investigando.
En la medida de lo posible, intentan recopilar datos para el sistema de tal manera que se pueda determinar la contribución de todas las variables, y donde los efectos de la variación en ciertas variables permanezcan aproximadamente constantes para que se puedan discernir los efectos de otras variables.
Por lo general, sin embargo, existe cierta correlación entre estas variables, lo que reduce la fiabilidad de los experimentos naturales en relación con lo que podría concluirse si se realizara un experimento controlado.
Por ejemplo, en astronomía es claramente imposible, al probar la hipótesis "Las estrellas son nubes colapsadas de hidrógeno", comenzar con una nube gigante de hidrógeno, y luego realizar el experimento de esperar unos pocos miles de millones de años para que forme una estrella.
Por esta razón, a veces se considera que los experimentos de campo tienen una validez externa más alta que los experimentos de laboratorio.
En estas situaciones, los estudios observacionales tienen valor porque a menudo sugieren hipótesis que pueden probarse con experimentos aleatorios o recolectando datos nuevos.
Además, los estudios observacionales (por ejemplo, en sistemas biológicos o sociales) a menudo implican variables que son difíciles de cuantificar o controlar.
Sin un modelo estadístico que refleje una aleatorización objetiva, el análisis estadístico se basa en un modelo subjetivo.
Por ejemplo, los estudios epidemiológicos del cáncer de colon muestran consistentemente correlaciones beneficiosas con el consumo de brócoli, mientras que los experimentos no encuentran ningún beneficio.
Para cualquier ensayo aleatorio, se espera alguna variación de la media, por supuesto, pero la aleatorización asegura que los grupos experimentales tienen valores medios que están cerca, debido al teorema del límite central y la desigualdad de Markov.
Para evitar condiciones que hacen que un experimento sea mucho menos útil, los médicos que realizan ensayos médicos, por ejemplo, para la aprobación de la Administración de Alimentos y Medicamentos de los Estados Unidos, cuantifican y aleatorizan las covariables que se pueden identificar.
También es generalmente poco ético (y a menudo ilegal) llevar a cabo experimentos aleatorios sobre los efectos de los tratamientos deficientes o dañinos, como los efectos de la ingestión de arsénico en la salud humana.
Un laboratorio de física podría contener un acelerador de partículas o una cámara de vacío, mientras que un laboratorio de metalurgia podría tener aparatos para fundir o refinar metales o para probar su resistencia.
Los científicos de otros campos utilizarán otros tipos de laboratorios.
A pesar de la noción subyacente del laboratorio como un espacio confinado para expertos, el término "laboratorio" también se aplica cada vez más a espacios de talleres como Living Labs, Fab Labs o Hackerspaces, en los que las personas se reúnen para trabajar en problemas sociales o hacer prototipos, trabajar en colaboración o compartir recursos.
Este laboratorio fue creado cuando Pitágoras realizó un experimento sobre los tonos de sonido y la vibración de las cuerdas.
Un laboratorio alquímico subterráneo del siglo XVI fue descubierto accidentalmente en el año 2002.
Los riesgos de laboratorio pueden incluir venenos; agentes infecciosos; materiales inflamables, explosivos o radiactivos; maquinaria en movimiento; temperaturas extremas; láseres, campos magnéticos fuertes o alto voltaje.
La Administración de Seguridad y Salud Ocupacional (OSHA) en los Estados Unidos, reconociendo las características únicas del lugar de trabajo del laboratorio, ha diseñado un estándar para la exposición ocupacional a productos químicos peligrosos en los laboratorios.
Al determinar el plan de higiene química adecuado para un negocio o laboratorio en particular, es necesario comprender los requisitos de la norma, la evaluación de las prácticas actuales de seguridad, salud y medio ambiente y la evaluación de los peligros.
Además, la revisión de terceros también se utiliza para proporcionar una "visión externa" objetiva que proporciona una nueva visión de las áreas y problemas que pueden darse por sentado o pasarse por alto debido al hábito.
La capacitación es fundamental para la operación segura en curso de las instalaciones de laboratorio.
Por ejemplo, un grupo de investigación tiene un cronograma en el que realizan investigaciones sobre su propio tema de interés para un día de la semana, pero para el resto trabajan en un proyecto de grupo determinado.
Un localizador es un empleado de un laboratorio que se encarga de saber dónde está cada miembro del laboratorio actualmente, en función de una señal única emitida por la insignia de cada miembro del personal.
A través de estudios etnográficos, un hallazgo es que, entre el personal, cada clase (investigadores, administradores ...) tiene un grado diferente de derecho, que varía según el laboratorio.
Al observar las diversas interacciones entre los miembros del personal, podemos determinar su posición social en la organización.
Entonces, una consecuencia de esta jerarquía social es que el Localizador revela varios grados de información, según el miembro del personal y sus derechos.
La jerarquía social también está relacionada con las actitudes hacia las tecnologías.
Por ejemplo, una recepcionista vería la insignia como útil, ya que les ayudaría a localizar a los miembros del personal durante el día.
Los miembros del personal se sienten mal cuando cambian los patrones de derecho, obligación, respeto, jerarquía informal y formal, y más.
La naturaleza, en el sentido más amplio, es el mundo natural, físico, material o universo.
Aunque los humanos son parte de la naturaleza, la actividad humana a menudo se entiende como una categoría separada de otros fenómenos naturales.
El concepto de la naturaleza en su conjunto, el universo físico, es una de las varias expansiones de la noción original; comenzó con ciertas aplicaciones centrales de la palabra por los filósofos presocráticos (aunque esta palabra tenía una dimensión dinámica entonces, especialmente para Heráclito), y ha ganado popularidad desde entonces.
Sin embargo, una visión vitalista de la naturaleza, más cercana a la presocrático, renació al mismo tiempo, especialmente después de Charles Darwin.
A menudo se entiende por “entorno natural” o desierto: animales salvajes, rocas, bosques y, en general, aquellas cosas que no han sido sustancialmente alteradas por la intervención humana o que persisten a pesar de la intervención humana.
Sus características climáticas más prominentes son sus dos grandes regiones polares, dos zonas templadas relativamente estrechas y una amplia región tropical ecuatorial a subtropical.
El resto se compone de continentes e islas, con la mayor parte de la tierra habitada en el hemisferio norte.
El interior permanece activo, con una gruesa capa de manto de plástico y un núcleo lleno de hierro que genera un campo magnético.
Las unidades de roca se colocan primero por deposición sobre la superficie o se entrometen en la roca superpuesta.
La desgasificación y la actividad volcánica produjeron la atmósfera primordial.
Los continentes se formaron, luego se rompieron y se reformaron a medida que la superficie de la Tierra se remodelaba a lo largo de cientos de millones de años, ocasionalmente combinándose para formar un supercontinente.
Durante la era neoproterozoica, las temperaturas heladas cubrieron gran parte de la Tierra en glaciares y capas de hielo.
La última extinción masiva ocurrió hace unos 66 millones de años, cuando una colisión de meteoritos probablemente desencadenó la extinción de los dinosaurios no aviares y otros reptiles grandes, pero salvó a animales pequeños como los mamíferos.
El advenimiento posterior de la vida humana, y el desarrollo de la agricultura y la civilización posterior permitió a los humanos afectar a la Tierra más rápidamente que cualquier forma de vida anterior, afectando tanto a la naturaleza y la cantidad de otros organismos, así como al clima global.
La delgada capa de gases que envuelve a la Tierra se mantiene en su lugar por la gravedad.
La capa de ozono juega un papel importante en el agotamiento de la cantidad de radiación ultravioleta (UV) que llega a la superficie.
El clima terrestre ocurre casi exclusivamente en la parte inferior de la atmósfera, y sirve como un sistema convectivo para redistribuir el calor.
Además, sin las redistribuciones de la energía térmica por las corrientes oceánicas y la atmósfera, los trópicos serían mucho más cálidos y las regiones polares mucho más frías.
La vegetación superficial ha evolucionado dependiendo de la variación estacional del clima, y los cambios repentinos que duran solo unos pocos años pueden tener un efecto dramático, tanto en la vegetación como en los animales que dependen de su crecimiento para su alimentación.
Según los registros históricos, se sabe que la Tierra ha sufrido cambios climáticos drásticos en el pasado, incluidas las edades de hielo.
Hay una serie de tales regiones, que van desde el clima tropical en el ecuador hasta el clima polar en los extremos norte y sur.
Esta exposición se alterna a medida que la Tierra gira en su órbita.
El agua cubre el 71% de la superficie de la Tierra.
Las regiones más pequeñas de los océanos se llaman mares, golfos, bahías y otros nombres.
No se sabe si los lagos de Titán son alimentados por ríos, aunque la superficie de Titán está tallada por numerosos lechos de ríos.
Una amplia variedad de cuerpos de agua artificiales se clasifican como estanques, incluidos jardines de agua diseñados para la ornamentación estética, estanques de peces diseñados para la cría comercial de peces y estanques solares diseñados para almacenar energía térmica.
Los ríos pequeños también pueden ser llamados por varios otros nombres, incluyendo arroyo, arroyo, riachuelo y riachuelo; no hay una regla general que defina lo que se puede llamar un río.
La estructura y la composición están determinadas por diversos factores ambientales que están interrelacionados.
El concepto central del ecosistema es la idea de que los organismos vivos interactúan con todos los demás elementos de su entorno local.
También se puede decir que la vida es simplemente el estado característico de los organismos.
Sin embargo, no todas las definiciones de vida consideran que todas estas propiedades sean esenciales.
Desde el punto de vista geofisiológico más amplio, la biosfera es el sistema ecológico global que integra a todos los seres vivos y sus relaciones, incluida su interacción con los elementos de la litosfera (rocas), la hidrosfera (agua) y la atmósfera (aire).
Hasta la fecha se han identificado más de 2 millones de especies de plantas y animales, y las estimaciones del número real de especies existentes oscilan entre varios millones y más de 50 millones.
Las especies que no pudieron adaptarse al entorno cambiante y la competencia de otras formas de vida se extinguieron.
Cuando las formas básicas de la vida vegetal desarrollaron el proceso de fotosíntesis, la energía del sol podría ser cosechada para crear condiciones que permitieran formas de vida más complejas.
Los microorganismos son organismos unicelulares que generalmente son microscópicos y más pequeños de lo que el ojo humano puede ver.
Su reproducción es rápida y profusa.
Desde entonces, ha quedado claro que las Plantas, tal como se definieron originalmente, incluían varios grupos no relacionados, y los hongos y varios grupos de algas fueron trasladados a nuevos reinos.
Entre las muchas formas de clasificar las plantas están las floras regionales, que, dependiendo del propósito del estudio, también pueden incluir flora fósil, restos de vida vegetal de una época anterior.
Algunos tipos de "flora nativa" en realidad han sido introducidos hace siglos por personas que migran de una región o continente a otro, y se convierten en una parte integral de la flora nativa o natural del lugar en el que fueron introducidos.
Los animales como categoría tienen varias características que generalmente los distinguen de otros seres vivos.
También se distinguen de las plantas, algas y hongos por carecer de paredes celulares.
También suele haber una cámara digestiva interna.
Un estudio de 2020 publicado en Nature encontró que la masa antropogénica (materiales hechos por el hombre) supera a toda la biomasa viva en la tierra, con plástico solo que excede la masa de todos los animales terrestres y marinos combinados.
A pesar de este progreso, sin embargo, el destino de la civilización humana sigue estrechamente ligado a los cambios en el medio ambiente.
Los seres humanos han contribuido a la extinción de muchas plantas y animales, con aproximadamente 1 millón de especies amenazadas de extinción en décadas.
Esto distorsiona los precios de mercado de los recursos naturales y, al mismo tiempo, conduce a una inversión insuficiente en nuestros activos naturales.
Los gobiernos no han impedido estas externalidades económicas.
Algunas actividades, como la caza y la pesca, se utilizan tanto para el sustento como para el ocio, a menudo por diferentes personas.
Esa naturaleza ha sido representada y celebrada por tanto arte, fotografía, poesía y otra literatura muestra la fuerza con la que muchas personas asocian la naturaleza y la belleza.
La naturaleza y la naturaleza salvaje han sido temas importantes en varias épocas de la historia mundial.
Aunque las maravillas naturales se celebran en los Salmos y el Libro de Job, las representaciones de la naturaleza en el arte se hicieron más frecuentes en el siglo XIX, especialmente en las obras del movimiento romántico.
Por esta razón, la ciencia más fundamental se entiende generalmente como "física", cuyo nombre aún es reconocible por el significado de que es el "estudio de la naturaleza".
Los componentes visibles del universo ahora se cree que componen sólo el 4,9 por ciento de la masa total.
El comportamiento de la materia y la energía en todo el universo observable parece seguir leyes físicas bien definidas.
No hay un límite discreto entre la atmósfera de la Tierra y el espacio, ya que la atmósfera se atenúa gradualmente con el aumento de la altitud.
También hay algo de gas, plasma y polvo, y pequeños meteoros.
Aunque la Tierra es el único cuerpo dentro del sistema solar conocido por albergar vida, la evidencia sugiere que en el pasado lejano el planeta Marte poseía cuerpos de agua líquida en la superficie.
Si la vida existe en Marte, lo más probable es que se encuentre bajo tierra, donde todavía puede existir agua líquida.
La observación es la adquisición activa de información de una fuente primaria.
El uso de la medición desarrollada para permitir el registro y la comparación de las observaciones realizadas en diferentes momentos y lugares, por diferentes personas.
En la medición se cuenta el número de unidades estándar que es igual a la observación.
Los instrumentos científicos se desarrollaron para ayudar a capacidades humanas de la observación, como balanzas, relojes, telescopios, microscopios, termómetros, cámaras y grabadoras, y también traducen en acontecimientos de la forma perceptibles que son inobservables por los sentidos, como tintes del indicador, voltímetros, espectrómetros, cámaras infrarrojas, osciloscopios, interferómetros, contadores del geiger y receptores de la radio.
Por ejemplo, normalmente no es posible comprobar la presión de aire en un neumático de automóvil sin dejar salir algo de aire, cambiando así la presión.
Por ejemplo, en la paradoja de los gemelos, un gemelo se va de viaje cerca de la velocidad de la luz y llega a casa más joven que el gemelo que se quedó en casa.
En la mecánica cuántica, que se ocupa del comportamiento de objetos muy pequeños, no es posible observar un sistema sin cambiar el sistema, y el "observador" debe considerarse parte del sistema que se está observando.
La percepción humana se produce por un proceso complejo e inconsciente de abstracción, en el que ciertos detalles de los datos sensoriales entrantes se notan y recuerdan, y el resto se olvidan.
Más tarde, cuando se recuerdan los eventos, los vacíos de memoria pueden incluso llenarse con datos "plausibles" que la mente compensa para adaptarse al modelo; esto se llama memoria reconstructiva.
En psicología, esto se llama sesgo de confirmación.
Por ejemplo, supongamos que un observador ve a un padre golpear a su hijo; y en consecuencia puede observar que tal acción es buena o mala.
La investigación es un trabajo creativo y sistemático emprendido para aumentar el stock de conocimiento.
Para probar la validez de los instrumentos, procedimientos o experimentos, la investigación puede replicar elementos de proyectos anteriores o el proyecto en su conjunto.
Este material es de carácter de fuente primaria.
En el trabajo experimental, por lo general implica la observación directa o indirecta de los sujetos investigados, por ejemplo, en el laboratorio o en el campo, documenta la metodología, los resultados y las conclusiones de un experimento o conjunto de experimentos, u ofrece una nueva interpretación de los resultados anteriores.
El grado de originalidad de la investigación se encuentra entre los principales criterios para que los artículos se publiquen en revistas académicas y generalmente se establezcan mediante revisión por pares.
Esta investigación proporciona información científica y teorías para la explicación de la naturaleza y las propiedades del mundo.
La investigación científica se puede subdividir en diferentes clasificaciones según sus disciplinas académicas y de aplicación.
Los estudiosos de humanidades generalmente no buscan la respuesta correcta definitiva a una pregunta, sino que exploran los problemas y detalles que la rodean.
Los historiadores usan fuentes primarias y otra evidencia para investigar sistemáticamente un tema, y luego escribir historias en forma de relatos del pasado.
La investigación deberá justificarse vinculando su importancia al conocimiento ya existente sobre el tema.
En general, una hipótesis se utiliza para hacer predicciones que se pueden probar mediante la observación de los resultados de un experimento.
Este lenguaje cuidadoso se utiliza porque los investigadores reconocen que las hipótesis alternativas también pueden ser consistentes con las observaciones.
A medida que la precisión de la observación mejora con el tiempo, la hipótesis ya no puede proporcionar una predicción precisa.
La investigación artística ha sido definida por la Escuela de Danza y Circo de Estocolmo de la siguiente manera: "La investigación artística es investigar y probar con el propósito de obtener conocimiento dentro y para nuestras disciplinas artísticas.
La investigación artística tiene como objetivo mejorar el conocimiento y la comprensión con la presentación de las artes.
Según el artista Hakan Topal, en la investigación artística, "quizás más que otras disciplinas, la intuición se utiliza como un método para identificar una amplia gama de modalidades productivas nuevas e inesperadas".
La investigación de antecedentes podría incluir, por ejemplo, la investigación geográfica o de procedimiento.
La revisión de la literatura identifica fallas o agujeros en investigaciones anteriores que proporcionan justificación para el estudio.
La pregunta de investigación puede ser paralela a la hipótesis.
Luego, el investigador analiza e interpreta los datos a través de una variedad de métodos estadísticos, participando en lo que se conoce como investigación empírica.
Sin embargo, algunos investigadores abogan por el enfoque inverso: comenzando con la articulación de los hallazgos y la discusión de ellos, avanzando hacia la identificación de un problema de investigación que surge en los hallazgos y la revisión de la literatura.
La investigación cualitativa a menudo se usa como un método de investigación exploratoria como base para hipótesis de investigación cuantitativa posteriores.
La investigación cuantitativa está vinculada con la postura filosófica y teórica del positivismo.
La investigación cuantitativa se refiere a probar hipótesis derivadas de la teoría o ser capaz de estimar el tamaño de un fenómeno de interés.
Si la intención es generalizar de los participantes de la investigación a una población más grande, el investigador empleará el muestreo de probabilidad para seleccionar a los participantes.
Los datos secundarios son datos que ya existen, como los datos del censo, que pueden reutilizarse para la investigación.
Este método tiene beneficios que el uso de un solo método no puede ofrecer.
La investigación no empírica no es una alternativa absoluta a la investigación empírica porque se pueden usar juntos para fortalecer un enfoque de investigación.
La gestión de la ética de la investigación es inconsistente en todos los países y no existe un enfoque universalmente aceptado sobre cómo debe abordarse.
Independientemente del enfoque, la aplicación de la teoría ética a temas controvertidos específicos se conoce como ética aplicada y la ética de la investigación puede verse como una forma de ética aplicada porque la teoría ética se aplica en escenarios de investigación del mundo real.
La ética de la investigación es más desarrollada como un concepto en la investigación médica, siendo el código más notable la Declaración de Helsinki de 1964.
La meta-investigación se ocupa de la detección de sesgos, fallas metodológicas y otros errores e ineficiencias.
Los estudiosos de la periferia se enfrentan a los desafíos de la exclusión y el linguismo en la investigación y la publicación académica.
Para la política comparativa, los países occidentales están sobre-representados en estudios de un solo país, con gran énfasis en Europa Occidental, Canadá, Australia y Nueva Zelanda.
Los estudios con un alcance estrecho pueden dar lugar a una falta de generalización, lo que significa que los resultados pueden no ser aplicables a otras poblaciones o regiones.
Por lo general, el proceso de revisión por pares involucra a expertos en el mismo campo que son consultados por los editores para dar una revisión de los trabajos académicos producidos por un colega de ellos desde un punto de vista imparcial e imparcial, y esto generalmente se hace de forma gratuita.
Por ejemplo, la mayoría de las comunidades indígenas consideran que el acceso a cierta información propia del grupo debe estar determinado por las relaciones.
El sistema varía ampliamente según el campo y también siempre está cambiando, aunque a menudo lentamente.
Estas formas de investigación se pueden encontrar en bases de datos explícitamente para tesis y disertaciones.
Los tipos de publicaciones que se aceptan como contribuciones de conocimiento o investigación varían mucho entre los campos, desde la impresión hasta el formato electrónico.
Los modelos de negocio son diferentes en el entorno electrónico.
Muchos investigadores de alto nivel (como los líderes de grupo) pasan una cantidad significativa de su tiempo solicitando subvenciones para fondos de investigación.
El método científico es un método empírico para adquirir conocimiento que ha caracterizado el desarrollo de la ciencia desde al menos el siglo XVII (con practicantes notables en siglos anteriores).
Estos son principios del método científico, a diferencia de una serie definitiva de pasos aplicables a todas las empresas científicas.
Una hipótesis es una conjetura, basada en el conocimiento obtenido mientras se buscan respuestas a la pregunta.
Sin embargo, hay dificultades en una declaración fórmula del método.
El término "método científico" surgió en el siglo XIX, cuando se estaba produciendo un importante desarrollo institucional de la ciencia y aparecieron terminologías que establecían límites claros entre la ciencia y la no ciencia, como "científico" y "pseudociencia".
Gauch 2003 y Tow 2010 no están de acuerdo con la afirmación de Feyerabend; los solucionadores de problemas y los investigadores deben ser prudentes con sus recursos durante su investigación.
Los filósofos Robert Nola y Howard Sankey, en su libro de 2007 Theories of Scientific Method, dijeron que los debates sobre el método científico continúan, y argumentaron que Feyerabend, a pesar del título de Against Method, aceptó ciertas reglas de método e intentó justificar esas reglas con una metametodología.
El elemento omnipresente en el método científico es el empirismo.
El método científico contrarresta las afirmaciones de que la revelación, el dogma político o religioso, apela a la tradición, las creencias comunes, el sentido común o las teorías actualmente sostenidas plantean el único medio posible de demostrar la verdad.
A partir del 16to siglo adelante, los experimentos fueron defendidos por Francis Bacon y realizados por Giambattista della Porta, Johannes Kepler y Galileo Galilei.
Al igual que en otras áreas de investigación, la ciencia (a través del método científico) puede basarse en el conocimiento previo y desarrollar una comprensión más sofisticada de sus temas de estudio a lo largo del tiempo.
Se puede ver que este modelo es la base de la revolución científica.: "
Una conjetura podría ser que un nuevo medicamento curará la enfermedad en algunas de las personas en esa población, como en un ensayo clínico del medicamento.
Estas predicciones son expectativas para los resultados de las pruebas.
La diferencia entre lo esperado y lo real indica qué hipótesis explica mejor los datos resultantes del experimento.
Dependiendo de la complejidad del experimento, la iteración del proceso puede ser necesaria para reunir pruebas suficientes para responder a la pregunta con confianza, o para construir otras respuestas a preguntas muy específicas, para responder a una sola pregunta más amplia.
Los patrones de difracción de rayos X del ADN de Florence Bell en su tesis doctoral (1939) fueron similares a (aunque no tan buenos como) "foto 51", pero esta investigación fue interrumpida por los acontecimientos de la Segunda Guerra Mundial.
Junio de 1952 - Watson había tenido éxito en la obtención de imágenes de rayos X de TMV que muestran un patrón de difracción consistente con la transformación de una hélice.
Esta predicción era una construcción matemática, completamente independiente del problema biológico en cuestión.
El ADN no es una hélice”.
Por ejemplo, el número de hebras en la columna vertebral de la hélice (Crick sospechaba 2 hebras, pero advirtió a Watson que examinara eso más críticamente), la ubicación de los pares de bases (dentro de la columna vertebral o fuera de la columna vertebral), etc.
Pero Wilkins se compromete a hacerlo sólo después de la partida de Franklin.
Él y Crick luego produjeron su modelo, utilizando esta información junto con la información previamente conocida sobre la composición del ADN, especialmente las reglas de emparejamiento de bases de Chargaff.
Para obtener resultados significativos o sorprendentes, otros científicos también pueden intentar replicar los resultados por sí mismos, especialmente si esos resultados serían importantes para su propio trabajo.
La revisión por pares no certifica la exactitud de los resultados, solo que, en opinión del revisor, los experimentos en sí eran sólidos (basándose en la descripción proporcionada por el experimentador).
Estos elementos metodológicos y la organización de los procedimientos tienden a ser más característicos de las ciencias experimentales que de las ciencias sociales.
Los elementos anteriores a menudo se enseñan en el sistema educativo como "el método científico".
Por ejemplo, cuando Einstein desarrolló las Teorías Especiales y Generales de la Relatividad, de ninguna manera refutó o descartó los Principia de Newton.
La colección sistemática y cuidadosa de mediciones o recuentos de cantidades relevantes es a menudo la diferencia crítica entre las pseudociencias, como la alquimia, y la ciencia, como la química o la biología.
Las incertidumbres también pueden calcularse teniendo en cuenta las incertidumbres de las cantidades subyacentes individuales utilizadas.
La definición operativa de una cosa a menudo se basa en comparaciones con los estándares: la definición operativa de "masa" en última instancia se basa en el uso de un artefacto, como un kilogramo particular de platino-iridio mantenido en un laboratorio en Francia.
Las cantidades científicas a menudo se caracterizan por sus unidades de medida que luego se pueden describir en términos de unidades físicas convencionales al comunicar el trabajo.
Se necesitaron miles de años de mediciones, de los astrónomos caldeos, indios, persas, griegos, árabes y europeos, para registrar completamente el movimiento del planeta Tierra.
La diferencia observada para la precesión de Mercurio entre la teoría newtoniana y la observación fue una de las cosas que se le ocurrió a Albert Einstein como una posible prueba temprana de su teoría de la relatividad general.
Los científicos son libres de usar cualquier recurso que tengan: su propia creatividad, ideas de otros campos, razonamiento inductivo, inferencia bayesiana, etc., para imaginar posibles explicaciones para un fenómeno bajo estudio.
Los científicos a menudo usan estos términos para referirse a una teoría que sigue los hechos conocidos, pero sin embargo es relativamente simple y fácil de manejar.
Es esencial que el resultado de probar tal predicción sea actualmente desconocido.
Si las predicciones no son accesibles por la observación o la experiencia, la hipótesis aún no es comprobable y por lo tanto permanecerá en esa medida no científica en un sentido estricto.
Esto implicaba que el patrón de difracción de rayos X del ADN tendría "forma de x".
A veces, los experimentos se llevan a cabo incorrectamente o no están muy bien diseñados en comparación con un experimento crucial.
Esta técnica utiliza el contraste entre múltiples muestras, u observaciones, o poblaciones, en diferentes condiciones, para ver qué varía o qué permanece igual.
El análisis factorial es una técnica para descubrir el factor importante en un efecto.
Incluso tomar un avión de Nueva York a París es un experimento que prueba las hipótesis aerodinámicas utilizadas para construir el avión.
Franklin inmediatamente descubrió los defectos que se referían al contenido de agua.
El fracaso en desarrollar una hipótesis interesante puede llevar a un científico a redefinir el tema bajo consideración.
Otros científicos pueden comenzar su propia investigación y entrar en el proceso en cualquier etapa.
Crucialmente, los resultados experimentales y teóricos deben ser reproducidos por otros dentro de la comunidad científica.
Cuanto mejor sea una explicación para hacer predicciones, más útil puede ser con frecuencia, y más probable es que continúe explicando un cuerpo de evidencia mejor que sus alternativas.
Los modelos científicos varían en la medida en que han sido probados experimentalmente y por cuánto tiempo, y en su aceptación en la comunidad científica.
Si se encuentra tal evidencia, se puede proponer una nueva teoría, o (más comúnmente) se encuentra que las modificaciones a la teoría anterior son suficientes para explicar la nueva evidencia.
Por ejemplo, las leyes de Newton explicaron miles de años de observaciones científicas de los planetas casi perfectamente.
Dado que las nuevas teorías podrían ser más completas que lo que las precedió, y por lo tanto ser capaces de explicar más que las anteriores, las teorías sucesoras podrían ser capaces de cumplir con un estándar más alto al explicar un cuerpo de observaciones más grande que sus predecesores.
Una vez que se ha formado un sistema estructuralmente completo y cerrado de opiniones que consta de muchos detalles y relaciones, ofrece una resistencia duradera a cualquier cosa que lo contradiga.
Sus éxitos pueden brillar, pero tienden a ser transitorios.
El método de lo a priori, que promueve la conformidad de manera menos brutal, pero fomenta las opiniones como algo parecido a los gustos, que surgen en la conversación y las comparaciones de perspectivas en términos de "lo que es agradable a la razón".
Ese es un destino tan lejano, o cercano, como la verdad misma para ti o para mí o para la comunidad finita dada.
De la abducción, Peirce distingue la inducción como inferir, basado en pruebas, la proporción de verdad en la hipótesis.
A menudo, incluso una mente bien preparada adivina mal.
Peirce, Charles S. (1902), solicitud de Carnegie, véase MS L75.329330, del Borrador D de la Memoria 27: "En consecuencia, descubrir es simplemente acelerar un evento que ocurriría tarde o temprano, si no nos hubiéramos preocupado por hacer el descubrimiento.
En consecuencia, la conducta de secuestro, que es principalmente una cuestión de heurético y es la primera cuestión de heurético, debe ser gobernada por consideraciones económicas.
La hipótesis, al ser insegura, debe tener implicaciones prácticas que conduzcan al menos a pruebas mentales y, en ciencia, se presten a pruebas científicas.
Einstein, Albert (1936, 1956) Se puede decir que “el misterio eterno del mundo es su comprensibilidad”.
Estos supuestos del naturalismo metodológico forman una base sobre la cual la ciencia puede basarse.
Sus observaciones de la práctica de la ciencia son esencialmente sociológicas y no hablan de cómo la ciencia es o puede ser practicada en otros tiempos y otras culturas.
Abre el capítulo 1 con una discusión de los cuerpos de Golgi y su rechazo inicial como un artefacto de la técnica de tinción, y una discusión de Brahe y Kepler observando el amanecer y viendo un amanecer "diferente" a pesar del mismo fenómeno fisiológico.
En esencia, dice que para cualquier método específico o norma de la ciencia, uno puede encontrar un episodio histórico en el que violarlo ha contribuido al progreso de la ciencia.
Las críticas posmodernas de la ciencia han sido objeto de intensa controversia.
Los modelos, tanto en la ciencia como en las matemáticas, deben ser internamente consistentes y también deben ser falsificables (capaces de refutar).
Por ejemplo, el concepto técnico del tiempo surgió en la ciencia, y la atemporalidad fue un sello distintivo de un tema matemático.
El artículo de Eugene Wigner, La efectividad irrazonable de las matemáticas en las ciencias naturales, es un relato muy conocido de la cuestión de un físico ganador del Premio Nobel.
En Pruebas y refutaciones, Lakatos dio varias reglas básicas para encontrar pruebas y contraejemplos a conjeturas.
Esto puede explicar por qué los científicos tan a menudo expresan que tuvieron suerte.
Mahwah, Nueva Jersey: Lawrence Erlbaum Associates.
Esto es lo que Nassim Nicholas Taleb llama "antifragilidad"; mientras que algunos sistemas de investigación son frágiles frente al error humano, el sesgo humano y la aleatoriedad, el método científico es más que resistente o difícil: en realidad se beneficia de tal aleatoriedad de muchas maneras (es antifrágil).
Estos resultados inesperados llevan a los investigadores a tratar de corregir lo que creen que es un error en su método.
Una teoría científica es una explicación de un aspecto del mundo natural y el universo que ha sido probado y verificado repetidamente de acuerdo con el método científico, utilizando protocolos aceptados de observación, medición y evaluación de resultados.
Las teorías científicas establecidas han resistido un escrutinio riguroso y encarnan el conocimiento científico.
Stephen Jay Gould escribió que "... los hechos y las teorías son cosas diferentes, no peldaños en una jerarquía de certeza creciente.
El significado del término teoría científica (a menudo contraído con la teoría por brevedad) como se usa en las disciplinas de la ciencia es significativamente diferente del uso común vernáculo de la teoría.
En el lenguaje cotidiano, la teoría puede implicar una explicación que representa una suposición infundada y especulativa, mientras que en la ciencia describe una explicación que ha sido probada y es ampliamente aceptada como válida.
Algunas teorías están tan bien establecidas que es poco probable que alguna vez cambien fundamentalmente (por ejemplo, teorías científicas como la evolución, la teoría heliocéntrica, la teoría celular, la teoría de la tectónica de placas, la teoría germinal de la enfermedad, etc.).
Las teorías científicas son comprobables y hacen predicciones falsables.
La característica definitoria de todo conocimiento científico, incluidas las teorías, es la capacidad de hacer predicciones falsables o comprobables.
Está bien apoyada por muchas líneas independientes de evidencia, en lugar de una sola base.
La teoría de la evolución biológica es más que "sólo una teoría".
Esto proporciona evidencia a favor o en contra de la hipótesis.
Esto puede llevar muchos años, ya que puede ser difícil o complicado reunir pruebas suficientes.
La fuerza de la evidencia es evaluada por la comunidad científica, y los experimentos más importantes habrán sido replicados por múltiples grupos independientes.
En química, hay muchas teorías ácido-base que proporcionan explicaciones muy divergentes de la naturaleza subyacente de los compuestos ácidos y básicos, pero son muy útiles para predecir su comportamiento químico.
La aceptación de una teoría no requiere que todas sus predicciones principales se prueben, si ya está respaldada por pruebas suficientemente sólidas.
Las soluciones pueden requerir cambios menores o mayores en la teoría, o ninguno en absoluto si se encuentra una explicación satisfactoria dentro del marco existente de la teoría.
Si las modificaciones a la teoría u otras explicaciones parecen ser insuficientes para dar cuenta de los nuevos resultados, entonces se puede requerir una nueva teoría.
Esto se debe a que sigue siendo la mejor explicación disponible para muchos otros fenómenos, como se verifica por su poder predictivo en otros contextos.
Después de los cambios, la teoría aceptada explicará más fenómenos y tendrá un mayor poder predictivo (si no lo hiciera, los cambios no serían adoptados); esta nueva explicación estará abierta a más reemplazo o modificación.
Por ejemplo, ahora se sabe que la electricidad y el magnetismo son dos aspectos del mismo fenómeno, denominado electromagnetismo.
Esto fue resuelto por el descubrimiento de la fusión nuclear, la principal fuente de energía del Sol.
Omitiendo de la relatividad especial el éter luminífero, Einstein declaró que la dilatación del tiempo y la contracción de la longitud medidas en un objeto en el movimiento relativo son inerciales, es decir, el objeto exhibe la velocidad constante, que es la velocidad con la dirección, cuando medido por su observador.
Einstein intentó generalizar el principio de invariancia a todos los marcos de referencia, ya sean inerciales o acelerados.
Incluso la energía sin masa ejerce movimiento gravitacional sobre objetos locales "curvando" la "superficie" geométrica del espacio-tiempo 4D.
Sin embargo, las leyes científicas son relatos descriptivos de cómo se comportará la naturaleza bajo ciertas condiciones.
Un error común es que las teorías científicas son ideas rudimentarias que eventualmente se graduarán en leyes científicas cuando se hayan acumulado suficientes datos y pruebas.
Tanto las teorías como las leyes podrían ser falsificadas por evidencia compensatoria.
La lógica de primer orden es un ejemplo de lenguaje formal.
Los fenómenos explicados por las teorías, si no podían ser observados directamente por los sentidos (por ejemplo, átomos y ondas de radio), fueron tratados como conceptos teóricos.
La frase "el punto de vista recibido de las teorías" se utiliza para describir este enfoque.
Se puede usar el lenguaje para describir un modelo; sin embargo, la teoría es el modelo (o una colección de modelos similares), y no la descripción del modelo.
Los parámetros del modelo, por ejemplo, la Ley de Gravitación de Newton, determinan cómo cambian las posiciones y las velocidades con el tiempo.
La palabra semántica se refiere a la forma en que un modelo representa el mundo real.
La práctica de la ingeniería hace una distinción entre "modelos matemáticos" y "modelos físicos"; el coste de fabricar un modelo físico se puede minimizar primero creando un modelo matemático usando un paquete del software del ordenador, tal como una herramienta de diseño asistida por el ordenador.
Ciertas suposiciones son necesarias para todas las afirmaciones empíricas (por ejemplo, la suposición de que la realidad existe).
Esto puede ser tan simple como observar que la teoría hace predicciones precisas, que es evidencia de que cualquier suposición hecha al principio es correcta o aproximadamente correcta bajo las condiciones probadas.
La teoría hace predicciones precisas cuando la suposición es válida, y no hace predicciones precisas cuando la suposición no es válida.
El Diccionario inglés de Oxford (OED) y Wiktionary en línea indican su fuente latina como assumere ("aceptar, tomar para sí, adoptar, usurpar"), que es una conjunción de ad- ("a, hacia, en") y sumere (to tomar).
El término se empleó originalmente en contextos religiosos como en "recibir al cielo", especialmente "la recepción de la Virgen María en el cielo, con el cuerpo preservado de la corrupción", (1297 CE), pero también se usó simplemente para referirse a "recibir en asociación" o "adoptar en asociación".
Las confirmaciones deberían contar sólo si son el resultado de predicciones arriesgadas; es decir, si, no iluminados por la teoría en cuestión, hubiéramos esperado un evento que fuera incompatible con la teoría, un evento que hubiera refutado la teoría.
Una teoría que no es refutable por ningún evento concebible no es científica.
Algunas teorías genuinamente comprobables, cuando se encuentran que son falsas, aún podrían ser sostenidas por sus admiradores, por ejemplo, introduciendo post hoc (después del hecho) alguna hipótesis o suposición auxiliar, o reinterpretando la teoría post hoc de tal manera que escape a la refutación.
Popper resumió estas declaraciones diciendo que el criterio central del estado científico de una teoría es su "falsificabilidad, o refutabilidad, o testabilidad".
Sin embargo, varios filósofos e historiadores de la ciencia han argumentado que la definición de teoría de Popper como un conjunto de declaraciones falsables es errónea porque, como Philip Kitcher ha señalado, si se tomara una visión estrictamente popperiana de la "teoría", las observaciones de Urano cuando se descubrieron por primera vez en 1781 habrían "falsificado" la mecánica celeste de Newton.
Fecundidad: "Una gran teoría científica, como la de Newton, abre nuevas áreas de investigación...
En cualquier momento, plantea más preguntas de las que actualmente puede responder.
Al igual que otras definiciones de teorías, incluida la de Popper, Kitcher deja en claro que una teoría debe incluir declaraciones que tengan consecuencias observacionales.
Puede establecerse en el papel como un sistema de reglas, y cuanto más verdaderamente una teoría más completamente se puede poner en tales términos.
Los aspectos matemáticos específicos de la teoría electromagnética clásica se denominan "leyes del electromagnetismo", que reflejan el nivel de evidencia consistente y reproducible que las respalda.
Un ejemplo de esto último podría ser la fuerza de reacción de radiación.
Un científico es una persona que realiza investigaciones científicas para avanzar en el conocimiento en un área de interés.
Los científicos de diferentes épocas (y antes que ellos, filósofos naturales, matemáticos, historiadores naturales, teólogos naturales, ingenieros y otros que contribuyeron al desarrollo de la ciencia) han tenido lugares muy diferentes en la sociedad, y las normas sociales, los valores éticos y las virtudes epistémicas asociadas con los científicos, y que se esperan de ellos, también han cambiado con el tiempo.
Muchos protocientíficos de la Edad de Oro islámica se consideran polímatas, en parte debido a la falta de algo que corresponda a las disciplinas científicas modernas.
Las proposiciones a las que se llega por medios puramente lógicos son completamente vacías en lo que respecta a la realidad.
Descartes no solo fue un pionero de la geometría analítica, sino que formuló una teoría de la mecánica e ideas avanzadas sobre los orígenes del movimiento y la percepción de los animales.
Proporcionó una formulación integral de la mecánica clásica e investigó la luz y la óptica.
Descubrió que una carga aplicada a la médula espinal de una rana podría generar espasmos musculares en todo su cuerpo.
Lazzaro Spallanzani es una de las figuras más influyentes en la fisiología experimental y las ciencias naturales.
Sin embargo, no hay un proceso formal para determinar quién es un científico y quién no lo es.
Un poco más de la mitad de los encuestados quería seguir una carrera en la academia, con proporciones más pequeñas con la esperanza de trabajar en la industria, el gobierno y los entornos sin fines de lucro.
Muestran una fuerte curiosidad por la realidad.
Algunos científicos tienen el deseo de aplicar el conocimiento científico en beneficio de la salud de las personas, las naciones, el mundo, la naturaleza o las industrias (científico académico y científico industrial).
Estos incluyen la cosmología y la biología, especialmente la biología molecular y el proyecto del genoma humano.
La cifra incluía el doble de hombres que de mujeres.
Los fenómenos relevantes incluyen explosiones de supernovas, explosiones de rayos gamma, cuásares, blazares, púlsares y radiación cósmica de fondo de microondas.
La astronomía es una de las ciencias naturales más antiguas.
En el pasado, la astronomía incluía disciplinas tan diversas como la astrometría, la navegación celeste, la astronomía observacional y la elaboración de calendarios.
La astronomía observacional se centra en la adquisición de datos a partir de observaciones de objetos astronómicos.
Estos dos campos se complementan entre sí.
Basado en definiciones del diccionario estrictas, "astronomía" se refiere al "estudio de objetos y materia fuera de la atmósfera de la Tierra y de sus propiedades físicas y químicas", mientras "astrofísica" se refiere a la rama de astronomía que trata con "el comportamiento, propiedades físicas y procesos dinámicos de objetos celestes y fenómenos".
Algunos campos, como la astrometría, son puramente astronómicos en lugar de astrofísicos.
A partir de estas observaciones, se formaron las primeras ideas sobre los movimientos de los planetas, y la naturaleza del Sol, la Luna y la Tierra en el Universo se exploraron filosóficamente.
Un desarrollo temprano particularmente importante era el principio de la astronomía matemática y científica, que comenzó entre los babilonios, que pusieron las fundaciones para las tradiciones astronómicas posteriores que se desarrollaron en muchas otras civilizaciones.
La astronomía griega se caracteriza desde el principio por buscar una explicación racional y física de los fenómenos celestes.
Hiparco también creó un catálogo completo de 1020 estrellas, y la mayoría de las constelaciones del hemisferio norte se derivan de la astronomía griega.
Georg von Peuerbach (1423-1461) y Regiomontanus (1436-1476) ayudaron a hacer que el progreso astronómico fuera instrumental para el desarrollo de Copérnico del modelo heliocéntrico décadas más tarde.
En 964, la galaxia de Andrómeda, la galaxia más grande en el Grupo Local, fue descrita por el astrónomo musulmán persa Abd al-Rahman al-Sufi en su Libro de Estrellas Fijas.
Los astrónomos durante ese tiempo introdujeron muchos nombres árabes que ahora se usan para estrellas individuales.
El historiador de Songhai Mahmud Kati documentó una lluvia de meteoritos en agosto de 1583.
Kepler fue el primero en diseñar un sistema que describiera correctamente los detalles del movimiento de los planetas alrededor del Sol.
El astrónomo inglés John Flamsteed catalogó más de 3000 estrellas, catálogos de estrellas más extensos fueron producidos por Nicolas Louis de Lacaille.
Este trabajo fue refinado por Joseph-Louis Lagrange y Pierre Simon Laplace, permitiendo estimar las masas de los planetas y lunas a partir de sus perturbaciones.
Se demostró que las estrellas eran similares al propio Sol de la Tierra, pero con una amplia gama de temperaturas, masas y tamaños.
La astronomía teórica condujo a especulaciones sobre la existencia de objetos como agujeros negros y estrellas de neutrones, que se han utilizado para explicar fenómenos observados como cuásares, púlsares, blazares y radiogalaxias.
La astronomía observacional puede clasificarse de acuerdo con la región correspondiente del espectro electromagnético en el que se realizan las observaciones.
Aunque algunas ondas de radio son emitidas directamente por objetos astronómicos, un producto de la emisión térmica, la mayor parte de la emisión de radio que se observa es el resultado de la radiación de sincrotrón, que se produce cuando los electrones orbitan los campos magnéticos.
Las observaciones del Wide-field Infrared Survey Explorer (WISE) han sido particularmente efectivas para revelar numerosas protoestrellas galácticas y sus cúmulos estelares anfitriones.
Las imágenes de las observaciones fueron originalmente dibujadas a mano.
La astronomía ultravioleta se adapta mejor al estudio de la radiación térmica y las líneas de emisión espectral de las estrellas azules calientes (estrellas OB) que son muy brillantes en esta banda de ondas.
Los rayos gamma pueden ser observados directamente por satélites como el Observatorio Compton de Rayos Gamma o por telescopios especializados llamados telescopios Cherenkov atmosféricos.
La astronomía de ondas gravitacionales es un campo emergente de la astronomía que emplea detectores de ondas gravitacionales para recopilar datos observacionales sobre objetos masivos distantes.
Históricamente, el conocimiento exacto de las posiciones del Sol, la Luna, los planetas y las estrellas ha sido esencial en la navegación celeste (el uso de objetos celestes para guiar la navegación) y en la fabricación de calendarios.
La medición de la paralaje estelar de estrellas cercanas proporciona una línea de base fundamental en la escala de distancia cósmica que se utiliza para medir la escala del Universo.
Los modelos analíticos de un proceso son mejores para dar una visión más amplia del corazón de lo que está sucediendo.
La observación de un fenómeno predicho por un modelo permite a los astrónomos seleccionar entre varios modelos alternativos o en conflicto como el más capaz de describir los fenómenos.
En algunos casos, una gran cantidad de datos inconsistentes a lo largo del tiempo puede llevar al abandono total de un modelo.
Debido a que la astrofísica es un tema muy amplio, los astrofísicos suelen aplicar muchas disciplinas de la física, incluyendo la mecánica, el electromagnetismo, la mecánica estadística, la termodinámica, la mecánica cuántica, la relatividad, la física nuclear y de partículas, y la física atómica y molecular.
La palabra "astroquímica" puede aplicarse tanto al Sistema Solar como al medio interestelar.
El término exobiología es similar.
Las observaciones de la estructura a gran escala del Universo, una rama conocida como cosmología física, han proporcionado una comprensión profunda de la formación y evolución del cosmos.
Una estructura jerárquica de la materia comenzó a formarse a partir de variaciones diminutas en la densidad de masa del espacio.
Las agregaciones gravitacionales se agrupan en filamentos, dejando vacíos en los huecos.
Diversos campos de la física son cruciales para estudiar el universo.
Finalmente, esto último es importante para la comprensión de la estructura a gran escala del cosmos.
Como su nombre indica, una galaxia elíptica tiene la forma transversal de una elipse.
Las galaxias elípticas se encuentran más comúnmente en el núcleo de los cúmulos galácticos, y pueden haberse formado a través de fusiones de grandes galaxias.
Las galaxias espirales están típicamente rodeadas por un halo de estrellas más viejas.
Alrededor de una cuarta parte de todas las galaxias son irregulares, y las formas peculiares de tales galaxias pueden ser el resultado de la interacción gravitacional.
Una galaxia de radio es una galaxia activa que es muy luminosa en la porción de radio del espectro, y está emitiendo inmensas plumas o lóbulos de gas.
La estructura a gran escala del cosmos está representada por grupos y cúmulos de galaxias.
En el centro de la Vía Láctea se encuentra el núcleo, una protuberancia en forma de barra con lo que se cree que es un agujero negro supermasivo en su centro.
El disco está rodeado por un halo esferoidal de estrellas más antiguas de la población II, así como concentraciones relativamente densas de estrellas conocidas como cúmulos globulares.
Estos comienzan como un núcleo preestelar compacto o nebulosas oscuras, que se concentran y colapsan (en volúmenes determinados por la longitud de Jeans) para formar protoestrellas compactas.
Estos cúmulos se dispersan gradualmente, y las estrellas se unen a la población de la Vía Láctea.
La formación de estrellas se produce en regiones densas de polvo y gas, conocidas como nubes moleculares gigantes.
Casi todos los elementos más pesados que el hidrógeno y el helio se crearon dentro de los núcleos de las estrellas.
Con el tiempo, este combustible de hidrógeno se convierte completamente en helio, y la estrella comienza a evolucionar.
La expulsión de las capas externas forma una nebulosa planetaria.
Esta es una oscilación de 11 años en el número de manchas solares.
El Sol también ha sufrido cambios periódicos en la luminosidad que pueden tener un impacto significativo en la Tierra.
Por encima de esta capa hay una región delgada conocida como la cromosfera.
Por encima del núcleo está la zona de radiación, donde el plasma transporta el flujo de energía por medio de radiación.
Un viento solar de partículas de plasma fluye constantemente hacia afuera desde el Sol hasta que, en el límite más externo del Sistema Solar, alcanza la heliopausa.
Los planetas se formaron hace 4.600 millones de años en el disco protoplanetario que rodeaba al Sol primitivo.
Los planetas continuaron barriendo, o expulsando, la materia restante durante un período de intenso bombardeo, evidenciado por los muchos cráteres de impacto en la Luna.
Este proceso puede formar un núcleo pétreo o metálico, rodeado por un manto y una corteza externa.
Algunos planetas y lunas acumulan suficiente calor para impulsar procesos geológicos como el vulcanismo y la tectónica.
La astroestadística es la aplicación de la estadística a la astrofísica para el análisis de una gran cantidad de datos astrofísicos observacionales.
La cosmoquímica es el estudio de las sustancias químicas que se encuentran dentro del Sistema Solar, incluidos los orígenes de los elementos y las variaciones en las proporciones de isótopos.
Los clubes de astronomía se encuentran en todo el mundo y muchos tienen programas para ayudar a sus miembros a establecer y completar programas de observación, incluidos los de observar todos los objetos en el Messier (110 objetos) o Herschel 400 catálogos de puntos de interés en el cielo nocturno.
La mayoría de los aficionados trabajan en longitudes de onda visibles, pero una pequeña minoría experimenta con longitudes de onda fuera del espectro visible.
Varios astrónomos aficionados usan telescopios caseros o usan radiotelescopios que se construyeron originalmente para la investigación astronómica pero que ahora están disponibles para los aficionados (por ejemplo, el Telescopio de una milla).
Las respuestas a estos pueden requerir la construcción de nuevos instrumentos terrestres y espaciales, y posiblemente nuevos desarrollos en física teórica y experimental.
Se necesita una comprensión más profunda de la formación de estrellas y planetas.
Si es así, ¿cuál es la explicación de la paradoja de Fermi?
¿Cuál es la naturaleza de la materia oscura y la energía oscura?
¿Cómo se formaron las primeras galaxias?
La astrobiología, anteriormente conocida como exobiología, es un campo científico interdisciplinario que estudia los orígenes, la evolución temprana, la distribución y el futuro de la vida en el universo.
El origen y la evolución temprana de la vida es una parte inseparable de la disciplina de la astrobiología.
La bioquímica pudo haber comenzado poco después del Big Bang, hace 13.800 millones de años, durante una época habitable cuando el Universo tenía solo 10-17 millones de años.
Sin embargo, la Tierra es el único lugar en el universo que los humanos conocen para albergar vida.
El término exobiología fue acuñado por el biólogo molecular y ganador del Premio Nobel Joshua Lederberg.
El término xenobiología ahora se usa en un sentido más especializado, para significar "biología basada en química extraña", ya sea de origen extraterrestre o terrestre (posiblemente sintético).
Aunque una vez considerado fuera de la corriente principal de la investigación científica, la astrobiología se ha convertido en un campo de estudio formalizado.
En 1959, la NASA financió su primer proyecto de exobiología, y en 1960, la NASA fundó un Programa de Exobiología, que ahora es uno de los cuatro elementos principales del actual Programa de Astrobiología de la NASA.
Los avances en los campos de la astrobiología, la astronomía observacional y el descubrimiento de grandes variedades de extremófilos con una capacidad extraordinaria para prosperar en los entornos más duros de la Tierra, han llevado a la especulación de que la vida puede estar prosperando en muchos de los cuerpos extraterrestres en el universo.
Las misiones diseñadas específicamente para buscar vida actual en Marte fueron el programa Viking y las sondas Beagle 2.
A finales de 2008, el módulo de aterrizaje Phoenix sondeó el medio ambiente para la habitabilidad planetaria pasada y presente de la vida microbiana en Marte, e investigó la historia del agua allí.
En noviembre de 2011, la NASA lanzó la misión Mars Science Laboratory que transportaba el rover Curiosity, que aterrizó en Marte en el cráter Gale en agosto de 2012.
Una es la suposición informada de que la gran mayoría de las formas de vida en nuestra galaxia se basan en la química del carbono, al igual que todas las formas de vida en la Tierra.
El hecho de que los átomos de carbono se unan fácilmente a otros átomos de carbono permite la construcción de moléculas extremadamente largas y complejas.
Una tercera suposición es centrarse en los planetas que orbitan estrellas similares al Sol para aumentar las probabilidades de habitabilidad planetaria.
Con ese fin, se han considerado una serie de instrumentos diseñados para detectar exoplanetas del tamaño de la Tierra, especialmente el Buscador de Planetas Terrestres (TPF) de la NASA y los programas Darwin de la ESA, ambos de los cuales han sido cancelados.
Drake formuló originalmente la ecuación simplemente como una agenda para la discusión en la conferencia de Green Bank, pero algunas aplicaciones de la fórmula se habían tomado literalmente y se habían relacionado con argumentos simplistas o pseudocientíficos.
El descubrimiento de extremófilos, organismos capaces de sobrevivir en ambientes extremos, se convirtió en un elemento de investigación central para los astrobiólogos, ya que son importantes para comprender cuatro áreas en los límites de la vida en el contexto planetario: el potencial de panspermia, la contaminación directa debido a las empresas de exploración humana, la colonización planetaria por humanos y la exploración de la vida extraterrestre extinta y existente.
Incluso se pensaba que la vida en las profundidades del océano, donde la luz solar no puede alcanzar, obtenía su alimento del consumo de detritos orgánicos que llovían de las aguas superficiales o del consumo de animales que sí lo hacían.
Esta quimiosíntesis revolucionó el estudio de la biología y la astrobiología al revelar que la vida no necesita ser dependiente del sol; solo requiere agua y un gradiente de energía para existir.
Diez organismos resistentes seleccionados para el proyecto LIFE, por Amir Alexander Deinococcus radiodurans, Bacillus subtilis, levadura Saccharomyces cerevisiae, semillas de Arabidopsis thaliana ("cress de oído de ratón"), así como el animal invertebrado Tardígrado.
La luna de Júpiter, Europa, y la luna de Saturno, Encelado, ahora se consideran los lugares más probables para la vida extraterrestre existente en el Sistema Solar debido a sus océanos de agua subsuperficial donde el calentamiento radiogénico y de mareas permite que exista agua líquida.
El polvo cósmico que impregna el universo contiene compuestos orgánicos complejos ("sólidos orgánicos amorfos con una estructura aromática-alifática mixta") que podrían ser creados de forma natural y rápida por las estrellas.
Los HAP parecen haberse formado poco después del Big Bang, están muy extendidos en todo el universo y están asociados con nuevas estrellas y exoplanetas.
La astroecología experimental investiga los recursos en suelos planetarios, utilizando materiales espaciales reales en meteoritos.
En la escala más grande, la cosmecología se refiere a la vida en el universo sobre los tiempos cosmológicos.
Las especializaciones incluyen cosmoquímica, bioquímica y geoquímica orgánica.
Algunas regiones de la Tierra, como el Pilbara en Australia Occidental y los Valles Secos McMurdo de la Antártida, también se consideran análogos geológicos a las regiones de Marte, y como tal, podrían proporcionar pistas sobre cómo buscar vida pasada en Marte.
De hecho, parece probable que los bloques de construcción básicos de la vida en cualquier lugar serán similares a los de la Tierra, en general, si no en el detalle.
Solo se sabe que dos de los átomos naturales, el carbono y el silicio, sirven como esqueletos de moléculas lo suficientemente grandes como para transportar información biológica.
Los cuatro candidatos más probables para la vida en el Sistema Solar son el planeta Marte, la luna joviana Europa y las lunas de Saturno Titán y Encélado.
A bajas temperaturas marcianas y baja presión, es probable que el agua líquida sea altamente salina.
El 11 de diciembre de 2013, la NASA informó de la detección de "minerales arcillosos" (específicamente, filosilicatos), a menudo asociados con materiales orgánicos, en la corteza helada de Europa.
Algunos científicos creen que es posible que estos hidrocarburos líquidos puedan ocupar el lugar del agua en células vivas diferentes a las de la Tierra.
No hay procesos abióticos conocidos en el planeta que puedan causar su presencia.
Yamato 000593, el segundo meteorito más grande de Marte, fue encontrado en la Tierra en 2000.
El 5 de marzo de 2011, Richard B. Hoover, un científico del Centro de Vuelo Espacial Marshall, especuló sobre el hallazgo de supuestos microfósiles similares a las cianobacterias en meteoritos carbonosos CI1 en la periferia del Journal of Cosmology, una historia ampliamente reportada por los principales medios de comunicación.
Se han encontrado evidencias de percloratos en todo el sistema solar, y específicamente en Marte.
Los métodos de detección mejorados y el aumento del tiempo de observación sin duda descubrirán más sistemas planetarios, y posiblemente más como el nuestro.
El objetivo es detectar aquellos organismos que son capaces de sobrevivir a las condiciones del viaje espacial y mantener la capacidad de proliferación.
Estas respuestas de estrés también podrían permitirles sobrevivir en duras condiciones espaciales, aunque la evolución también pone algunas restricciones en su uso como análogos de la vida extraterrestre.
La formación de esporas permite que sobreviva a ambientes extremos sin dejar de ser capaz de reiniciar el crecimiento celular.
Los dos módulos de aterrizaje eran idénticos, por lo que las mismas pruebas se llevaron a cabo en dos lugares de la superficie de Marte: Viking 1 cerca del ecuador y Viking 2 más al norte.
En astronomía, la extinción es la absorción y dispersión de la radiación electromagnética por el polvo y el gas entre un objeto astronómico emisor y el observador.
Para las estrellas que se encuentran cerca del plano de la Vía Láctea y están dentro de unos pocos miles de parsecs de la Tierra, la extinción en la banda visual de frecuencias (sistema fotométrico) es de aproximadamente 1,8 magnitudes por kiloparsec.
El enrojecimiento ocurre debido a la luz que se dispersa del polvo y otra materia en el medio interestelar.
En la mayoría de los sistemas fotométricos se utilizan filtros (bandas de paso) a partir de los cuales las lecturas de magnitud de la luz pueden tener en cuenta la latitud y la humedad entre los factores terrestres.
En términos generales, la extinción interestelar es más fuerte en longitudes de onda cortas, generalmente observada mediante el uso de técnicas de espectroscopia.
La cantidad de extinción puede ser significativamente mayor que esto en direcciones específicas.
Como resultado, cuando se calculan las distancias cósmicas, puede ser ventajoso mover los datos de estrellas desde el infario cercano (de los cuales el filtro o banda de paso Ks es bastante estándar) donde las variaciones y la cantidad de extinción son significativamente menores, y las proporciones similares a R(Ks): 0,49-0,02 y 0,528-0,015 fueron encontradas respectivamente por grupos independientes.
Esta característica se observó por primera vez en la década de 1960, pero su origen aún no se entiende bien.
En el SMC, se observa una variación más extrema sin 2175 ° y una extinción muy fuerte de UV lejano en la barra de formación estelar y una extinción ultravioleta bastante normal en el ala más quiescente.
Encontrar curvas de extinción tanto en la LMC como en la SMC que son similares a las que se encuentran en la Vía Láctea y encontrar curvas de extinción en la Vía Láctea que se parecen más a las que se encuentran en la supercapa LMC2 de la LMC y en la Barra SMC ha dado lugar a una nueva interpretación.
Esta extinción tiene tres componentes principales: dispersión de Rayleigh por moléculas de aire, dispersión por partículas y absorción molecular.
La cantidad de tal extinción es más baja en el cenit del observador y más alta cerca del horizonte.
La ecuación de Drake especula sobre la existencia de vida inteligente en otras partes del universo.
Esto abarca una búsqueda de vida extraterrestre actual e histórica, y una búsqueda más estrecha de vida inteligente extraterrestre.
A lo largo de los años, la ciencia ficción comunicó ideas científicas, imaginó una amplia gama de posibilidades e influyó en el interés público y las perspectivas de la vida extraterrestre.
Según este argumento, hecho por científicos como Carl Sagan y Stephen Hawking, así como personalidades notables como Winston Churchill, sería improbable que la vida no exista en otro lugar que no sea la Tierra.
La vida puede haber surgido de forma independiente en muchos lugares del universo.
En cada nivel del organismo habrá mecanismos para eliminar el conflicto, mantener la cooperación y mantener el funcionamiento del organismo.
La vida a base de amoníaco (en lugar de agua) se ha sugerido como alternativa, aunque este disolvente parece menos adecuado que el agua.
Alrededor del 95% de la materia viva se basa en sólo seis elementos: carbono, hidrógeno, nitrógeno, oxígeno, fósforo y azufre.
El átomo de carbono tiene la capacidad única de hacer cuatro enlaces químicos fuertes con otros átomos, incluidos otros átomos de carbono.
De acuerdo con la Estrategia de Astrobiología 2015 de la NASA, "la vida en otros mundos es más probable que incluya microbios, y cualquier sistema vivo complejo en otros lugares es probable que haya surgido y se base en la vida microbiana.
Rick Colwell, miembro del equipo del Observatorio de Carbono Profundo de la Universidad Estatal de Oregón, le dijo a la BBC: "Creo que probablemente sea razonable suponer que el subsuelo de otros planetas y sus lunas son habitables, especialmente desde que hemos visto aquí en la Tierra que los organismos pueden funcionar lejos de la luz solar utilizando la energía proporcionada directamente desde las rocas subterráneas profundas".
La hipótesis de la panspermia propone que la vida en otras partes del Sistema Solar puede tener un origen común.
En el 19no siglo fue otra vez reanimado en la forma moderna por varios científicos, incluso Johns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) y, algo más tarde, por Svante Arrhenius (1903).
Una de las primeras investigaciones científicas sobre el tema apareció en un artículo de 1878 de Scientific American titulado "¿Está habitada la Luna?"
Las regiones cálidas y presurizadas en el interior de la Luna aún podrían contener agua líquida.
Hay evidencia de que Marte tuvo un pasado más cálido y húmedo: se han encontrado lechos de ríos secos, casquetes polares, volcanes y minerales que se forman en presencia de agua.
El vapor podría haber sido producido por volcanes de hielo o por hielo cerca de la superficie sublimando (transformándose de sólido a gas).
También es posible que Europa pueda soportar la macrofauna aeróbica utilizando el oxígeno creado por los rayos cósmicos que impactan su hielo superficial.
El 11 de diciembre de 2013, la NASA informó de la detección de "minerales arcillosos" (específicamente, filosilicatos), a menudo asociados con materiales orgánicos, en la corteza helada de Europa.
Algunos afirman haber identificado evidencia de que la vida microbiana ha existido en Marte.
En 1996, un controvertido informe declaró que se descubrieron estructuras parecidas a nanobacterias en un meteorito, ALH84001, formado por roca expulsada de Marte.
Los funcionarios de la NASA pronto distanciaron a la NASA de las afirmaciones de los científicos, y Stoker se retractó de sus afirmaciones iniciales.
Está diseñado para evaluar la habitabilidad pasada y presente en Marte utilizando una variedad de instrumentos científicos.
Sin embargo, son necesarios avances significativos en la capacidad de encontrar y resolver la luz de mundos rocosos más pequeños cerca de sus estrellas antes de que tales métodos espectroscópicos puedan usarse para analizar planetas extrasolares.
En agosto de 2011, los hallazgos de la NASA, basados en estudios de meteoritos encontrados en la Tierra, sugieren que los componentes de ADN y ARN (adenina, guanina y moléculas orgánicas relacionadas), bloques de construcción para la vida tal como la conocemos, pueden formarse extraterrestremente en el espacio exterior.
En agosto de 2012, y por primera vez en el mundo, los astrónomos de la Universidad de Copenhague informaron sobre la detección de una molécula específica de azúcar, el glicolaldehído, en un sistema estelar distante.
El telescopio espacial Kepler también ha detectado unos pocos miles de planetas candidatos, de los cuales alrededor del 11% pueden ser falsos positivos.
El planeta más masivo listado en el Archivo de Exoplanetas de la NASA es DENIS-P J082303.1-491201 b, aproximadamente 29 veces la masa de Júpiter, aunque según la mayoría de las definiciones de un planeta, es demasiado masivo para ser un planeta y puede ser una enana marrón en su lugar.
Una señal de que un planeta probablemente ya contiene vida es la presencia de una atmósfera con cantidades significativas de oxígeno, ya que ese gas es altamente reactivo y generalmente no duraría mucho sin una reposición constante.
Incluso si se supone que solo una de cada mil millones de estas estrellas tiene planetas que sostienen vida, habría unos 6.250 millones de sistemas planetarios que sostienen la vida en el universo observable.
La primera afirmación registrada de la vida humana extraterrestre se encuentra en las antiguas escrituras del jainismo.
Los escritores musulmanes medievales como Fakhr al-Din al-Razi y Muhammad al-Baqir apoyaron el pluralismo cósmico sobre la base del Corán.
Una vez que quedó claro que la Tierra era simplemente un planeta entre innumerables cuerpos en el universo, la teoría de la vida extraterrestre comenzó a convertirse en un tema en la comunidad científica.
La posibilidad de extraterrestres seguía siendo una especulación generalizada a medida que se aceleraba el descubrimiento científico.
La idea de la vida en Marte llevó al escritor británico H. G. Wells a escribir la novela La guerra de los mundos en 1897, que habla de una invasión de extraterrestres de Marte que huían de la desecación del planeta.
La creencia en seres extraterrestres continúa siendo expresada en pseudociencia, teorías de conspiración y folklore popular, especialmente en el "Área 51" y leyendas.
Ward y Brownlee están abiertos a la idea de la evolución en otros planetas que no se basa en características esenciales similares a la Tierra (como el ADN y el carbono).
Si los extraterrestres nos visitan, el resultado sería como cuando Colón aterrizó en Estados Unidos, lo que no resultó bien para los nativos americanos”, dijo.
COSPAR también proporciona directrices para la protección planetaria.
Además, según la respuesta, "no hay información creíble que sugiera que cualquier evidencia se está ocultando a los ojos del público".
Arriba: Fuentes de luz de diferentes magnitudes.
Cometa Borrelly, los colores muestran su brillo en el rango de tres órdenes de magnitud (derecha).
La escala es logarítmica y se define de tal manera que cada paso de una magnitud cambia el brillo por un factor de la quinta raíz de 100, o aproximadamente 2,512.
Los astrónomos utilizan dos definiciones diferentes de magnitud: magnitud aparente y magnitud absoluta.
La magnitud absoluta describe la luminosidad intrínseca emitida por un objeto y se define como igual a la magnitud aparente que tendría el objeto si se colocara a una cierta distancia de la Tierra, 10 parsecs para las estrellas.
El desarrollo del telescopio mostró que estos grandes tamaños eran ilusorios: las estrellas parecían mucho más pequeñas a través del telescopio.
Cuanto más negativo es el valor, más brillante es el objeto.
Las estrellas que tienen magnitudes entre 1.5 y 2.5 se llaman segunda magnitud; hay unas 20 estrellas más brillantes que 1.5, que son estrellas de primera magnitud (ver la lista de estrellas más brillantes).
Las magnitudes absolutas para los objetos del sistema solar se citan con frecuencia en base a una distancia de 1 UA.
La forma más simple de tecnología es el desarrollo y uso de herramientas básicas.
Ha ayudado a desarrollar economías más avanzadas (incluida la economía global actual) y ha permitido el surgimiento de una clase ociosa.
Los ejemplos incluyen el aumento de la noción de eficiencia en términos de productividad humana y los desafíos de la bioética.
Los significados del término cambiaron a principios del siglo XX cuando los científicos sociales estadounidenses, comenzando con Thorstein Veblen, tradujeron ideas del concepto alemán de Technik a "tecnología".
En 1937, el sociólogo estadounidense Read Bain escribió que "la tecnología incluye todas las herramientas, máquinas, utensilios, armas, instrumentos, vivienda, ropa, dispositivos de comunicación y transporte y las habilidades por las cuales producimos y usamos".
Más recientemente, los estudiosos han tomado prestado de los filósofos europeos de la "técnica" para extender el significado de la tecnología a diversas formas de razón instrumental, como en el trabajo de Foucault sobre las tecnologías del yo (técnicas de soi).
inventar cosas útiles o resolver problemas" y "una máquina, una pieza de equipo, un método, etc.,
El término se utiliza a menudo para implicar un campo específico de la tecnología, o para referirse a la alta tecnología o simplemente la electrónica de consumo, en lugar de la tecnología en su conjunto.
En este uso, la tecnología se refiere a las herramientas y máquinas que se pueden utilizar para resolver problemas del mundo real.
W. Brian Arthur define la tecnología de una manera similar como "un medio para cumplir un propósito humano".
Cuando se combina con otro término, como "tecnología médica" o "tecnología espacial", se refiere al estado del conocimiento y las herramientas del campo respectivo.
Además, la tecnología es la aplicación de las matemáticas, la ciencia y las artes para el beneficio de la vida como se la conoce.
La ingeniería es el proceso orientado a objetivos de diseño y fabricación de herramientas y sistemas para explotar los fenómenos naturales por medios humanos prácticos, a menudo (pero no siempre) utilizando resultados y técnicas de la ciencia.
Por ejemplo, la ciencia podría estudiar el flujo de electrones en los conductores eléctricos mediante el uso de herramientas y conocimientos ya existentes.
Las relaciones exactas entre la ciencia y la tecnología, en particular, han sido debatidas por científicos, historiadores y políticos a finales del siglo XX, en parte porque el debate puede informar la financiación de la ciencia básica y aplicada.
Los primeros humanos evolucionaron a partir de una especie de homínidos forrajeros que ya eran bípedos, con una masa cerebral de aproximadamente un tercio de los humanos modernos.
La invención de hachas de piedra pulida fue un avance importante que permitió la tala de bosques a gran escala para crear granjas.
El primer uso conocido de la energía eólica es el barco de vela; el primer registro de un barco a vela es el de un barco del Nilo que data del octavo milenio antes de Cristo.
Según los arqueólogos, la rueda fue inventada alrededor de 4000 aC probablemente de forma independiente y casi simultáneamente en Mesopotamia (en el actual Irak), el Cáucaso del Norte (cultura Maykop) y Europa Central.
Más recientemente, la rueda de madera más antigua conocida en el mundo se encontró en las marismas de Ljubljana en Eslovenia.
Los antiguos sumerios usaron la rueda del alfarero y pueden haberla inventado.
Los primeros carros de dos ruedas se derivaron de travois y se utilizaron por primera vez en Mesopotamia e Irán alrededor de 3000 aC.
Una bañera prácticamente idéntica a las modernas fue desenterrada en el Palacio de Knossos.
El alcantarillado primario en Roma era Cloaca Maxima; la construcción comenzó en ello en el sexto siglo BCE y todavía está en el uso hoy.
La tecnología medieval vio el uso de máquinas simples (como la palanca, el tornillo y la polea) combinarse para formar herramientas más complicadas, como la carretilla, los molinos de viento y los relojes, y un sistema de universidades desarrolló y difundió ideas y prácticas científicas.
Comenzando en el Reino Unido en el 18vo siglo, la Revolución industrial era un período del gran descubrimiento tecnológico, en particular en las áreas de agricultura, fabricación, minería, metalurgia y transporte, conducido por el descubrimiento de la energía del vapor y la aplicación extendida del sistema de la fábrica.
El auge de la tecnología ha llevado a rascacielos y amplias áreas urbanas cuyos habitantes dependen de motores para transportarlos y sus suministros de alimentos.
El siglo XX trajo consigo una serie de innovaciones.
La tecnología de la información posteriormente llevó al nacimiento en los años 1980 de Internet, que marcó el comienzo de la Era de la Información corriente.
Se necesitan técnicas y organizaciones complejas de fabricación y construcción para fabricar y mantener algunas de las tecnologías más nuevas, y han surgido industrias enteras para apoyar y desarrollar generaciones sucesivas de herramientas cada vez más complejas.
Los transhumanistas generalmente creen que el objetivo de la tecnología es superar las barreras, y que lo que comúnmente llamamos la condición humana es solo otra barrera que debe superarse.
Sugieren que el resultado inevitable de tal sociedad es volverse cada vez más tecnológico a costa de la libertad y la salud psicológica.
Espera revelar la esencia de la tecnología de una manera que "de ninguna manera nos confina a una compulsión aturdida para seguir adelante ciegamente con la tecnología o, lo que es lo mismo, para rebelarse impotentemente contra ella".
Algunas de las críticas más conmovedoras a la tecnología se encuentran en lo que ahora se consideran clásicos literarios distópicos, como Brave New World de Aldous Huxley, A Clockwork Orange de Anthony Burgess y Mil novecientos ochenta y cuatro de George Orwell.
El difunto crítico cultural Neil Postman distinguió a las sociedades que utilizan herramientas de las sociedades tecnológicas y de lo que llamó "tecnopolios", sociedades dominadas por la ideología del progreso tecnológico y científico con exclusión o daño de otras prácticas culturales, valores y visiones del mundo.
Nikolas Kompridis también ha escrito sobre los peligros de las nuevas tecnologías, como la ingeniería genética, la nanotecnología, la biología sintética y la robótica.
Otro destacado crítico de la tecnología es Hubert Dreyfus, que ha publicado libros como On the Internet y What Computers Still Can't Do.
En su artículo, Jared Bernstein, miembro principal del Centro de Presupuesto y Prioridades Políticas, cuestiona la idea generalizada de que la automatización y, en términos más generales, los avances tecnológicos, han contribuido principalmente a este creciente problema del mercado laboral.
Utiliza dos argumentos principales para defender su punto.
De hecho, la automatización amenaza los trabajos repetitivos, pero los trabajos de gama alta siguen siendo necesarios porque complementan la tecnología y los trabajos manuales que "requieren un juicio flexible y sentido común" siguen siendo difíciles de reemplazar con máquinas.
La tecnología a menudo se considera demasiado estrecha; según Hughes, "La tecnología es un proceso creativo que implica el ingenio humano".
A menudo han supuesto que la tecnología es fácilmente controlable y esta suposición tiene que ser completamente cuestionada.
El solucionismo es la ideología de que cada problema social se puede resolver gracias a la tecnología y especialmente gracias a Internet.
Benjamin R. Cohen y Gwen Ottinger también discutieron los efectos multivalentes de la tecnología.
El uso de la tecnología básica también es una característica de otras especies animales aparte de los humanos.
La capacidad de hacer y usar herramientas alguna vez se consideró una característica definitoria del género Homo.
En 2005, el futurista Ray Kurzweil predijo que el futuro de la tecnología consistiría principalmente en una "Revolución GNR" superpuesta de genética, nanotecnología y robótica, siendo la robótica la más importante de las tres.
Los seres humanos ya han dado algunos de los primeros pasos hacia el logro de la revolución GNR.
Algunos creen que el futuro de la robótica implicará una "inteligencia no biológica mayor que la humana".
Este futuro comparte muchas similitudes con el concepto de obsolescencia planificada, sin embargo, la obsolescencia planificada es vista como una "siniestra estrategia de negocios".
También se ha explorado la genética, con los humanos entendiendo la ingeniería genética hasta cierto punto.
Otros piensan que la ingeniería genética se utilizará para hacer que los seres humanos sean más resistentes o completamente inmunes a algunas enfermedades.
Los futuristas creen que la tecnología nanobot permitirá a los humanos "manipular la materia a escala molecular y atómica".
En este contexto, ahora obsoleto, un "motor" se refería a una máquina militar, es decir, un artilugio mecánico utilizado en la guerra (por ejemplo, una catapulta).
Las seis máquinas simples clásicas eran conocidas en el antiguo Cercano Oriente.
El mecanismo de palanca apareció por primera vez hace unos 5.000 años en el Cercano Oriente, donde se utilizó en una escala de equilibrio simple, y para mover objetos grandes en la tecnología del antiguo Egipto.
El tornillo, la última de las máquinas simples que se inventaron, apareció por primera vez en Mesopotamia durante el período neoasirio (911-609) aC.
Como uno de los funcionarios del faraón, Djosir, probablemente diseñó y supervisó la construcción de la Pirámide de Djoser (la Pirámide escalonada) en Saqqara en Egipto alrededor de 2630-2611 aC.
Los antepasados de Kushite construyeron speos durante la Edad de Bronce entre 3700 y 3250 A.C. Los bloomeries y los altos hornos también se crearon durante los 7mos siglos A.C. en Kush.
Algunos de los inventos de Arquímedes, así como el mecanismo de Anticitera, requerían un conocimiento sofisticado de engranajes diferenciales o engranajes epicíclicos, dos principios clave en la teoría de la máquina que ayudaron a diseñar los trenes de engranajes de la Revolución Industrial, y todavía se utilizan ampliamente en diversos campos como la robótica y la ingeniería automotriz.
La rueda giratoria también fue un precursor de la hilatura de Jenny, que fue un desarrollo clave durante la Revolución Industrial en el siglo 18.
Describió a cuatro músicos autómatas, incluidos bateristas operados por una caja de ritmos programable, donde se les podía hacer tocar diferentes ritmos y diferentes patrones de batería.
Aparte de estas profesiones, no se creía que las universidades tuvieran mucho significado práctico para la tecnología.
La construcción de canales fue una importante obra de ingeniería durante las primeras fases de la Revolución Industrial.
También era un ingeniero mecánico capaz y un físico eminente.
Smeaton también hizo mejoras mecánicas a la máquina de vapor Newcomen.
Samuel Morland, un matemático e inventor que trabajó en bombas, dejó notas en la Oficina de Ordenanzas de Vauxhall sobre un diseño de bomba de vapor que Thomas Savery leyó.
El comerciante de hierro Thomas Newcomen, que construyó la primera máquina de vapor de pistón comercial en 1712, no se sabía que tuviera ninguna formación científica.
Estas innovaciones redujeron el costo del hierro, haciendo que los ferrocarriles de caballos y los puentes de hierro sean prácticos.
Con el desarrollo de la máquina de vapor de alta presión, la relación potencia-peso de las máquinas de vapor hizo posible la práctica de los barcos de vapor y locomotoras.
La Revolución Industrial creó una demanda de maquinaria con piezas metálicas, lo que llevó al desarrollo de varias máquinas herramientas.
Las técnicas de mecanizado de precisión se desarrollaron en la primera mitad del siglo XIX.
El censo de los Estados Unidos de 1850 enumeró la ocupación del "ingeniero" por primera vez con una cuenta de 2.000.
En 1890, había 6.000 ingenieros en civil, minería, mecánica y eléctrica.
Los fundamentos de la ingeniería eléctrica en el siglo XIX incluyeron los experimentos de Alessandro Volta, Michael Faraday, Georg Ohm y otros y la invención del telégrafo eléctrico en 1816 y el motor eléctrico en 1872.
La ingeniería aeronáutica se ocupa del diseño de procesos de diseño de aeronaves, mientras que la ingeniería aeroespacial es un término más moderno que amplía el alcance de la disciplina al incluir el diseño de naves espaciales.
Históricamente, la ingeniería naval y la ingeniería minera eran ramas principales.
Como resultado, muchos ingenieros continúan aprendiendo nuevo material a lo largo de sus carreras.
En general, es insuficiente construir un producto técnicamente exitoso, más bien, también debe cumplir con requisitos adicionales.
Genrich Altshuller, después de recopilar estadísticas sobre un gran número de patentes, sugirió que los compromisos están en el corazón de los diseños de ingeniería de "bajo nivel", mientras que en un nivel superior el mejor diseño es aquel que elimina la contradicción central que causa el problema.
Las pruebas aseguran que los productos funcionarán como se espera.
Además del software de aplicación comercial típico, hay varias aplicaciones asistidas por computadora (tecnologías asistidas por computadora) específicamente para ingeniería.
Permite a los ingenieros crear modelos 3D, dibujos 2D y esquemas de sus diseños.
El acceso y la distribución de toda esta información generalmente se organiza con el uso de software de gestión de datos de productos.
Por su propia naturaleza, la ingeniería tiene interconexiones con la sociedad, la cultura y el comportamiento humano.
Los proyectos de ingeniería pueden ser objeto de controversia.
La ingeniería es un motor clave de la innovación y el desarrollo humano.
Hay muchos problemas económicos y políticos negativos que esto puede causar, así como problemas éticos.
Los científicos también pueden tener que completar tareas de ingeniería, como diseñar aparatos experimentales o construir prototipos.
En primer lugar, a menudo se trata de áreas en las que la física básica o la química se entienden bien, pero los problemas en sí mismos son demasiado complejos para resolverlos de manera exacta.
El primero equipara la comprensión con un principio matemático, mientras que el segundo mide las variables involucradas y crea tecnología.
Un físico normalmente requeriría capacitación adicional y relevante.
Un ejemplo de esto es el uso de aproximaciones numéricas a las ecuaciones de Navier-Stokes para describir el flujo aerodinámico sobre una aeronave, o el uso del método de elementos finitos para calcular las tensiones en componentes complejos.
Los ingenieros enfatizan la innovación y la invención.
Dado que un diseño tiene que ser realista y funcional, debe tener sus datos de geometría, dimensiones y características definidas.
Así estudiaron matemáticas, física, química, biología y mecánica.
La medicina moderna puede reemplazar varias de las funciones del cuerpo a través del uso de órganos artificiales y puede alterar significativamente la función del cuerpo humano a través de dispositivos artificiales como, por ejemplo, implantes cerebrales y marcapasos.
Ambos campos proporcionan soluciones a los problemas del mundo real.
La gestión de ingeniería o "ingeniería de gestión" es un campo especializado de la gestión que se ocupa de la práctica de la ingeniería o el sector de la industria de la ingeniería.
Los ingenieros especializados en la gestión del cambio deben tener un conocimiento profundo de la aplicación de los principios y métodos de la psicología industrial y organizacional.
La inteligencia artificial (IA) es la inteligencia demostrada por las máquinas, a diferencia de la inteligencia natural mostrada por humanos o animales.
La investigación de la IA ha probado y descartado muchos enfoques diferentes durante su vida, incluida la simulación del cerebro, el modelado de la resolución de problemas humanos, la lógica formal, las grandes bases de datos de conocimiento y la imitación del comportamiento animal.
Los objetivos tradicionales de la investigación de la IA incluyen el razonamiento, la representación del conocimiento, la planificación, el aprendizaje, el procesamiento del lenguaje natural, la percepción y la capacidad de mover y manipular objetos.
La IA también se basa en la informática, la psicología, la linguística, la filosofía y muchos otros campos.
El estudio del razonamiento mecánico o "formal" comenzó con los filósofos y matemáticos en la antiguedad.
La tesis Church-Turing, junto con descubrimientos concurrentes en neurobiología, teoría de la información y cibernética, llevó a los investigadores a considerar la posibilidad de construir un cerebro electrónico.
Los asistentes Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) y Arthur Samuel (IBM) se convirtieron en los fundadores y líderes de la investigación de IA.
Los fundadores de AI eran optimistas sobre el futuro: Herbert Simon predijo: "Las máquinas serán capaces, dentro de veinte años, de hacer cualquier trabajo que un hombre pueda hacer".
El progreso se ralentizó y en 1974, en respuesta a las críticas de Sir James Lighthill y la presión continua del Congreso de los Estados Unidos para financiar proyectos más productivos, tanto los gobiernos de los Estados Unidos como los británicos cortaron la investigación exploratoria en IA.
En 1985, el mercado de la IA había alcanzado más de mil millones de dólares.
Las computadoras más rápidas, las mejoras algorítmicas y el acceso a grandes cantidades de datos permitieron avances en el aprendizaje automático y la percepción; los métodos de aprendizaje profundo hambrientos de datos comenzaron a dominar los puntos de referencia de precisión alrededor de 2012.
La investigación de IA se dividió en subcampos competidores que a menudo no se comunicaban entre sí.
La investigación se centró en tres instituciones: la Universidad Carnegie Mellon, Stanford y el MIT, y como se describe a continuación, cada una desarrolló su propio estilo de investigación.
Llamaron a su trabajo por varios nombres: por ejemplo, encarnado, situado, basado en el comportamiento o en el desarrollo.
El lenguaje matemático compartido permitió un alto nivel de colaboración con campos más establecidos (como matemáticas, economía o investigación de operaciones).
Hoy en día los resultados de los experimentos son a menudo rigurosamente medibles, y son a veces (con dificultad) reproducibles.
Estos algoritmos demostraron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una "explosión combinatoria": se volvieron exponencialmente más lentos a medida que los problemas se hicieron más grandes.
Entre las cosas que una base de conocimiento de sentido común integral contendría están: objetos, propiedades, categorías y relaciones entre objetos; situaciones, eventos, estados y tiempo; causas y efectos; conocimiento sobre el conocimiento (lo que sabemos sobre lo que otras personas saben); y muchos otros dominios menos investigados.
Por ejemplo, si un pájaro aparece en una conversación, la gente normalmente se imagina un animal del tamaño de un puño que canta y vuela.
Casi nada es simplemente verdadero o falso en la forma en que la lógica abstracta requiere.
Los proyectos de investigación que intentan construir una base de conocimiento completa de conocimiento de sentido común (por ejemplo, Cyc) requieren enormes cantidades de ingeniería ontológica laboriosa: deben construirse, a mano, un concepto complicado a la vez.
Necesitan una forma de visualizar el futuro, una representación del estado del mundo y poder hacer predicciones sobre cómo sus acciones lo cambiarán, y poder tomar decisiones que maximicen la utilidad (o "valor") de las opciones disponibles.
Esto requiere un agente que no solo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en función de su evaluación.
La clasificación se utiliza para determinar a qué categoría pertenece algo, y ocurre después de que un programa ve una serie de ejemplos de cosas de varias categorías.
La teoría del aprendizaje computacional puede evaluar a los alumnos por la complejidad computacional, por la complejidad de la muestra (cuántos datos se requieren) o por otras nociones de optimización.
Muchos enfoques actuales utilizan frecuencias de co-ocurrencia de palabras para construir representaciones sintácticas del texto.
Los enfoques estadísticos modernos de PNL pueden combinar todas estas estrategias, así como otras, y a menudo lograr una precisión aceptable a nivel de página o párrafo.
Un robot móvil moderno, cuando se le da un entorno pequeño, estático y visible, puede determinar fácilmente su ubicación y mapear su entorno; sin embargo, los entornos dinámicos, como (en endoscopia) el interior del cuerpo respiratorio de un paciente, plantean un mayor desafío.
Por ejemplo, algunos asistentes virtuales están programados para hablar conversacionalmente o incluso bromear humorísticamente; los hace parecer más sensibles a la dinámica emocional de la interacción humana, o para facilitar la interacción humano-computadora.
¿Se puede describir el comportamiento inteligente utilizando principios simples y elegantes (como la lógica o la optimización)?
¿O usamos algoritmos que solo pueden darnos una solución "razonable" (por ejemplo, métodos probabilísticos) pero pueden caer presas del mismo tipo de errores inescrutables que comete la intuición humana?
Stuart Russell y Peter Norvig observan que a la mayoría de los investigadores de IA "no les importa la fuerte hipótesis de la IA, siempre y cuando el programa funcione, no les importa si lo llamas una simulación de inteligencia o inteligencia real".
Por lo tanto, la nueva inteligencia podría aumentar exponencialmente y superar dramáticamente a los humanos.
La relación entre automatización y empleo es complicada.
Las estimaciones subjetivas del riesgo varían ampliamente; por ejemplo, Michael Osborne y Carl Benedikt Frey estiman que el 47% de los empleos en los Estados Unidos están en "alto riesgo" de automatización potencial, mientras que un informe de la OCDE clasifica solo el 9% de los empleos en los Estados Unidos como de "alto riesgo".
A largo plazo, los científicos han propuesto continuar optimizando la función al tiempo que minimizan los posibles riesgos de seguridad que vienen junto con las nuevas tecnologías.
En su libro Superinteligencia, el filósofo Nick Bostrom argumenta que la inteligencia artificial representará una amenaza para la humanidad.
Bostrom también enfatiza la dificultad de transmitir completamente los valores de la humanidad a una IA avanzada.
En su libro Human Compatible, el investigador de IA Stuart J. Russell se hace eco de algunas de las preocupaciones de Bostrom al tiempo que propone un enfoque para desarrollar máquinas probablemente beneficiosas centradas en la incertidumbre y la deferencia a los humanos, posiblemente involucrando un aprendizaje de refuerzo inverso.
La opinión de los expertos en el campo de la inteligencia artificial es mixta, con fracciones considerables tanto preocupadas como despreocupadas por el riesgo de una eventual IA sobrehumanamente capaz.
El CEO de Facebook, Mark Zuckerberg, cree que la IA “desbloqueará una gran cantidad de cosas positivas”, como curar enfermedades y aumentar la seguridad de los automóviles autónomos.
Musk también financia compañías que desarrollan inteligencia artificial como DeepMind y Vicarious para "solo vigilar lo que está sucediendo con la inteligencia artificial".
La investigación en esta área incluye la ética de la máquina, los agentes morales artificiales, la IA amistosa y la discusión hacia la construcción de un marco de derechos humanos también está en conversaciones.
Ha llegado el momento de añadir una dimensión ética a al menos algunas máquinas.
La investigación en la ética de la máquina es clave para aliviar las preocupaciones con los sistemas autónomos: se podría argumentar que la noción de máquinas autónomas sin tal dimensión está en la raíz de todo temor con respecto a la inteligencia de la máquina.
Los humanos no deberían asumir que las máquinas o los robots nos tratarían favorablemente porque no hay ninguna razón a priori para creer que simpatizarían con nuestro sistema de moralidad, que ha evolucionado junto con nuestra biología particular (que AI no compartiría).
Una propuesta para hacer frente a esto es garantizar que la primera IA generalmente inteligente sea "AI amigable" y pueda controlar las IA desarrolladas posteriormente.
Creo que la preocupación se deriva de un error fundamental al no distinguir la diferencia entre los avances recientes muy reales en un aspecto particular de la IA y la enormidad y complejidad de la construcción de inteligencia volitiva sensible.
La regulación se considera necesaria tanto para fomentar la IA como para gestionar los riesgos asociados.
Un tropo común en estas obras comenzó con Frankenstein de Mary Shelley, donde una creación humana se convierte en una amenaza para sus amos.
Isaac Asimov introdujo las Tres Leyes de la Robótica en muchos libros e historias, sobre todo la serie "Multivac" sobre una computadora súper inteligente del mismo nombre.
En la década de 1980, la serie Sexy Robots del artista Hajime Sorayama fue pintada y publicada en Japón, representando la forma humana orgánica real con pieles metálicas musculares realistas y más tarde siguió el libro "the Gynoids" que fue utilizado o influenciado por cineastas como George Lucas y otros creativos.
La biotecnología es un área amplia de la biología, que implica el uso de sistemas vivos y organismos para desarrollar o fabricar productos.
La American Chemical Society define la biotecnología como la aplicación de organismos biológicos, sistemas o procesos por diversas industrias para aprender sobre la ciencia de la vida y la mejora del valor de materiales y organismos como productos farmacéuticos, cultivos y ganado.
La bioingeniería es la aplicación de los principios de la ingeniería y las ciencias naturales a los tejidos, células y moléculas.
A través de la biotecnología temprana, los primeros agricultores seleccionaron y criaron los cultivos más adecuados, con los rendimientos más altos, para producir suficientes alimentos para mantener a una población en crecimiento.
Estos procesos también se incluyeron en la fermentación temprana de la cerveza.
En este proceso, los carbohidratos en los granos se descomponen en alcoholes, como el etanol.
Aunque el proceso de fermentación no se entendió completamente hasta el trabajo de Louis Pasteur en 1857, sigue siendo el primer uso de la biotecnología para convertir una fuente de alimento en otra forma.
Estos relatos contribuyeron a la teoría de la selección natural de Darwin.
En 1928, Alexander Fleming descubrió el moho Penicillium.
El transistor de efecto de campo MOSFET (metal-óxido-semiconductor) fue inventado por Mohamed M. Atalla y Dawon Kahng en 1959.
El primer BioFET fue el transistor de efecto de campo sensible a los iones (ISFET), inventado por Piet Bergveld en 1970.
A mediados de la década de 1980, se habían desarrollado otros BioFET, incluido el sensor de gas FET (GASFET), el sensor de presión FET (PRESSFET), el transistor de efecto de campo químico (ChemFET), el ISFET de referencia (REFET), el FET modificado con enzimas (ENFET) y el FET modificado inmunológicamente (IMFET).
Se espera que la creciente demanda de biocombustibles sea una buena noticia para el sector de la biotecnología, y el Departamento de Energía estima que el uso de etanol podría reducir el consumo de combustible derivado del petróleo de los Estados Unidos hasta en un 30% para 2030.
TCE: El Ingeniero Químico, (816), 26-31.
Otro ejemplo es el diseño de plantas transgénicas para crecer en entornos específicos en presencia (o ausencia) de productos químicos.
Por otro lado, algunos de los usos de la biotecnología verde implican microorganismos para limpiar y reducir los residuos.
Así como el desarrollo de hormonas, células madre, anticuerpos, ARNip y pruebas diagnósticas.
Una aplicación es la creación de semillas mejoradas que resistan las condiciones ambientales extremas de las regiones áridas, lo que está relacionado con la innovación, la creación de técnicas agrícolas y la gestión de los recursos.
El propósito de la farmacogenómica es desarrollar medios racionales para optimizar la terapia farmacológica, con respecto al genotipo de los pacientes, para garantizar la máxima eficacia con efectos adversos mínimos.
La biotecnología moderna se puede utilizar para fabricar medicamentos existentes de manera relativamente fácil y económica.
Las pruebas genéticas permiten el diagnóstico genético de vulnerabilidades a enfermedades hereditarias, y también se pueden usar para determinar el parentesco de un niño (madre y padre genéticos) o, en general, la ascendencia de una persona.
La mayoría de las veces, las pruebas se utilizan para encontrar cambios que están asociados con trastornos hereditarios.
Las empresas de biotecnología pueden contribuir a la seguridad alimentaria futura mejorando la nutrición y la viabilidad de la agricultura urbana.
El 10% de las tierras de cultivo del mundo fueron plantadas con cultivos transgénicos en 2010.
Estas técnicas han permitido la introducción de nuevos rasgos de cultivo, así como un control mucho mayor sobre la estructura genética de un alimento que el que permitían previamente métodos como la reproducción selectiva y la reproducción por mutación.
Estos han sido diseñados para la resistencia a patógenos y herbicidas y mejores perfiles de nutrientes.
Sin embargo, los miembros del público son mucho menos propensos que los científicos a percibir los alimentos transgénicos como seguros.
Sin embargo, los opositores se han opuesto a los cultivos transgénicos per se por varios motivos, incluidas las preocupaciones ambientales, si los alimentos producidos a partir de cultivos transgénicos son seguros, si los cultivos transgénicos son necesarios para atender las necesidades alimentarias del mundo, y las preocupaciones económicas planteadas por el hecho de que estos organismos están sujetos a la ley de propiedad intelectual.
Hay diferencias en la regulación de los OGM entre los países, con algunas de las diferencias más marcadas que ocurren entre los EE.UU. y Europa.
La Unión Europea diferencia entre la aprobación para el cultivo dentro de la UE y la aprobación para la importación y el procesamiento.
Cada solicitud exitosa generalmente se financia durante cinco años y luego debe renovarse competitivamente.
La clonación es el proceso de producción de organismos individuales con ADN idéntico o virtualmente idéntico, ya sea por medios naturales o artificiales.
Se utiliza en una amplia gama de experimentos biológicos y aplicaciones prácticas que van desde la huella genética hasta la producción de proteínas a gran escala.
Inicialmente, el ADN de interés necesita aislarse para proporcionar un segmento de ADN de tamaño adecuado.
Después de la ligación, el vector con el inserto de interés se transfecta en las células.
Una técnica de cultivo tisular útil usada para clonar distintos linajes de líneas celulares implica el uso de anillos de clonación (cilindros).
Este proceso también se denomina "clonación de investigación" o "clonación terapéutica".
La clonación terapéutica se logra mediante la creación de células madre embrionarias con la esperanza de tratar enfermedades como la diabetes y el Alzheimer.
La razón por la que se usa SCNT para la clonación es porque las células somáticas se pueden adquirir y cultivar fácilmente en el laboratorio.
El ovocito reaccionará al núcleo de la célula somática, de la misma manera que lo haría al núcleo de una célula espermática.
Las células somáticas podrían usarse inmediatamente o almacenarse en el laboratorio para su uso posterior.
Esto crea un embrión unicelular.
Los embriones desarrollados con éxito se colocan en receptores sustitutos, como una vaca u oveja en el caso de animales de granja.
Otro beneficio es que SCNT es visto como una solución para clonar especies en peligro de extinción que están a punto de extinguirse.
Solo tres de estos embriones sobrevivieron hasta el nacimiento, y solo uno sobrevivió hasta la edad adulta.
Sin embargo, en 2014 los investigadores informaron tasas de éxito de clonación de siete a ocho de cada diez y en 2016, se informó que una compañía coreana Sooam Biotech producía 500 embriones clonados por día.
La reproducción asexual es un fenómeno natural en muchas especies, incluyendo la mayoría de las plantas y algunos insectos.
Como ejemplo, algunos cultivares europeos de uvas representan clones que se han propagado durante más de dos milenios.
Muchos árboles, arbustos, vides, helechos y otras plantas perennes herbáceas forman colonias clonales de forma natural.
En las plantas, partenogénesis significa el desarrollo de un embrión a partir de un óvulo no fertilizado, y es un proceso componente de la apomixis.
Tales clones no son estrictamente idénticos ya que las células somáticas pueden contener mutaciones en su ADN nuclear.
La división de embriones artificiales o el hermanamiento de embriones, una técnica que crea gemelos monocigóticos a partir de un solo embrión, no se considera de la misma manera que otros métodos de clonación.
El embrión de Dolly se creó tomando la célula e insertándola en un óvulo de oveja.
Fue clonada en el Instituto Roslin en Escocia por los científicos británicos Sir Ian Wilmut y Keith Campbell y vivió allí desde su nacimiento en 1996 hasta su muerte en 2003 cuando tenía seis años.
Dolly fue públicamente significativo porque el esfuerzo mostró que el material genético de una célula adulta específica, diseñado para expresar solo un subconjunto distinto de sus genes, puede rediseñarse para hacer crecer un organismo completamente nuevo.
La primera clonación de mamíferos (que resultó en la oveja Dolly) tuvo una tasa de éxito de 29 embriones por cada 277 huevos fertilizados, lo que produjo tres corderos al nacer, uno de los cuales vivió.
Notablemente, aunque los primeros clones fueron ranas, aún no se ha producido ninguna rana clonada adulta a partir de una célula somática donante de núcleo adulto.
Sin embargo, otros investigadores, incluido Ian Wilmut, que dirigió el equipo que clonó con éxito a Dolly, argumentan que la muerte temprana de Dolly debido a una infección respiratoria no estaba relacionada con problemas con el proceso de clonación.
Los científicos soviéticos Chaylakhyan, Veprencev, Sviridova y Nikitin clonaron el ratón "Masha".
Más parecido a la formación artificial de gemelos.
Perro: Snuppy, un sabueso afgano macho fue el primer perro clonado (2005).
Bufalo de agua: Samrupa fue el primer búfalo de agua clonado.
Camel: (2009) Injaz, es el primer camello clonado.
Cabra: (2001) Científicos de la Universidad Northwest A&F clonaron con éxito la primera cabra que usa la célula femenina adulta.
Realizado en Chína en 2017, y reportado en enero de 2018.
Hurón de patas negras: En 2020, un equipo de científicos clonó a una hembra llamada Willa, que murió a mediados de la década de 1980 y no dejó descendientes vivos.
No se refiere a la concepción natural y la entrega de gemelos idénticos.
A partir de ahora, los científicos no tienen intención de tratar de clonar a las personas y creen que sus resultados deberían provocar una discusión más amplia sobre las leyes y regulaciones que el mundo necesita para regular la clonación.
Si bien muchos de estos puntos de vista son de origen religioso, las preguntas planteadas por la clonación también se enfrentan a perspectivas seculares.
A los que se oponen a la clonación les preocupa que la tecnología aún no esté lo suficientemente desarrollada como para ser segura y que pueda ser propensa al abuso (lo que lleva a la generación de humanos de los cuales se extraerían órganos y tejidos), así como las preocupaciones sobre cómo los individuos clonados podrían integrarse con las familias y con la sociedad en general.
Esto también se conoce como "clonación de conservación".
Estos éxitos proporcionaron la esperanza de que técnicas similares (usando madres sustitutas de otra especie) podrían usarse para clonar especies extintas.
En 2002, los genetistas del Museo Australiano anunciaron que habían replicado el ADN del tilacino (tigre de Tasmania), en ese momento extinto durante unos 65 años, utilizando la reacción en cadena de la polimerasa.
En 2003, por primera vez, se clonó un animal extinto, el íbice pirenaico mencionado anteriormente, en el Centro de Tecnología e Investigación de Alimentos de Aragón, utilizando el núcleo celular congelado conservado de las muestras de piel de 2001 y células de huevo de cabra domésticas.
Otro problema es la supervivencia del mamut reconstruido: los rumiantes dependen de una simbiosis con microbiota específica en sus estómagos para la digestión.
Debido a esto, algunos postularon que pudo haber envejecido más rápido que otros animales nacidos naturalmente, ya que murió relativamente temprano por una oveja a la edad de seis años.
Sin embargo, la pérdida temprana del embarazo y las pérdidas neonatales son aún mayores con la clonación que la concepción natural o la reproducción asistida (FIV).
El concepto de clonación, particularmente la clonación humana, ha presentado una amplia variedad de obras de ciencia ficción.
Muchas obras representan la creación artificial de seres humanos por un método de crecimiento de células a partir de un tejido o muestra de ADN; la replicación puede ser instantánea, o tener lugar a través del crecimiento lento de embriones humanos en úteros artificiales.
Películas de ciencia ficción como The Matrix y Star Wars: Episodio II – El Ataque de los Clones han presentado escenas de fetos humanos cultivados a escala industrial en tanques mecánicos.
A Number fue adaptada por Caryl Churchill para televisión, en una coproducción entre la BBC y HBO Films.
Creció siempre dudando del amor de su madre, que no se parecía en nada a ella y que murió nueve años antes.
En la novela de 1976 de Ira Levin Los Muchachos de Brasil y su adaptación de la película de 1978, Josef Mengele usa la clonación para crear copias de Adolf Hitler.
En Doctor Who, una raza alienígena de seres blindados y guerreros llamados Sontarans se introdujo en la serie de 1973 "The Time Warrior".
El concepto de soldados clonados que se crían para el combate fue revisado en "La hija del Doctor" (2008), cuando el ADN del Doctor se utiliza para crear una mujer guerrera llamada Jenny.
La novela de Kazuo Ishiguro de 2005 Never Let Me Go y la adaptación cinematográfica de 2010 están ambientadas en una historia alternativa en la que los humanos clonados se crean con el único propósito de proporcionar donaciones de órganos a humanos nacidos naturalmente, a pesar del hecho de que son totalmente sensibles y conscientes de sí mismos.
En la novela futurista Cloud Atlas y la película posterior, una de las líneas de la historia se centra en un clon fabricante genéticamente modificado llamado Sonmi-451, uno de los millones criados en un "wombtank" artificial, destinado a servir desde el nacimiento.
En la película Us, en algún momento anterior a la década de 1980, el gobierno de los Estados Unidos crea clones de todos los ciudadanos de los Estados Unidos con la intención de usarlos para controlar a sus homólogos originales, similares a las muñecas vudú.
En la actualidad, los clones lanzan un ataque sorpresa y logran completar un genocidio masivo de sus contrapartes inconscientes.
Los genes se han transferido dentro de la misma especie, a través de especies (creando organismos transgénicos), e incluso a través de reinos.
Los ingenieros genéticos deben aislar el gen que desean insertar en el organismo huésped y combinarlo con otros elementos genéticos, incluyendo una región promotora y terminadora y, a menudo, un marcador seleccionable.
Herbert Boyer y Stanley Cohen crearon el primer organismo genéticamente modificado en 1973, una bacteria resistente al antibiótico kanamicina.
El primer animal modificado genéticamente que se comercializó fue el GloFish (2003) y el primer animal modificado genéticamente que se aprobó para su uso alimentario fue el salmón AquAdvantage en 2015.
Los hongos han sido diseñados con los mismos objetivos.
Hay propuestas para eliminar los genes virulentos de los virus para crear vacunas.
La mayoría están diseñados para la tolerancia a herbicidas o resistencia a insectos.
Los animales son generalmente mucho más difíciles de transformar y la gran mayoría todavía están en la etapa de investigación.
El ganado se modifica con la intención de mejorar los rasgos económicamente importantes, como la tasa de crecimiento, la calidad de la carne, la composición de la leche, la resistencia a las enfermedades y la supervivencia.
Aunque la terapia génica humana todavía es relativamente nueva, se ha utilizado para tratar trastornos genéticos como la inmunodeficiencia combinada grave y la amaurosis congénita de Leber.
Otras preocupaciones son la objetividad y el rigor de las autoridades reguladoras, la contaminación de los alimentos no modificados genéticamente, el control del suministro de alimentos, el patentamiento de la vida y el uso de los derechos de propiedad intelectual.
Los países han adoptado medidas reglamentarias para hacer frente a estas preocupaciones.
Una definición amplia de ingeniería genética también incluye la cría selectiva y otros medios de selección artificial.
Por ejemplo, el cultivo de grano triticale se desarrolló completamente en un laboratorio en 1930 utilizando diversas técnicas para alterar su genoma.
La biotecnología moderna se define además como "técnicas de ácido nucleico in vitro, que incluyen ácido desoxirribonucleico recombinante (ADN) e inyección directa de ácido nucleico en células u orgánulos, o fusión de células más allá de la familia taxonómica".
Las definiciones se centran en el proceso más que en el producto, lo que significa que podría haber transgénicos y no transgénicos con genotipos y fenotipos muy similares.
También plantea problemas a medida que se desarrollan nuevos procesos.
Los ingenieros genéticos deben aislar el gen que desean insertar en el organismo huésped.
El gen se combina entonces con otros elementos genéticos, incluyendo una región promotora y terminadora y un marcador seleccionable.
El ADN generalmente se inserta en las células animales mediante microinyección, donde se puede inyectar a través de la envoltura nuclear de la célula directamente en el núcleo, o mediante el uso de vectores virales.
En las plantas, esto se logra a través del cultivo de tejidos.
Tradicionalmente, el nuevo material genético se insertó aleatoriamente dentro del genoma del huésped.
Hay cuatro familias de nucleasas diseñadas: meganucleasas, nucleasas de dedos de zinc, nucleasas efectoras de tipo activador de la transcripción (TALEN) y el sistema Cas9-guideRNA (adaptado de CRISPR).
En 1972 Paul Berg creó la primera molécula de ADN recombinante cuando combinó el ADN de un virus mono con el del virus lambda.
Las bacterias que habían incorporado con éxito el plásmido fueron capaces de sobrevivir en presencia de kanamicina.
En 1974, Rudolf Jaenisch creó un ratón transgénico mediante la introducción de ADN extraño en su embrión, convirtiéndolo en el primer animal transgénico del mundo.
Los ratones con genes eliminados (denominados ratones knockout) se crearon en 1989.
En 1983 la primera planta genéticamente modificada fue desarrollada por Michael W. Bevan, Richard B. Flavell y Mary-Dell Chilton.
En 2000, el arroz dorado enriquecido con vitamina A fue la primera planta desarrollada con un mayor valor nutritivo.
La insulina producida por la bacteria, llamada humulin, fue aprobada para su liberación por la Administración de Alimentos y Medicamentos en 1982.
En 1994 Calgene obtuvo la aprobación para liberar comercialmente el tomate Flavr Savr, el primer alimento genéticamente modificado.
En 2010, los científicos del Instituto J. Craig Venter anunciaron que habían creado el primer genoma bacteriano sintético.
Fue lanzado al mercado estadounidense en 2003.
Los genes y otra información genética de una amplia gama de organismos pueden añadirse a un plásmido e insertarse en bacterias para su almacenamiento y modificación.
Un gran número de plásmidos personalizados hacen que la manipulación del ADN extraído de bacterias sea relativamente fácil.
Los científicos pueden manipular y combinar fácilmente genes dentro de las bacterias para crear proteínas nuevas o alteradas y observar el efecto que esto tiene en varios sistemas moleculares.
Las bacterias se han utilizado en la producción de alimentos durante mucho tiempo, y se han desarrollado y seleccionado cepas específicas para ese trabajo a escala industrial.
La mayoría de las bacterias productoras de alimentos son bacterias del ácido láctico, y aquí es donde se ha ido la mayoría de las investigaciones sobre la ingeniería genética de bacterias productoras de alimentos.
La mayoría se producen en los EE.UU. y aunque existen regulaciones para permitir la producción en Europa, a partir de 2015 no hay productos alimenticios derivados de bacterias disponibles actualmente allí.
A continuación, se recogen las bacterias y se purifica la proteína deseada a partir de ellas.
Muchas de estas proteínas son imposibles o difíciles de obtener a través de métodos naturales y es menos probable que estén contaminadas con patógenos, lo que las hace más seguras.
Fuera de la medicina se han utilizado para producir biocombustibles.
Las ideas incluyen la alteración de las bacterias intestinales para que destruyan las bacterias dañinas, o el uso de bacterias para reemplazar o aumentar las enzimas o proteínas deficientes.
Permitir que las bacterias formen una colonia podría proporcionar una solución a más largo plazo, pero también podría plantear problemas de seguridad, ya que las interacciones entre las bacterias y el cuerpo humano son menos conocidas que con los medicamentos tradicionales.
Durante más de un siglo, las bacterias se han utilizado en la agricultura.
Con los avances en ingeniería genética, estas bacterias han sido manipuladas para aumentar la eficiencia y ampliar el rango del huésped.
Las cepas de bacterias Pseudomonas causan daño por heladas al nuclear agua en cristales de hielo a su alrededor.
Otros usos para las bacterias genéticamente modificadas incluyen la biorremediación, donde las bacterias se utilizan para convertir los contaminantes en una forma menos tóxica.
En la década de 1980, el artista Jon Davis y la genetista Dana Boyd convirtieron el símbolo germánico de la feminidad en código binario y luego en una secuencia de ADN, que luego se expresó en Escherichia coli.
Los investigadores pueden usar esto para controlar varios factores, incluida la ubicación objetivo, el tamaño de la inserción y la duración de la expresión génica.
Aunque principalmente todavía en las etapas de prueba, ha habido algunos éxitos utilizando la terapia génica para reemplazar los genes defectuosos.
A partir de 2018, hay un número sustancial de ensayos clínicos en curso, incluidos tratamientos para la hemofilia, el glioblastoma, la enfermedad granulomatosa crónica, la fibrosis quística y varios cánceres.
Los virus del herpes simple producen vectores prometedores, que tienen una capacidad de carga de más de 30 kb y proporcionan expresión a largo plazo, aunque son menos eficientes en el suministro de genes que otros vectores.
Otros virus que se han usado como vectores incluyen alfavirus, flavivirus, virus del sarampión, rabdovirus, virus de la enfermedad de Newcastle, poxvirus y picornavirus.
Esto no afecta la infectividad del virus, invoca una respuesta inmune natural y no hay posibilidad de que recuperen su función de virulencia, lo que puede ocurrir con algunas otras vacunas.
La vacuna más eficaz contra la tuberculosis, la vacuna Bacillus Calmette-Guérin (BCG), solo proporciona protección parcial.
Ya se han aprobado otras vacunas basadas en vectores y se están desarrollando muchas más.
En 2004, los investigadores informaron que un virus genéticamente modificado que explota el comportamiento egoísta de las células cancerosas podría ofrecer una forma alternativa de matar tumores.
El virus se inyectó en los naranjos para combatir la enfermedad de los cítricos que había reducido la producción de naranja en un 70% desde 2005.
Los virus modificados genéticamente que hacen que los animales objetivo sean infértiles a través de la inmunoconcepción se han creado en el laboratorio, así como otros que se dirigen a la etapa de desarrollo del animal.
Se ha propuesto la modificación genética del virus del mixoma para conservar los conejos salvajes europeos en la península ibérica y para ayudar a regularlos en Australia.
Es posible diseñar bacteriófagos para expresar proteínas modificadas en su superficie y unirlas en patrones específicos (una técnica llamada presentación en fagos).
Para aplicaciones industriales, las levaduras combinan las ventajas bacterianas de ser un organismo unicelular que es fácil de manipular y crecer con las modificaciones avanzadas de proteínas que se encuentran en los eucariotas.
Uno tiene una mayor eficiencia de fermentación maloláctica, mientras que el otro evita la producción de compuestos peligrosos de carbamato de etilo durante la fermentación.
A diferencia de las bacterias y los virus, tienen la ventaja de infectar a los insectos solo por contacto, aunque son superados en eficiencia por los pesticidas químicos.
Un objetivo atractivo para el control biológico son los mosquitos, vectores de una variedad de enfermedades mortales, incluida la malaria, la fiebre amarilla y la fiebre del dengue.
Otra estrategia es agregar proteínas a los hongos que bloquean la transmisión de la malaria o eliminar el Plasmodium por completo.
Muchas plantas son pluripotentes, lo que significa que una sola célula de una planta madura puede ser cosechada y en las condiciones adecuadas puede convertirse en una nueva planta.
Los principales avances en el cultivo de tejidos y los mecanismos celulares de las plantas para una amplia gama de plantas se han originado a partir de sistemas desarrollados en el tabaco.
Otro organismo modelo importante relevante para la ingeniería genética es Arabidopsis thaliana.
En la investigación, las plantas están diseñadas para ayudar a descubrir las funciones de ciertos genes.
A diferencia de la mutagénesis, la ingeniería genética permite la eliminación dirigida sin alterar otros genes en el organismo.
Otras estrategias incluyen unir el gen a un promotor fuerte y ver qué sucede cuando se sobreexpresa, obligando a que un gen se exprese en una ubicación diferente o en diferentes etapas de desarrollo.
Los primeros ornamentales modificados genéticamente comercializaron colores alterados.
Otros ornamentales modificados genéticamente incluyen Crisantemo y Petunia.
El virus de la mancha anular de la papaya devastó los árboles de papaya en Hawai en el siglo XX hasta que las plantas de papaya transgénicas recibieron resistencia derivada de patógenos.
La segunda generación de cultivos tuvo como objetivo mejorar la calidad, a menudo alterando el perfil de nutrientes.
Los cultivos transgénicos contribuyen mejorando las cosechas a través de la reducción de la presión de los insectos, aumentando el valor de los nutrientes y tolerando diferentes tensiones abióticas.
La mayoría de los cultivos transgénicos se han modificado para ser resistentes a herbicidas seleccionados, generalmente a base de glifosato o glufosinato.
Algunos usan los genes que codifican las proteínas insecticidas vegetativas.
Menos del uno por ciento de los cultivos transgénicos contenían otros rasgos, que incluyen proporcionar resistencia a los virus, retrasar la senescencia y alterar la composición de las plantas.
Las plantas y las células vegetales se han modificado genéticamente para la producción de productos biofarmacéuticos en biorreactores, un proceso conocido como pharming.
Muchos medicamentos también contienen ingredientes vegetales naturales y las vías que conducen a su producción han sido alteradas genéticamente o transferidas a otras especies de plantas para producir un mayor volumen.
También presentan menos riesgo de estar contaminados.
Las vacunas son costosas de producir, transportar y administrar, por lo que tener un sistema que pueda producirlas localmente permitiría un mayor acceso a las áreas más pobres y en desarrollo.
El almacenamiento en plantas reduce el costo a largo plazo, ya que pueden diseminarse sin necesidad de almacenamiento en frío, no necesitan purificarse y tienen estabilidad a largo plazo.
A partir de 2018 solo se han aprobado tres animales genéticamente modificados, todos en los Estados Unidos.
Los primeros mamíferos transgénicos se produjeron inyectando ADN viral en embriones y luego implantando los embriones en hembras.
El desarrollo del sistema de edición de genes CRISPR-Cas9 como una forma barata y rápida de modificar directamente las células germinales, reduciendo a la mitad la cantidad de tiempo necesaria para desarrollar mamíferos genéticamente modificados.
Los ratones modificados genéticamente han sido los mamíferos más comunes utilizados en la investigación biomédica, ya que son baratos y fáciles de manipular.
En 2009, los científicos anunciaron que habían transferido con éxito un gen a una especie de primate (marmosets) por primera vez.
Se ha logrado una expresión estable en ovejas, cerdos, ratas y otros animales.
La alfa-1-antitripsina humana es otra proteína que se ha producido a partir de cabras y se utiliza en el tratamiento de los seres humanos con esta deficiencia.
Los pulmones de cerdo de cerdos modificados genéticamente están siendo considerados para trasplante en humanos.
Los animales han sido diseñados para crecer más rápido, ser más saludables y resistir enfermedades.
Se creó un cerdo transgénico llamado Enviropig con la capacidad de digerir el fósforo de la planta de manera más eficiente que los cerdos convencionales.
Esto podría beneficiar a las madres que no pueden producir leche materna, pero quieren que sus hijos tengan leche materna en lugar de fórmula.
Se ha sugerido que la ingeniería genética podría usarse para sacar a los animales de la extinción.
Se ha utilizado para tratar trastornos genéticos como la inmunodeficiencia combinada grave y la amaurosis congénita de Leber.
La terapia génica de línea germinal da como resultado que cualquier cambio sea heredable, lo que ha planteado preocupaciones dentro de la comunidad científica.
La acuicultura es una industria en crecimiento, que actualmente proporciona más de la mitad del pescado consumido en todo el mundo.
Varios grupos han estado desarrollando peces cebra para detectar la contaminación mediante la unión de proteínas fluorescentes a genes activados por la presencia de contaminantes.
Originalmente fue desarrollado por uno de los grupos para detectar la contaminación, pero ahora es parte del comercio de peces ornamentales, convirtiéndose en el primer animal modificado genéticamente en estar disponible públicamente como mascota cuando en 2003 se introdujo para la venta en los Estados Unidos.
Los peces cebra son organismos modelo para los procesos de desarrollo, la regeneración, la genética, el comportamiento, los mecanismos de la enfermedad y las pruebas de toxicidad.
Los peces transgénicos se han desarrollado con promotores que impulsan una sobreproducción de hormona de crecimiento para su uso en la industria acuícola para aumentar la velocidad de desarrollo y potencialmente reducir la presión pesquera sobre las poblaciones silvestres.
Obtuvo la aprobación regulatoria en 2015, el primer alimento transgénico no vegetal en ser comercializado.
Drosophila se han utilizado para estudiar la genética y la herencia, el desarrollo embrionario, el aprendizaje, el comportamiento y el envejecimiento.
Los mosquitos resistentes a la malaria se han desarrollado en el laboratorio mediante la inserción de un gen que reduce el desarrollo del parásito de la malaria y luego el uso de endonucleasas de orientación para propagar rápidamente ese gen en toda la población masculina (conocido como impulso genético).
Otro enfoque es usar una técnica de insectos estériles, mediante la cual los machos genéticamente modificados para ser estériles compiten con los machos viables, para reducir el número de poblaciones.
El enfoque es similar a la técnica estéril probada en mosquitos, donde los machos se transforman con un gen que impide que las hembras nacidas alcancen la madurez.
En este caso, una cepa de gusano rosado que se esterilizó con radiación se modificó genéticamente para expresar una proteína fluorescente roja, lo que facilitó a los investigadores su monitoreo.
También existe la posibilidad de utilizar la maquinaria de producción de seda para producir otras proteínas valiosas.
Una gallina transgénica que produce el medicamento Kanuma, una enzima que trata una condición rara, en su huevo pasó la aprobación regulatoria de los Estados Unidos en 2015.
Hay propuestas para utilizar la ingeniería genética para controlar los sapos de caña en Australia.
También es relativamente fácil producir nematodos transgénicos estables y esto junto con la iARN son las principales herramientas utilizadas en el estudio de sus genes.
Los nematodos transgénicos se han utilizado para estudiar virus, toxicología, enfermedades y para detectar contaminantes ambientales.
Los gusanos planos tienen la capacidad de regenerarse a partir de una sola célula.
El gusano de cerdas, un anélido marino, ha sido modificado.
El desarrollo de un marco regulatorio sobre ingeniería genética comenzó en 1975, en Asilomar, California.
Es un tratado internacional que rige la transferencia, manejo y uso de organismos genéticamente modificados.
Muchos experimentos también necesitan el permiso de un grupo regulador nacional o legislación.
Existe un sistema casi universal para evaluar los riesgos relativos asociados con los OMG y otros agentes para el personal de laboratorio y la comunidad.
Diferentes países utilizan diferentes nomenclaturas para describir los niveles y pueden tener diferentes requisitos para lo que se puede hacer en cada nivel.
Por ejemplo, un cultivo no destinado al uso alimentario generalmente no es revisado por las autoridades responsables de la seguridad alimentaria.
La mayoría de los países que no permiten el cultivo de OGM permiten la investigación con OGM.
Si bien solo se han aprobado unos pocos OMG para el cultivo en la UE, se han aprobado varios OMG para la importación y el procesamiento.
La política de los Estados Unidos no se centra tanto en el proceso como otros países, analiza los riesgos científicos verificables y utiliza el concepto de equivalencia sustancial.
Una de las cuestiones clave con respecto a los reguladores es si los productos GM deben ser etiquetados.
La disputa involucra a consumidores, productores, compañías de biotecnología, reguladores gubernamentales, organizaciones no gubernamentales y científicos.
La mayoría de las preocupaciones giran en torno a los efectos sobre la salud y el medio ambiente de los OGM.
Sin embargo, los miembros del público son mucho menos propensos que los científicos a percibir los alimentos transgénicos como seguros.
El flujo de genes entre los cultivos transgénicos y las plantas compatibles, junto con un mayor uso de herbicidas de amplio espectro, puede aumentar el riesgo de poblaciones de malezas resistentes a los herbicidas.
Con el fin de abordar algunas de estas preocupaciones, se han desarrollado algunos OGM con rasgos para ayudar a controlar su propagación.
Otras preocupaciones ambientales y agronómicas incluyen una disminución de la biodiversidad, un aumento de plagas secundarias (plagas no dirigidas) y la evolución de plagas de insectos resistentes.
El impacto de los cultivos de Bt en organismos beneficiosos no objetivo se convirtió en un problema público después de que un artículo de 1999 sugiriera que podrían ser tóxicos para las mariposas monarca.
Con la capacidad de diseñar genéticamente a los humanos, ahora hay preocupaciones éticas sobre hasta dónde debería llegar esta tecnología, o si debería usarse en absoluto.
Octubre de 2006 el rigor del proceso regulatorio, la consolidación del control del suministro de alimentos en las empresas que fabrican y venden OMG, la exageración de los beneficios de la modificación genética, o las preocupaciones sobre el uso de herbicidas con glifosato.
Los OGM llegaron a la escena cuando la confianza pública en la seguridad alimentaria, atribuida a recientes temores alimentarios como la encefalopatía espongiforme bovina y otros escándalos relacionados con la regulación gubernamental de productos en Europa, fue baja.
La ingeniería genética, también llamada modificación genética o manipulación genética, es la manipulación directa de los genes de un organismo utilizando biotecnología.
Normalmente se crea una construcción y se usa para insertar este ADN en el organismo huésped.
El nuevo ADN puede insertarse aleatoriamente o dirigirse a una parte específica del genoma.
Rudolf Jaenisch creó el primer animal transgénico cuando insertó ADN extraño en un ratón en 1974.
Los alimentos genéticamente modificados se han vendido desde 1994, con la liberación del tomate Flavr Savr.
En 2016 se vendió salmón modificado con una hormona de crecimiento.
Al eliminar los genes responsables de ciertas condiciones, es posible crear organismos modelo animales de enfermedades humanas.
El aumento de los cultivos genéticamente modificados comercializados ha proporcionado beneficios económicos a los agricultores en muchos países diferentes, pero también ha sido la fuente de la mayor parte de la controversia que rodea a la tecnología.
El flujo de genes, el impacto en los organismos no objetivo, el control del suministro de alimentos y los derechos de propiedad intelectual también se han planteado como posibles problemas.
Esto es mucho más rápido, se puede usar para insertar cualquier gen de cualquier organismo (incluso los de diferentes dominios) y evita que también se agreguen otros genes indeseables.
Los medicamentos, vacunas y otros productos han sido cosechados a partir de organismos diseñados para producirlos.
La biología sintética es una disciplina emergente que lleva la ingeniería genética un paso más allá al introducir material sintetizado artificialmente en un organismo.
Si se añade material genético de otra especie al huésped, el organismo resultante se denomina transgénico.
En 1973 Herbert Boyer y Stanley Cohen crearon el primer organismo transgénico insertando genes de resistencia a antibióticos en el plásmido de una bacteria Escherichia coli.
En 1976 Genentech, la primera compañía de ingeniería genética, fue fundada por Herbert Boyer y Robert Swanson y un año más tarde la compañía produjo una proteína humana (somatostatina) en E. coli.
La insulina producida por las bacterias fue aprobada para su liberación por la Administración de Alimentos y Medicamentos (FDA) en 1982.
La República Popular de Chína fue el primer país en comercializar plantas transgénicas, introduciendo un tabaco resistente a los virus en 1992.
En 1995, Bt Potato fue aprobado como seguro por la Agencia de Protección Ambiental, después de haber sido aprobado por la FDA, lo que lo convierte en el primer cultivo productor de pesticidas en ser aprobado en los Estados Unidos.
Se pueden realizar exámenes genéticos para determinar genes potenciales y luego se pueden usar pruebas adicionales para identificar a los mejores candidatos.
Estos segmentos se pueden extraer a través de electroforesis en gel.
Una vez aislado, el gen se liga en un plásmido que luego se inserta en una bacteria.
Estos incluyen una región promotora y terminadora, que inicia y finaliza la transcripción.
Esta capacidad puede ser inducida en otras bacterias a través del estrés (por ejemplo, choque térmico o eléctrico), lo que aumenta la permeabilidad de la membrana celular al ADN; el ADN captado puede integrarse con el genoma o existir como ADN extracromosómico.
En las plantas, el ADN a menudo se inserta utilizando la transformación mediada por Agrobacterium, aprovechando la secuencia de ADN-T de Agrobacterium que permite la inserción natural de material genético en las células vegetales.
En las plantas, esto se logra mediante el uso de cultivo de tejidos.
Los marcadores seleccionables se utilizan para diferenciar fácilmente las células transformadas de las no transformadas.
Estas pruebas también pueden confirmar la ubicación cromosómica y el número de copias del gen insertado.
El nuevo material genético puede insertarse aleatoriamente dentro del genoma del huésped o dirigirse a una ubicación específica.
La frecuencia de direccionamiento de genes puede mejorarse en gran medida a través de la edición del genoma.
TALEN y CRISPR son los dos más utilizados y cada uno tiene sus propias ventajas.
La mayoría de los OGM comercializados son plantas de cultivo resistentes a insectos o tolerantes a herbicidas.
Los hibridomas de ratón, células fusionadas entre sí para crear anticuerpos monoclonales, se han adaptado mediante ingeniería genética para crear anticuerpos monoclonales humanos.
La ingeniería genética también se utiliza para crear modelos animales de enfermedades humanas.
Las curaciones potenciales se pueden probar contra estos modelos de ratón.
En 2015, se utilizó un virus para insertar un gen sano en las células de la piel de un niño que sufría de una enfermedad cutánea rara, la epidermólisis ampollosa, para crecer y luego injertar piel sana en el 80 por ciento del cuerpo del niño que se vio afectado por la enfermedad.
También existe la preocupación de que la tecnología pueda usarse no solo para el tratamiento, sino también para la mejora, modificación o alteración de la apariencia, adaptabilidad, inteligencia, carácter o comportamiento de un ser humano.
Dijo que las gemelas, Lulu y Nana, habían nacido unas semanas antes.
Actualmente, la modificación de la línea germinal está prohibida en 40 países.
Las bacterias son baratas, fáciles de cultivar, clonales, se multiplican rápidamente, son relativamente fáciles de transformar y se pueden almacenar a -80 ° C casi indefinidamente.
Este podría ser el efecto sobre el fenotipo del organismo, dónde se expresa el gen o con qué otros genes interactúa.
En un simple knockout se ha alterado una copia del gen deseado para hacerlo no funcional.
Esto permite al experimentador analizar los defectos causados por esta mutación y, por lo tanto, determinar el papel de genes particulares.
El método más simple, y el primero en ser utilizado, es la "exploración de alanina", donde cada posición a su vez se muta al aminoácido no reactivo alanina.
El proceso es muy similar al de la ingeniería knockout, excepto que la construcción está diseñada para aumentar la función del gen, normalmente proporcionando copias adicionales del gen o induciendo la síntesis de la proteína con mayor frecuencia.
Una forma de hacer esto es reemplazar el gen de tipo silvestre con un gen de 'fusión', que es una yuxtaposición del gen de tipo silvestre con un elemento informador tal como la proteína fluorescente verde (GFP) que permitirá una fácil visualización de los productos de la modificación genética.
Los estudios de expresión tienen como objetivo descubrir dónde y cuándo se producen proteínas específicas.
Algunos genes no funcionan bien en bacterias, por lo que también se pueden usar levaduras, células de insectos o células de mamíferos.
Ciertos microbios modificados genéticamente también se pueden usar en biominería y biorremediación, debido a su capacidad para extraer metales pesados de su entorno e incorporarlos en compuestos que son más fáciles de recuperar.
También se han desarrollado o están en desarrollo cultivos resistentes a hongos y virus.
En 2016, el salmón ha sido modificado genéticamente con hormonas de crecimiento para alcanzar el tamaño normal de un adulto mucho más rápido.
La soja y la canola han sido modificadas genéticamente para producir aceites más saludables.
La transferencia de genes a través de vectores virales se ha propuesto como un medio para controlar las especies invasoras, así como para vacunar a la fauna amenazada de la enfermedad.
Las aplicaciones de la ingeniería genética en la conservación son hasta ahora en su mayoría teóricas y aún no se han puesto en práctica.
La reunión de Asilomar recomendó un conjunto de directrices voluntarias sobre el uso de la tecnología recombinante.
Ciento cincuenta y siete países son miembros del Protocolo y muchos lo utilizan como punto de referencia para sus propias regulaciones.
La mayoría de los países que no permiten el cultivo de OGM permiten la investigación.
Emily Marden, Risk and Regulation: U.S. Regulatory Policy on Genetically Modified Food and Agriculture, 44 B.C.L. Rev. 733 (2003) La Unión Europea, por el contrario, tiene posiblemente las regulaciones de OGM más estrictas del mundo.
Una de las cuestiones clave con respecto a los reguladores es si los productos GM deben ser etiquetados.
Estas controversias han llevado a litigios, disputas comerciales internacionales y protestas, y a la regulación restrictiva de productos comerciales en algunos países.
Aunque se han planteado dudas, económicamente la mayoría de los estudios han encontrado que los cultivos transgénicos son beneficiosos para los agricultores.
Muchos de los impactos ambientales con respecto a los cultivos transgénicos pueden tardar muchos años en entenderse y también son evidentes en las prácticas agrícolas convencionales.
Pocas películas han informado al público sobre la ingeniería genética, con la excepción de The Boys de Brasil de 1978 y Jurassic Park de 1993, que hicieron uso de una lección, una demostración y un clip de película científica.
La nanotecnología, también abreviada como nanotecnología, es el uso de la materia a escala atómica, molecular y supramolecular para fines industriales.
Esta definición refleja el hecho de que los efectos de la mecánica cuántica son importantes en esta escala del reino cuántico, por lo que la definición pasó de un objetivo tecnológico particular a una categoría de investigación que incluye todos los tipos de investigación y tecnologías que se ocupan de las propiedades especiales de la materia que ocurren por debajo del umbral de tamaño dado.
La investigación y las aplicaciones asociadas son igualmente diversas, desde extensiones de la física de dispositivos convencionales hasta enfoques completamente nuevos basados en el autoensamblaje molecular, desde el desarrollo de nuevos materiales con dimensiones en la nanoescala hasta el control directo de la materia en la escala atómica.
El término "nanotecnología" fue utilizado por primera vez por Norio Taniguchi en 1974, aunque no era ampliamente conocido.
La aparición de la nanotecnología como un campo en la década de 1980 se produjo a través de la convergencia de la obra teórica y pública de Drexler, que desarrolló y popularizó un marco conceptual para la nanotecnología, y los avances experimentales de alta visibilidad que llamaron la atención a gran escala adicional a las perspectivas de control atómico de la materia.
Los desarrolladores del microscopio Gerd Binnig y Heinrich Rohrer en IBM Zurich Research Laboratory recibieron el Premio Nobel de Física en 1986.
C60 no se describió inicialmente como nanotecnología; el término se usó con respecto al trabajo posterior con nanotubos de carbono relacionados (a veces llamados tubos de grafeno o tubos Bucky) que sugerían posibles aplicaciones para la electrónica y dispositivos a nanoescala.
Décadas más tarde, los avances en la tecnología multi-puerta permitieron escalar los dispositivos de transistor de efecto de campo de óxido metálico (MOSFET) hasta niveles de nano-escala menores que la longitud de puerta de 20 nm, comenzando con el FinFET (transistor de efecto de campo de aleta), un MOSFET de doble puerta tridimensional, no plano.
Surgieron controversias con respecto a las definiciones y las posibles implicaciones de las nanotecnologías, ejemplificadas por el informe de la Royal Society sobre nanotecnología.
Estos productos se limitan a aplicaciones a granel de nanomateriales y no implican el control atómico de la materia.
Se basa en la tecnología gate-all-around (GAA) FinFET.
Esto abarca tanto el trabajo actual como los conceptos que están más avanzados.
El límite inferior se establece por el tamaño de los átomos (el hidrógeno tiene los átomos más pequeños, que son aproximadamente un cuarto de un diámetro cinético nm) ya que la nanotecnología debe construir sus dispositivos a partir de átomos y moléculas.
Para poner esa escala en otro contexto, el tamaño comparativo de un nanómetro a un metro es el mismo que el de un mármol al tamaño de la tierra.
En el enfoque "de abajo hacia arriba", los materiales y dispositivos se construyen a partir de componentes moleculares que se ensamblan químicamente por principios de reconocimiento molecular.
Un ejemplo es el aumento en la relación de superficie a volumen que altera las propiedades mecánicas, térmicas y catalíticas de los materiales.
La actividad catalítica de los nanomateriales también abre riesgos potenciales en su interacción con los biomateriales.
El concepto de reconocimiento molecular es especialmente importante: las moléculas pueden diseñarse de modo que se favorezca una configuración o disposición específica debido a fuerzas intermoleculares no covalentes.
Tales enfoques de abajo hacia arriba deberían ser capaces de producir dispositivos en paralelo y ser mucho más baratos que los métodos de arriba hacia abajo, pero podrían ser potencialmente abrumados a medida que aumenta el tamaño y la complejidad del conjunto deseado.
La fabricación en el contexto de nanosistemas productivos no está relacionada con, y debe distinguirse claramente de, las tecnologías convencionales utilizadas para fabricar nanomateriales tales como nanotubos de carbono y nanopartículas.
Se espera que los desarrollos en nanotecnología hagan posible su construcción por algún otro medio, tal vez utilizando principios biomiméticos.
En general, es muy difícil ensamblar dispositivos a escala atómica, ya que uno tiene que colocar átomos en otros átomos de tamaño y pegajosidad comparables.
Esto llevó a un intercambio de cartas en la publicación de ACS Chemical & Engineering News en 2003.
Han construido al menos tres dispositivos moleculares distintos cuyo movimiento se controla desde el escritorio con voltaje cambiante: un nanomotor de nanotubos, un actuador molecular y un oscilador de relajación nanoelectromecánico.
Los nanomateriales con transporte rápido de iones también están relacionados con la nanoiónica y la nanoelectrónica.
Los materiales a nanoescala como los nanopilares a veces se usan en células solares que combaten el costo de las células solares de silicio tradicionales.
Más generalmente, el autoensamblaje molecular busca usar conceptos de química supramolecular, y reconocimiento molecular en particular, para hacer que los componentes de una sola molécula se organicen automáticamente en alguna conformación útil.
Los discos duros gigantes basados en magnetorresistencia que ya están en el mercado se ajustan a esta descripción, al igual que las técnicas de deposición de capa atómica (ALD).
Los haces de iones enfocados pueden eliminar directamente el material, o incluso depositar material cuando se aplican gases precursores adecuados al mismo tiempo.
Estos podrían ser utilizados como componentes de una sola molécula en un dispositivo nanoelectrónico.
La nanotecnología molecular es un enfoque propuesto que implica manipular moléculas individuales de manera determinista y finamente controlada.
Hay esperanzas de aplicar nanorobots en medicina.
Debido a la naturaleza discreta (es decir, atómica) de la materia y la posibilidad de crecimiento exponencial, esta etapa se ve como la base de otra revolución industrial.
Con la disminución de la dimensionalidad, se observa un aumento en la relación superficie-volumen.
Aunque conceptualmente similar al microscopio confocal de barrido desarrollado por Marvin Minsky en 1961 y el microscopio acústico de barrido (SAM) desarrollado por Calvin Quate y sus compañeros de trabajo en la década de 1970, los microscopios de sonda de barrido más nuevos tienen una resolución mucho mayor, ya que no están limitados por la longitud de onda del sonido o la luz.
Sin embargo, esto sigue siendo un proceso lento debido a la baja velocidad de barrido del microscopio.
Otro grupo de técnicas nanotecnológicas incluyen las usadas para la fabricación de nanotubos y nanocables, las usadas en la fabricación de semiconductores tales como litografía ultravioleta profunda, litografía de haz de electrones, mecanizado de haz de iones enfocado, litografía de nanoimpresión, deposición de capa atómica y deposición de vapor molecular, y que incluyen además técnicas de autoensamblaje molecular tales como las que emplean copolímeros de dos bloques.
La microscopía de sonda de barrido es una técnica importante tanto para la caracterización como para la síntesis de nanomateriales.
Usando, por ejemplo, un enfoque de escaneo orientado a características, los átomos o moléculas pueden moverse sobre una superficie con técnicas de microscopía de sonda de escaneo.
Estas técnicas incluyen síntesis química, autoensamblaje y ensamblaje posicional.
Investigadores de Bell Telephone Laboratories como John R. Arthur.
MBE permite a los científicos establecer capas atómicamente precisas de átomos y, en el proceso, construir estructuras complejas.
Los vendajes están siendo infundidos con nanopartículas de plata para curar los cortes más rápido.
La nanotecnología puede hacer que las aplicaciones médicas existentes sean más baratas y fáciles de usar en lugares como la oficina del médico general y en el hogar.
El platino se utiliza actualmente como catalizador de motores diesel en estos motores.
A continuación, el catalizador de oxidación oxida los hidrocarburos y el monóxido de carbono para formar dióxido de carbono y agua.
La compañía danesa InnovationsFonden invirtió 15 millones de coronas danesas en la búsqueda de nuevos sustitutos de catalizadores utilizando nanotecnología.
Si se maximiza el área de superficie del catalizador que está expuesta a los gases de escape, se maximiza la eficiencia del catalizador.
Por lo tanto, la creación de estas nanopartículas aumentará la efectividad del catalizador del motor diesel resultante, lo que a su vez conducirá a gases de escape más limpios, y disminuirá el costo.
Al diseñar andamios, los investigadores intentan imitar las características a nanoescala del microambiente de una célula para dirigir su diferenciación hacia un linaje adecuado.
TSMC comenzó la producción de un proceso de 7 nm en 2017, y Samsung comenzó la producción de un proceso de 5 nm en 2018.
Por estas razones, algunos grupos abogan por que la nanotecnología sea regulada por los gobiernos.
Algunos productos de nanopartículas pueden tener consecuencias no deseadas.
La inhalación de nanopartículas y nanofibras en el aire puede conducir a una serie de enfermedades pulmonares, por ejemplo, fibrosis.
Un importante estudio publicado más recientemente en Nature Nanotechnology sugiere que algunas formas de nanotubos de carbono, un ejemplo de la "revolución de la nanotecnología", podrían ser tan dañinas como el asbesto si se inhalan en cantidades suficientes.
Davies (2008) ha propuesto una hoja de ruta reguladora que describe los pasos para hacer frente a estas deficiencias.
Como resultado, algunos académicos han pedido una aplicación más estricta del principio de precaución, con una aprobación de comercialización retrasada, un etiquetado mejorado y requisitos adicionales de desarrollo de datos de seguridad en relación con ciertas formas de nanotecnología.
La tecnología nuclear es una tecnología que involucra las reacciones nucleares de los núcleos atómicos.
Él, Pierre Curie y Marie Curie comenzaron a investigar el fenómeno.
Algunos de estos tipos de radiación podrían pasar a través de la materia ordinaria, y todos ellos podrían ser dañinos en grandes cantidades.
Poco a poco se dio cuenta de que la radiación producida por la desintegración radiactiva era radiación ionizante, y que incluso cantidades demasiado pequeñas para quemar podrían representar un grave peligro a largo plazo.
A medida que el átomo llegó a ser mejor entendido, la naturaleza de la radiactividad se hizo más clara.
La desintegración alfa es cuando un núcleo libera una partícula alfa, que es dos protones y dos neutrones, equivalente a un núcleo de helio.
Este tipo de radiación es la más peligrosa y la más difícil de bloquear.
El número promedio de neutrones liberados por núcleo que pasan a la fisión de otro núcleo se conoce como k. Los valores de k mayores que 1 significan que la reacción de fisión está liberando más neutrones de los que absorbe, y por lo tanto se conoce como una reacción en cadena autosostenible.
Si hay suficientes decaimientos inmediatos para llevar a cabo la reacción en cadena, se dice que la masa es rápidamente crítica, y la liberación de energía crecerá rápida e incontrolablemente, lo que generalmente conduce a una explosión.
Durante el proyecto, también se desarrollaron los primeros reactores de fisión, aunque eran principalmente para la fabricación de armas y no generaban electricidad.
Sin embargo, si la masa es crítica solo cuando se incluyen los neutrones retardados, entonces la reacción puede controlarse, por ejemplo, mediante la introducción o eliminación de absorbentes de neutrones.
Cuando el núcleo resultante es más ligero que el del hierro, normalmente se libera energía; cuando el núcleo es más pesado que el del hierro, generalmente se absorbe energía.
La abundancia restante de elementos pesados, desde níquel hasta uranio y más allá, se debe a la nucleosíntesis de supernova, el proceso R.
Las bombas de hidrógeno obtienen su enorme poder destructivo de la fusión, pero su energía no puede ser controlada.
Sin embargo, ambos dispositivos funcionan con una pérdida neta de energía.
La fusión nuclear se llevó a cabo inicialmente solo en etapas teóricas durante la Segunda Guerra Mundial, cuando los científicos del Proyecto Manhattan (dirigido por Edward Teller) lo investigaron como un método para construir una bomba.
Incluso pequeños dispositivos nucleares pueden devastar una ciudad por explosión, fuego y radiación.
Tal arma debe mantener una o más masas fisibles subcríticas estables para el despliegue, luego inducir la criticidad (crear una masa crítica) para la detonación.
Un isótopo del uranio, a saber, el uranio-235, es de origen natural y lo suficientemente inestable, pero siempre se encuentra mezclado con el isótopo más estable uranio-238.
Alternativamente, el elemento plutonio posee un isótopo que es suficientemente inestable para que este proceso sea utilizable.
Detonaron la primera arma nuclear en una prueba llamada "Trinity", cerca de Alamogordo, Nuevo México, el 16 de julio de 1945.
A raíz de la devastación sin precedentes y las bajas de una sola arma, el gobierno japonés pronto se rindió, poniendo fin a la Segunda Guerra Mundial.
Poco más de cuatro años después, el 29 de agosto de 1949, la Unión Soviética detonó su primera arma de fisión.
Un arma radiológica es un tipo de arma nuclear diseñada para distribuir material nuclear peligroso en áreas enemigas.
Aunque se considera inútil por un ejército convencional, tal arma plantea preocupaciones sobre el terrorismo nuclear.
El tratado permitió pruebas nucleares subterráneas.
Después de firmar el Tratado de Prohibición Completa de Pruebas en 1996 (que hasta 2011 no había entrado en vigor), todos estos estados se han comprometido a suspender todas las pruebas nucleares.
A lo largo de la Guerra Fría, las potencias opuestas tenían enormes arsenales nucleares, suficientes para matar a cientos de millones de personas.
Actualmente la energía nuclear proporciona aproximadamente el 15,7% de la electricidad del mundo (en 2004) y se usa para propulsar portaaviones, rompehielos y submarinos (hasta ahora la economía y los miedos en algunos puertos han impedido el uso de la energía nuclear en barcos de transporte).
Radiografías médicas y dentales que utilizan cobalto-60 u otras fuentes de rayos X.
Ambos contienen una pequeña fuente de 241Am que da lugar a una pequeña corriente constante.
Otro uso en el control de insectos es la técnica de insectos estériles, donde los insectos machos son esterilizados por radiación y liberados, por lo que no tienen descendencia, para reducir la población.
Las fuentes de radiación utilizadas incluyen fuentes de rayos gamma de radioisótopos, generadores de rayos X y aceleradores de electrones.
Como tal, también se usa en artículos no alimenticios, como hardware médico, plásticos, tubos para tuberías de gas, mangueras para calefacción de piso, láminas retráctiles para envasado de alimentos, piezas de automóviles, cables y cables (aislamiento), neumáticos e incluso piedras preciosas.
Los microorganismos ya no pueden proliferar y continuar sus actividades malignas o patógenas.
Las plantas no pueden continuar el proceso natural de maduración o envejecimiento.
La especialidad del procesamiento de alimentos por radiación ionizante es el hecho de que la densidad de energía por transición atómica es muy alta, puede escindir moléculas e inducir ionización (de ahí el nombre) que no se puede lograr mediante el simple calentamiento.
Sin embargo, el uso del término, pasteurización en frío, para describir los alimentos irradiados es controvertido, porque la pasteurización y la irradiación son procesos fundamentalmente diferentes, aunque los resultados finales previstos pueden ser similares en algunos casos.
Marie Curie murió de anemia aplásica que resultó de sus altos niveles de exposición.
Aproximadamente la mitad de las muertes de Hiroshima y Nagasaki murieron dos a cinco años después de la exposición a la radiación.
Una fusión nuclear se refiere al peligro más grave de liberar material nuclear en el entorno circundante.
Los reactores militares que experimentaron accidentes similares fueron Windscale en el Reino Unido y SL-1 en los Estados Unidos.
Otro tema de la investigación transhumanista es cómo proteger a la humanidad contra los riesgos existenciales, como la guerra nuclear o la colisión de asteroides.
La afirmación sentaría las bases intelectuales para que el filósofo británico Max More comenzara a articular los principios del transhumanismo como una filosofía futurista en 1990, y organizara en California una escuela de pensamiento que desde entonces se ha convertido en el movimiento transhumanista mundial.
En el Discurso, Descartes imaginó un nuevo tipo de medicina que podría otorgar tanto inmortalidad física como mentes más fuertes.
San León puede haber sido la inspiración para la novela Frankenstein de su hija Mary Shelley.
En particular, se interesó en el desarrollo de la ciencia de la eugenesia, la ectogénesis (creación y mantenimiento de la vida en un entorno artificial) y la aplicación de la genética para mejorar las características humanas, como la salud y la inteligencia.
Estas ideas han sido temas transhumanistas comunes desde entonces.
En la sección Material and Man del manifiesto, Noboru Kawazoe sugiere que: Después de varias décadas, con el rápido progreso de la tecnología de la comunicación, cada uno tendrá un "receptor de ondas cerebrales" en su oído, que transmite directamente y exactamente lo que otras personas piensan de él y viceversa.
En 1966, FM-2030 (anteriormente F. M. Esfandiary), un futurista que enseñó "nuevos conceptos de lo humano" en The New School, en la ciudad de Nueva York, comenzó a identificar a las personas que adoptan tecnologías, estilos de vida y visiones del mundo de transición a la posthumanidad como "transhumanos".
FM-2030 y Vita-More pronto comenzaron a celebrar reuniones para transhumanistas en Los Ángeles, que incluían estudiantes de los cursos de FM-2030 y audiencias de las producciones artísticas de Vita-More.
Una preocupación particular es la igualdad de acceso a las tecnologías de mejora humana a través de clases y fronteras.
Esto dejó a la Asociación Transhumanista Mundial como la principal organización transhumanista internacional.
La Asociación Mormona Transhumanista fue fundada en 2006.
El transhumanismo enfatiza la perspectiva evolutiva, incluyendo a veces la creación de una especie animal altamente inteligente a través de la mejora cognitiva (es decir, elevación biológica), pero se aferra a un "futuro posthumano" como el objetivo final de la evolución participante.
Si bien tal "posthumanismo cultural" ofrecería recursos para repensar las relaciones entre los humanos y las máquinas cada vez más sofisticadas, el transhumanismo y posthumanismos similares no están, en este punto de vista, abandonando conceptos obsoletos del "sujeto liberal autónomo", sino que están expandiendo sus "prerrogativas" al reino de lo posthumano.
Sin embargo, otros progresistas han argumentado que el posthumanismo, ya sean sus formas filosóficas o activistas, equivale a un alejamiento de las preocupaciones sobre la justicia social, de la reforma de las instituciones humanas y de otras preocupaciones de la Ilustración, hacia anhelos narcisistas por una trascendencia del cuerpo humano en busca de formas más exquisitas de ser.
Muchos transhumanistas evalúan activamente el potencial de las tecnologías futuras y los sistemas sociales innovadores para mejorar la calidad de vida, mientras buscan hacer que la realidad material de la condición humana cumpla la promesa de igualdad legal y política al eliminar las barreras mentales y físicas congénitas.
Algunos teóricos como Ray Kurzweil piensan que el ritmo de la innovación tecnológica se está acelerando y que los próximos 50 años pueden producir no solo avances tecnológicos radicales, sino posiblemente una singularidad tecnológica, que puede cambiar fundamentalmente la naturaleza de los seres humanos.
Por ejemplo, Bostrom ha escrito extensamente sobre los riesgos existenciales para el bienestar futuro de la humanidad, incluidos los que podrían ser creados por las tecnologías emergentes.
Para contrarrestar esto, Hawking enfatiza el autodiseño del genoma humano o la mejora mecánica (por ejemplo, la interfaz cerebro-computadora) para mejorar la inteligencia humana y reducir la agresión, sin lo cual implica que la civilización humana puede ser demasiado estúpida colectivamente para sobrevivir a un sistema cada vez más inestable, lo que resulta en un colapso social.
Estos pensadores argumentan que la capacidad de discutir de una manera basada en la falsificación constituye un umbral que no es arbitrario en el que se hace posible que un individuo hable por sí mismo de una manera que no depende de suposiciones exteriores.
De acuerdo con esto, muchos defensores transhumanistas prominentes, como Dan Agin, se refieren a los críticos del transhumanismo, en la derecha política y la izquierda conjuntamente, como "bioconservadores" o "bioluditas", el último término aludiendo al movimiento social antiindustrialización del siglo XIX que se opuso a la sustitución de los trabajadores manuales humanos por máquinas.
El mismo escenario ocurre cuando las personas tienen ciertos implantes neuronales que les dan una ventaja en el lugar de trabajo y en los aspectos educativos.
Inmortalismo, una ideología moral basada en la creencia de que la extensión de la vida radical y la inmortalidad tecnológica es posible y deseable, y aboga por la investigación y el desarrollo para garantizar su realización.
Matemáticas (del griego: ) incluye el estudio de temas tales como la cantidad (teoría de números), la estructura (álgebra), el espacio (geometría), y el cambio (análisis).
Cuando las estructuras matemáticas son buenos modelos de fenómenos reales, el razonamiento matemático se puede utilizar para proporcionar información o predicciones sobre la naturaleza.
La investigación requerida para resolver problemas matemáticos puede llevar años o incluso siglos de investigación sostenida.
Las matemáticas se desarrollaron a un ritmo relativamente lento hasta el Renacimiento, cuando las innovaciones matemáticas que interactúan con los nuevos descubrimientos científicos llevaron a un rápido aumento en la tasa de descubrimiento matemático que ha continuado hasta nuestros días.
Como lo demuestran los recuentos encontrados en los huesos, además de reconocer cómo contar objetos físicos, los pueblos prehistóricos también pueden haber reconocido cómo contar cantidades abstractas, como días, estaciones o años.
Comenzando en el 6to siglo A.C. con Pythagoreans, con matemáticas griegas los griegos Antiguos comenzaron un estudio sistemático de matemáticas como un sujeto en su propio derecho.
A menudo se cree que el matemático más grande de la antig-edad es Arquímedes (c. 287-212 a.C.) de Siracusa.
El sistema numérico hindú-árabe y las reglas para el uso de sus operaciones, en uso en todo el mundo hoy en día, evolucionaron a lo largo del primer milenio dC en la India y se transmitieron al mundo occidental a través de las matemáticas islámicas.
El logro más notable de las matemáticas islámicas fue el desarrollo del álgebra.
Durante el período moderno temprano, las matemáticas comenzaron a desarrollarse a un ritmo acelerado en Europa Occidental.
Quizás el matemático principal del 19no siglo era el matemático alemán Carl Friedrich Gauss, que hizo numerosas contribuciones a campos como álgebra, análisis, geometría diferencial, teoría de la matriz, teoría de números y estadística.
Hoy en día se siguen haciendo descubrimientos matemáticos.
En particular, mathematik tékhn significaba "el arte matemático".
En inglés, el sustantivo matemáticas toma un verbo singular.
Sin embargo, Aristóteles también notó que un enfoque en la cantidad sola no puede distinguir matemáticas de ciencias como la física; en su opinión, la abstracción y el estudio de la cantidad como una propiedad "separable en el pensamiento" de casos reales ponen matemáticas aparte.
Una peculiaridad del intuicionismo es que rechaza algunas ideas matemáticas consideradas válidas de acuerdo con otras definiciones.
Haskell Curry definió las matemáticas simplemente como "la ciencia de los sistemas formales".
Popper también señaló que "Ciertamente admitiré un sistema como empírico o científico solo si es capaz de ser probado por la experiencia".
La intuición y la experimentación también juegan un papel en la formulación de conjeturas tanto en matemáticas como en las (otras) ciencias.
Por ejemplo, el físico Richard Feynman inventó la formulación integral del camino de la mecánica cuántica utilizando una combinación de razonamiento matemático y visión física, y la teoría de cuerdas de hoy, una teoría científica aún en desarrollo que intenta unificar las cuatro fuerzas fundamentales de la naturaleza, continúa inspirando nuevas matemáticas.
A menudo se hace una distinción entre las matemáticas puras y las matemáticas aplicadas.
Como en la mayoría de las áreas de estudio, la explosión del conocimiento en la era científica ha llevado a la especialización: ahora hay cientos de áreas especializadas en matemáticas y la última Clasificación de Materias de Matemáticas tiene 46 páginas.
Muchos matemáticos hablan de la elegancia de las matemáticas, su estética intrínseca y su belleza interior.
G. H. Hardy en la Apología de un matemático expresó la creencia de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras.
Un teorema expresado como una caracterización del objeto por estas características es el premio.
Euler (1707-1783) fue responsable de muchas de las notaciones en uso hoy en día.
A diferencia del lenguaje natural, donde las personas a menudo pueden equiparar una palabra (como vaca) con el objeto físico al que corresponde, los símbolos matemáticos son abstractos, carentes de cualquier análogo físico.
El lenguaje matemático también incluye muchos términos técnicos como homeomorfismo e integrable que no tienen ningún significado fuera de las matemáticas.
Los matemáticos se refieren a esta precisión del lenguaje y la lógica como "rigor".
Esto es para evitar "teoremas" equivocados, basados en intuiciones falibles, de los cuales han ocurrido muchos casos en la historia del tema.
Malentender el rigor es una causa de algunos de los conceptos erróneos comunes de las matemáticas.
Por otro lado, los asistentes de prueba permiten verificar todos los detalles que no se pueden dar en una prueba escrita a mano, y proporcionan certeza de la exactitud de pruebas largas como la del teorema de Feit-Thompson.
Además de estas principales preocupaciones, también hay subdivisiones dedicadas a explorar vínculos desde el corazón de las matemáticas a otros campos: a la lógica, a la teoría de conjuntos (fundamentos), a las matemáticas empíricas de las diversas ciencias (matemáticas aplicadas), y más recientemente al estudio riguroso de la incertidumbre.
Algunos desacuerdos sobre los fundamentos de las matemáticas continúan hasta nuestros días.
Como tal, es el hogar de los teoremas de incompletitud de Godel que (informalmente) implican que cualquier sistema formal efectivo que contenga aritmética básica, si es sonido (lo que significa que todos los teoremas que se pueden probar son verdaderos), es necesariamente incompleto (lo que significa que hay verdaderos teoremas que no se pueden probar en ese sistema).
La lógica moderna se divide en teoría de recursividad, teoría de modelos y teoría de pruebas, y está estrechamente vinculada a la informática teórica, así como a la teoría de categorías.
La teoría de la computabilidad examina las limitaciones de varios modelos teóricos de la computadora, incluido el modelo más conocido: la máquina de Turing.
La consideración de los números naturales también conduce a los números transfinitos, que formalizan el concepto de "infinito".
Así uno puede estudiar grupos, anillos, campos y otros sistemas abstractos; juntos tales estudios (para estructuras definidas por operaciones algebraicas) constituyen el dominio del álgebra abstracta.
La trigonometría es la rama de las matemáticas que se ocupa de las relaciones entre los lados y los ángulos de los triángulos y con las funciones trigonométricas.
La geometría convexa y discreta se desarrollaron para resolver problemas en la teoría de números y el análisis funcional, pero ahora se persiguen con un ojo en las aplicaciones de optimización y ciencias de la computación.
Los grupos de mentiras se utilizan para estudiar el espacio, la estructura y el cambio.
Las funciones surgen aquí como un concepto central que describe una cantidad cambiante.
Una de las muchas aplicaciones del análisis funcional es la mecánica cuántica.
Los estadísticos (que trabajan como parte de un proyecto de investigación) "crean datos que tienen sentido" con muestreo aleatorio y con experimentos aleatorios; el diseño de una muestra estadística o experimento especifica el análisis de los datos (antes de que los datos estén disponibles).
El análisis numérico estudia métodos para problemas en el análisis usando el análisis funcional y la teoría de aproximación; el análisis numérico incluye el estudio de aproximación y discretización ampliamente con la preocupación especial por errores de redondeo.
La Medalla Chern se introdujo en 2010 para reconocer el logro de toda la vida.
Esta lista logró una gran celebridad entre los matemáticos, y al menos nueve de los problemas ya han sido resueltos.
El valor de Pi fue calculado primero por él.
Fueron los pitagóricos quienes acuñaron el término "matemáticas", y con quienes comienza el estudio de las matemáticas por sí mismo.
Debido a una disputa política, la comunidad cristiana en Alejandría la castigó, suponiendo que estaba involucrada, desnudándola y raspándose la piel con conchas de almejas (algunos dicen que tejas).
La financiación para la traducción de textos científicos en otras lenguas estaba en curso en todas partes del reinado de ciertos califas, y resultó que ciertos eruditos se hicieron expertos en los trabajos que tradujeron y por su parte recibieron el apoyo adicional para seguir desarrollando ciertas ciencias.
Una característica notable de muchos eruditos que trabajan bajo el gobierno musulmán en tiempos medievales es que a menudo eran polímatas.
Durante este período de transición de una cultura principalmente feudal y eclesiástica a una predominantemente secular, muchos matemáticos notables tuvieron otras ocupaciones: Luca Pacioli (fundador de la contabilidad); Niccola Fontana Tartaglia (notable ingeniero y contador); Gerolamo Cardano (primer fundador de la probabilidad y la expansión binomial); Robert Recorde (médico) y Francois Viéte (abogado).
Las universidades británicas de este período adoptaron algunos enfoques familiares a las universidades italianas y alemanas, pero ya que disfrutaron de libertades sustanciales y autonomía los cambios allí habían comenzado con la Edad de Aclaración, las mismas influencias que inspiraron a Humboldt.
Los estudiantes podían realizar investigaciones en seminarios o laboratorios y comenzaron a producir tesis doctorales con más contenido científico.
Los matemáticos y los matemáticos aplicados se consideran dos de las carreras STEM (ciencia, tecnología, ingeniería y matemáticas).
Los actuarios también abordan cuestiones financieras, incluidas las relacionadas con el nivel de contribuciones a las pensiones requeridas para producir ciertos ingresos de jubilación y la forma en que una empresa debe invertir recursos para maximizar su rendimiento de las inversiones a la luz del riesgo potencial.
El sistema jeroglífico para los números egipcios, como los números romanos posteriores, desciende de las marcas de conteo utilizadas para contar.
Los primeros sistemas numéricos que incluían notación posicional no eran decimales, incluyendo el sistema sexagesimal (base 60) para los números babilónicos, y el sistema vigesimal (base 20) que definía los números mayas.
Antes de las obras de Euclides alrededor del 300 aC, los estudios griegos en matemáticas se superponían con las creencias filosóficas y místicas.
Los antiguos griegos carecían de un símbolo de cero hasta el período helenístico, y utilizaron tres conjuntos separados de símbolos como dígitos: un conjunto para el lugar de las unidades, uno para el lugar de las decenas y uno para los cientos.
Su algoritmo de división largo era el mismo, y el algoritmo de la raíz cuadrada del dígito por dígito, popularmente usado tan recientemente como el 20mo siglo, era conocido por Arquímedes (quien lo pudo haber inventado).
Los antiguos chinos habían avanzado los estudios aritméticos que datan de la dinastía Shang y que continúan a través de la dinastía Tang, desde los números básicos hasta el álgebra avanzada.
Para los cientos de lugar, luego reutilizaron los símbolos para el lugar de las unidades, y así sucesivamente.
Los antiguos chinos fueron los primeros en descubrir, entender y aplicar números negativos de manera significativa.
Su contemporáneo, el obispo siríaco Severus Sebokht (650 dC) dijo: "Los indios poseen un método de cálculo que ninguna palabra puede alabar lo suficiente.
Los árabes también aprendieron este nuevo método y lo llamaron hesab.
El florecimiento del álgebra en el mundo islámico medieval, y también en la Europa renacentista, fue una consecuencia de la enorme simplificación de la computación a través de la notación decimal.
Las expresiones aritméticas deben evaluarse de acuerdo con la secuencia de operaciones prevista.
Por ejemplo, los ordenadores digitales pueden reutilizar circuitos de adición existentes y guardar circuitos adicionales para implementar una sustracción, empleando el método del complemento de dos para representar los inversos aditivos, que es extremadamente fácil de implementar en hardware (negación).
La multiplicación también combina dos números en un solo número, el producto.
Si los números se imaginan como situados en una línea, la multiplicación por un número mayor que 1, digamos x, es lo mismo que estirar todo desde 0 uniformemente, de tal manera que el número 1 en sí se estira hasta donde estaba x.
Cualquier dividendo dividido por cero no está definido.
El teorema fundamental de la aritmética fue probado por primera vez por Carl Friedrich Gauss.
La notación posicional (también conocida como "notación de valor posicional") se refiere a la representación o codificación de números usando el mismo símbolo para los diferentes órdenes de magnitud (por ejemplo, el "un lugar", "decenas", "cien lugares") y, con un punto radix, usando esos mismos símbolos para representar fracciones (por ejemplo, el "décimo lugar", "cienavo lugar").
El uso de 0 como marcador de posición y, por lo tanto, el uso de una notación posicional se atestigua por primera vez en el texto jainista de la India titulado Lokavibhaga, fechado en 458 dC y fue solo a principios del siglo XIII que estos conceptos, transmitidos a través de la erudición del mundo árabe, fueron introducidos en Europa por Fibonacci utilizando el sistema numérico hindú-árabe.
El resultado se calcula mediante la adición repetida de un solo dígito de cada número que ocupa la misma posición, procediendo de derecha a izquierda.
El dígito más a la derecha es el valor de la posición actual, y el resultado para la adición posterior de los dígitos a la izquierda aumenta en el valor del segundo dígito (más a la izquierda), que siempre es uno (si no cero).
Una tabla de multiplicar con diez filas y diez columnas enumera los resultados para cada par de dígitos.
Existen técnicas similares para la sustracción y la división.
En terminología matemática, esta característica se define como cierre, y la lista anterior se describe como .
El total en la columna de peniques es 25.
Esta operación se repite usando los valores en la columna de chelines, con la etapa adicional de sumar el valor que se llevó adelante desde la columna de centavos.
Un cuadernillo típico que corría a 150 páginas tabulaba múltiplos "de uno a diez mil a los diversos precios de un centavo a una libra".
Este estudio a veces se conoce como algorismo.
Además, la aritmética fue utilizada por los eruditos islámicos para enseñar la aplicación de las reglas relacionadas con el Zakat y el Irth.
La suma (generalmente significada por el símbolo más) es una de las cuatro operaciones básicas de la aritmética, las otras tres son la resta, la multiplicación y la división.
En álgebra, otra área de las matemáticas, la adición también se puede realizar en objetos abstractos como vectores, matrices, subespacios y subgrupos.
Usando el sufijo gerundive -nd resultados en "agregar", "cosa a añadir".
"Suma" y "summand" derivan del sustantivo latino summa "el más alto, el más alto" y del verbo asociado summare.
Los términos ingleses medios posteriores "adden" y "agregar" fueron popularizados por Chaucer.
Como ejemplo, ¿debería definirse la expresión a + b + c para que signifique (a + b) + c o a + (b + c)?
Incluso algunos animales no humanos muestran una capacidad limitada para añadir, particularmente primates.
Con experiencia adicional, los niños aprenden a agregar más rápidamente explotando la conmutatividad de la adición contando desde el número más grande, en este caso, comenzando con tres y contando "cuatro, cinco".
Cero: Dado que cero es la identidad aditiva, sumar cero es trivial.
Uno alinea dos fracciones decimales una encima de la otra, con el punto decimal en la misma ubicación.
Si las sumas son las velocidades de rotación de dos ejes, se pueden agregar con un diferencial.
Hizo uso de un mecanismo de transporte asistido por gravedad.
Para restar, el operador tuvo que usar el complemento de la calculadora de Pascal, que requería tantos pasos como una adición.
Tanto las puertas XOR como AND son sencillas de realizar en lógica digital, lo que permite la realización de circuitos sumadores completos que, a su vez, pueden combinarse en operaciones lógicas más complejas.
Muchas implementaciones son, de hecho, híbridos de estos tres últimos diseños.
El desbordamiento aritmético imprevisto es una causa bastante común de errores de programa.
Tomada literalmente, la definición anterior es una aplicación del teorema de recursión en el conjunto parcialmente ordenado N2.
Si a o b es cero, trátelo como una identidad.
Aquí, el semigrupo está formado por los números naturales y el grupo es el grupo aditivo de números enteros.
La conmutatividad y la asociatividad de la adición real son inmediatas; definiendo el número real 0 como el conjunto de racionales negativas, se ve fácilmente que es la identidad aditiva.
Uno debe probar que esta operación está bien definida, tratando con secuencias co-Cauchy.
El conjunto de enteros módulo 2 tiene solo dos elementos; la operación de adición que hereda se conoce en la lógica booleana como la función "exclusiva o".
Estos dan dos generalizaciones diferentes de la adición de números naturales al transfinito.
Hay incluso más generalizaciones de la multiplicación que de la suma.
De hecho, si dos números no negativos a y b son de diferentes órdenes de magnitud, entonces su suma es aproximadamente igual a su máximo.
Incluye la idea de la suma de un solo número, que es él mismo, y la suma vacía, que es cero.
La integración es una especie de "suma" sobre un continuo, o más precisamente y en general, sobre una variedad diferenciable.
Las combinaciones lineales son especialmente útiles en contextos donde la adición directa violaría alguna regla de normalización, como la mezcla de estrategias en la teoría de juegos o la superposición de estados en la mecánica cuántica.
La división es una de las cuatro operaciones básicas de la aritmética, las formas en que los números se combinan para hacer nuevos números.
Aquellos en los que se define una división euclidiana (con el resto) se llaman dominios euclidianos e incluyen anillos polinómicos en un indeterminado (que definen la multiplicación y la adición sobre fórmulas de una sola variable).
Este signo de división también se usa solo para representar la operación de división en sí, como por ejemplo una etiqueta en una clave de una calculadora.
Distribuir los objetos varios a la vez en cada ronda de compartir a cada porción conduce a la idea de "trocear" una forma de división donde uno repetidamente resta múltiplos del divisor del dividendo mismo.
Una persona puede usar tablas de logaritmos para dividir dos números, restando los logaritmos de los dos números y luego buscando el antilogaritmo del resultado.
Algunos lenguajes de programación, como C, tratan la división de enteros como en el caso 5 anterior, por lo que la respuesta es un entero.
Del mismo modo, la división derecha de b por a (escrito) es la solución y a la ecuación .
Los ejemplos incluyen álgebras de matriz y álgebras de cuaternión.
La entrada de tal expresión en la mayoría de las calculadoras produce un mensaje de error.
Dado que esta sustitución reduce el mayor de los dos números, la repetición de este proceso da sucesivamente pares más pequeños de números hasta que los dos números se vuelven iguales.
El hecho de que el GCD siempre se pueda expresar de esta manera se conoce como la identidad de Bézout.
Con esta mejora, el algoritmo nunca requiere más pasos que cinco veces el número de dígitos (base 10) del entero más pequeño.
El algoritmo euclidiano tiene muchas aplicaciones teóricas y prácticas.
El algoritmo euclidiano se puede usar para resolver ecuaciones diofánticas, como encontrar números que satisfagan múltiples congruencias de acuerdo con el teorema del resto chino, para construir fracciones continuas y para encontrar aproximaciones racionales precisas a números reales.
El divisor común más grande a menudo se escribe como gcd (a, b) o, más simplemente, como (a, b), aunque la última notación sea ambigua, también usada para conceptos como un ideal en el anillo de números enteros, que estrechamente se relaciona con GCD.
Por ejemplo, ni 6 ni 35 es un número primo, ya que ambos tienen dos factores primos: 6 + 2 + 3 y 35 + 5 + 7.
Se cree que la factorización de enteros grandes es un problema computacionalmente muy difícil, y la seguridad de muchos protocolos criptográficos ampliamente utilizados se basa en su inviabilidad.
El conjunto de todas las combinaciones lineales integrales de a y b es realmente el mismo que el conjunto de todos los múltiplos de g (mg, donde m es un número entero).
En otras palabras, los múltiplos del número más pequeño rk-1 se restan del número más grande rk-2 hasta que el resto rk es más pequeño que rk-1.
Por lo tanto, c divide el resto inicial r0, ya que r0 + a +- q0b +- mc +- q0nc +- (m +- q0n)c.
Primero tratamos de tejer el rectángulo usando tejas cuadradas b-por-b; sin embargo, esto deja un rectángulo residual r0-por-b hasta el final, donde r0 es b. Luego tratamos de tejer el rectángulo residual con tejas cuadradas r0-por-r0.
El teorema que subyace a la definición de la división euclidiana asegura que tal cociente y resto siempre existen y son únicos.
Al final de la iteración del bucle, la variable b contiene el resto rk, mientras que la variable a contiene su predecesora, rk-1.
El matemático e historiador B. L. van der Waerden sugiere que el Libro VII deriva de un libro de texto sobre teoría de números escrito por matemáticos en la escuela de Pitágoras.
Siglos más tarde, el algoritmo de Euclides fue descubierto independientemente tanto en la India como en Chína, principalmente para resolver ecuaciones diofánticas que surgieron en astronomía y hacer calendarios precisos.
El algoritmo euclidiano se describió por primera vez numéricamente y se popularizó en Europa en la segunda edición de Bachet's Problémes plaisants et délectables (Problemas agradables y agradables, 1624).
En el siglo XIX, el algoritmo euclidiano condujo al desarrollo de nuevos sistemas numéricos, como los enteros gaussianos y los enteros de Eisenstein.
Peter Gustav Lejeune Dirichlet parece haber sido el primero en describir el algoritmo euclidiano como la base de gran parte de la teoría de números.
Por ejemplo, Dedekind fue el primero en probar el teorema de dos cuadrados de Fermat utilizando la factorización única de los enteros gaussianos.
Otras aplicaciones del algoritmo de Euclides se desarrollaron en el siglo XIX.
Se han desarrollado varios algoritmos de relación de enteros novedosos, tales como el algoritmo de Helaman Ferguson y R. W. Forcade (1979) y el algoritmo LLL.
Los jugadores se turnan para eliminar m múltiplos de la pila más pequeña de la más grande.
Al permitir que u varíe sobre todos los números enteros posibles, se puede generar una familia infinita de soluciones a partir de una única solución (x1, y1).
En este campo, los resultados de cualquier operación matemática (adición, sustracción, multiplicación o división) se reducen módulo 13; es decir, múltiplos de 13 se suman o restan hasta que el resultado se lleva dentro del rango 0-12.
Ahora supongamos que el resultado es válido para todos los valores de N hasta M x 1.
Por ejemplo, la probabilidad de un cociente de 1, 2, 3 o 4 es aproximadamente 41,5%, 17,0%, 9,3% y 5,9%, respectivamente.
Un enfoque ineficaz para encontrar el GCD de dos números naturales a y b es calcular todos sus divisores comunes; el GCD es entonces el divisor común más grande.
Como se señaló anteriormente, el GCD es igual al producto de los factores primos compartidos por los dos números a y b. Los métodos actuales para la factorización de primos también son ineficientes; muchos sistemas de criptografía modernos incluso se basan en esa ineficiencia.
El algoritmo GCD de Lehmer utiliza el mismo principio general que el algoritmo binario para acelerar los cálculos GCD en bases arbitrarias.
El algoritmo euclidiano se puede usar para resolver ecuaciones diofánticas lineales y problemas de resto chino para polinomios; también se pueden definir fracciones continuas de polinomios.
Cualquier dominio euclidiano es un dominio de factorización único (UFD), aunque lo contrario no es cierto.
Un dominio euclidiano es siempre un dominio ideal principal (PID), un dominio integral en el que cada ideal es un ideal principal.
Los numeradores y denominadores también se usan en fracciones que no son comunes, incluyendo fracciones compuestas, fracciones complejas y números mixtos.
El término fue utilizado originalmente para distinguir este tipo de fracción de la fracción sexagesimal utilizada en astronomía.
Esto fue explicado en el libro de texto del siglo XVII The Ground of Arts.
El producto de una fracción y su recíproco es 1, por lo tanto el recíproco es el inverso multiplicativo de una fracción.
El resto se convierte en el numerador de la parte fraccionaria.
Dado que 5-17 es mayor que 4-18, el resultado de la comparación es .
Dado que un tercio de un cuarto es un duodécimo, dos tercios de un cuarto son dos duodécimos.
A veces se requiere un decimal de repetición infinita para alcanzar la misma precisión.
Los egipcios usaban fracciones egipcias antes de Cristo.
Sus métodos dieron la misma respuesta que los métodos modernos.
Una expresión moderna de fracciones conocidas como bhinnarasi parece haberse originado en la India en el trabajo de Aryabhatta, Brahmagupta y Bhaskara.
En matemáticas, la aritmética modular es un sistema de aritmética para enteros, donde los números "se envuelven" cuando alcanzan un cierto valor, llamado módulo.
Una aplicación muy práctica es calcular sumas de comprobación dentro de identificadores de números de serie.
RSA y Diffie-Hellman utilizan la exponenciación modular.
Es utilizado por las implementaciones más eficientes del polinomio mayor común divisor, álgebra lineal exacta y algoritmos de base de Gr-bner sobre los enteros y los números racionales.
La operación módulo, tal como se implementa en muchos lenguajes de programación y calculadoras, es una aplicación de aritmética modular que se usa a menudo en este contexto.
El método de lanzar nueves ofrece una comprobación rápida de los cálculos aritméticos decimales realizados a mano.
Un sistema lineal de congruencias se puede resolver en tiempo polinómico con una forma de eliminación gaussiana, para más detalles ver teorema de congruencia lineal.
La multiplicación de enteros (incluyendo números negativos), números racionales (fracciones) y números reales se define por una generalización sistemática de esta definición básica.
El producto de dos mediciones es un nuevo tipo de medición.
La operación inversa de la multiplicación es la división.
La división de un número distinto de 0 por sí mismo es igual a 1.
Este uso implícito de la multiplicación puede causar ambiguedad cuando las variables concatenadas coinciden con el nombre de otra variable, cuando un nombre de variable delante de un paréntesis se puede confundir con un nombre de función, o en la determinación correcta del orden de las operaciones.
Los números que se multiplican generalmente se llaman los "factores".
Además, como el resultado de una multiplicación no depende del orden de los factores, la distinción entre "multiplicando" y "multiplicador" es útil solo a un nivel muy elemental y en algunos algoritmos de multiplicación, como la multiplicación larga.
El resultado de una multiplicación se llama producto.
La regla de cálculo permitió que los números se multiplicaran rápidamente a aproximadamente tres lugares de precisión.
La teoría general se da por análisis dimensional.
Los números complejos no tienen una orden.
Aquí tenemos la identidad 1, a diferencia de los grupos bajo adición donde la identidad es típicamente 0.
Para ver esto, considere el conjunto de matrices cuadradas invertibles de una dimensión dada sobre un campo dado.
Otro hecho que vale la pena notar es que los enteros bajo multiplicación no son un grupo, incluso si excluimos cero.
En matemáticas, un porcentaje (del latín por ciento "por cien") es un número o proporción expresado como una fracción de 100.
La computación con estas fracciones fue equivalente a calcular porcentajes.
Cuando se comunica sobre un porcentaje, es importante especificar a qué es relativo (es decir, cuál es el total que corresponde al 100%).
Cuando se habla de un "10% de aumento" o una "caída del 10%" en una cantidad, la interpretación habitual es que esto es relativo al valor inicial de esa cantidad.
La misma confusión entre los diferentes conceptos de porcentaje (edad) y puntos porcentuales puede causar un gran malentendido cuando los periodistas informan sobre los resultados de las elecciones, por ejemplo, expresando tanto los nuevos resultados como las diferencias con los resultados anteriores como porcentajes.
El término se ha atribuido al latín por ciento.
Las guías de gramática y estilo a menudo difieren en cuanto a cómo se escriben los porcentajes.
Cuando las tasas de interés son muy bajas, se incluye el número 0 si la tasa de interés es inferior al 1%, por ejemplo, "% Treasury Stock", no "% Treasury Stock").
Del mismo modo, el porcentaje ganador de un equipo, la fracción de partidos que el club ha ganado, también suele expresarse como una proporción decimal; un equipo que tiene un porcentaje ganador de .500 ha ganado el 50% de sus partidos.
La sustracción también obedece a reglas predecibles con respecto a las operaciones relacionadas, como la suma y la multiplicación.
Realizar la resta en números naturales es una de las tareas numéricas más simples.
Formalmente, el número que se resta se conoce como el sustraendo, mientras que el número del que se resta es el minuendo.
Sustracción es una palabra inglesa derivada del verbo latino subtrahere, que a su vez es un compuesto de sub "de abajo" y trahere "tirar".
Desde la posición 3, no toma ningún paso a la izquierda para permanecer en 3, por lo que .
Para representar tal operación, la línea debe ser extendida.
El dígito inicial "1" del resultado se descarta.
En el lugar de los diez, 0 es menor que 1, por lo que el 0 se incrementa en 10, y la diferencia con 1, que es 9, se escribe en el lugar de los diez.
La resta luego procede en el lugar de los cientos, donde 6 no es menor que 5, por lo que la diferencia se escribe en el lugar del cien del resultado.
Más bien aumenta el dígito del cien por uno.
La respuesta es 1, y se escribe en el lugar de los cien resultados.
Este teorema fue conjeturado primero por Pierre de Fermat en 1637 en el margen de una copia de Aritmética, donde afirmó que tenía una prueba que era demasiado grande para caber en el margen.
El teorema de los cinco colores, que tiene una prueba elemental corta, establece que cinco colores son suficientes para colorear un mapa y se demostró a fines del siglo XIX; sin embargo, probar que cuatro colores son suficientes resultó ser significativamente más difícil.
Fue el primer teorema importante que se probó usando una computadora.
Además, cualquier mapa que potencialmente podría ser un contraejemplo debe tener una parte que se parezca a uno de estos 1.936 mapas.
Fue formulada originalmente en 1908 por Steinitz y Tietze.
Una variedad V sobre un campo finito con q elementos tiene un número finito de puntos racionales, así como puntos sobre cada campo finito con qk elementos que contienen ese campo.
Originalmente conjeturado por Henri Poincaré, el teorema se refiere a un espacio que localmente se parece al espacio tridimensional ordinario, pero está conectado, finito en tamaño, y carece de cualquier límite (una 3-variedad cerrada).
Después de casi un siglo de esfuerzo por parte de los matemáticos, Grigori Perelman presentó una prueba de la conjetura en tres artículos disponibles en 2002 y 2003 en arXiv.
Perelman completó esta parte de la prueba.
Informalmente, se pregunta si cada problema cuya solución puede ser verificada rápidamente por una computadora también puede ser resuelto rápidamente por una computadora; se conjetura ampliamente que la respuesta es no.
No se ha demostrado cuál es falso, pero se cree ampliamente que la primera conjetura es verdadera y la segunda es falsa.
Por ejemplo, la conjetura de Collatz, que se refiere a si ciertas secuencias de números enteros terminan o no, se ha probado para todos los números enteros hasta 1,2 +- 1012 (más de un billón).
Esa evidencia puede ser de varios tipos, como la verificación de las consecuencias de la misma o fuertes interconexiones con resultados conocidos.
Un método de prueba, aplicable cuando solo hay un número finito de casos que podrían conducir a contraejemplos, se conoce como "fuerza bruta": en este enfoque, todos los casos posibles se consideran y se muestra que no dan contraejemplos.
La hipótesis del continuo, que trata de determinar la cardinalidad relativa de ciertos conjuntos infinitos, finalmente demostró ser independiente del conjunto generalmente aceptado de los axiomas de Zermelo-Fraenkel de la teoría de conjuntos.
Pocos teóricos dudan de que la hipótesis de Riemann sea cierta.
El mapa logístico es un mapa polinómico, a menudo citado como un ejemplo arquetípico de cómo el comportamiento caótico puede surgir de ecuaciones dinámicas no lineales muy simples.
Kepler demostró que es el límite de la proporción de números de Fibonacci consecutivos.
Por dos razones esta representación puede causar problemas.
Por ejemplo, las dos representaciones 0.999... y 1 son equivalentes en el sentido de que representan el mismo número.
Usando computadoras y supercomputadoras, algunas de las constantes matemáticas, incluyendo e, y la raíz cuadrada de 2, se han calculado a más de cien mil millones de dígitos.
Algunas constantes difieren tanto del tipo habitual que se ha inventado una nueva notación para representarlas razonablemente.
A veces, el símbolo que representa una constante es una palabra completa.
0 (cero) es un número, y el dígito numérico utilizado para representar ese número en números.
Los nombres para el número 0 en inglés incluyen cero, cero (Reino Unido), nada (EE.UU.), nil, o -en contextos donde al menos un dígito adyacente lo distingue de la letra "O" - oh u o.
Para la noción simple de carencia, las palabras nada y ninguno a menudo se usan.
A menudo se llama oh en el contexto de los números de teléfono.
El símbolo nfr, que significa hermoso, también se usó para indicar el nivel de base en dibujos de tumbas y pirámides, y las distancias se midieron en relación con la línea de base como por encima o por debajo de esta línea.
El marcador de posición babilónico no era un cero verdadero porque no se usaba solo, ni se usaba al final de un número.
Hacia el año 150, Ptolomeo, influenciado por Hiparco y los babilonios, estaba usando un símbolo de cero en su trabajo sobre astronomía matemática llamado Syntaxis Mathematica, también conocido como el Almagest.
Este uso se repitió en AD525 en una tabla equivalente, que fue traducida vía nulla latino o "ninguno" por Dionysius Exiguus, junto a números romanos.
El Lokavibhaga, un texto jainista sobre cosmología que sobrevive en una traducción sánscrita medieval del original de Prakrit, que está internamente fechado en el año 458 (era Saka 380), usa un sistema decimal de valor posicional, incluido un cero.
En 813, al-Khwarizmi usó los números hindúes en sus tablas astronómicas.
Este libro se tradujo más tarde al latín en el siglo XII bajo el título Algoritmi de numero Indorum.
Proseguí mi estudio en profundidad y aprendí el toma y daca de la disputa.
Me he esforzado por componer este libro en su totalidad de la manera más comprensible posible, dividiéndolo en quince capítulos.
Las nueve cifras indias son: 9 8 7 6 5 4 3 2 1.
254-255 incluyen 0 como un número natural, en cuyo caso es el único número natural que no es positivo.
Como un valor o un número, cero no es lo mismo que el dígito cero, utilizado en sistemas numéricos con notación posicional.
El número 0 puede o no ser considerado un número natural, pero es un número entero, y por lo tanto un número racional y un número real (así como un número algebraico y un número complejo).
No puede ser primo porque tiene un número infinito de factores, y no puede ser compuesto porque no puede expresarse como un producto de números primos (ya que el 0 siempre debe ser uno de los factores).
Estas reglas se aplican a cualquier número real o complejo x, a menos que se indique lo contrario.
La función cardinalidad, aplicada al conjunto vacío, devuelve el conjunto vacío como un valor, asignándole así 0 elementos.
En álgebra abstracta, 0 se usa comúnmente para denotar un elemento cero, que es un elemento neutro para la adición (si se define en la estructura en consideración) y un elemento absorbente para la multiplicación (si se define).
Para algunas cantidades, el nivel cero se distingue naturalmente de todos los demás niveles, mientras que para otros se elige más o menos arbitrariamente.
Se ha demostrado que un grupo de cuatro neutrones puede ser lo suficientemente estable como para ser considerado un átomo por derecho propio.
Por ejemplo, los elementos de una matriz se numeran a partir de 0 en C, de modo que para una matriz de n elementos la secuencia de índices de matriz va de 0 a .
En las bases de datos, es posible que un campo no tenga un valor.
Para los campos de texto esto no está en blanco ni la cadena vacía.
Cualquier cálculo que incluya un valor nulo entrega un resultado nulo.
En la Fórmula Uno, si el Campeón del Mundo reinante ya no compite en la Fórmula Uno en el año siguiente a su victoria en la carrera por el título, se le da 0 a uno de los pilotos del equipo con el que el campeón reinante ganó el título.
Las máquinas de escribir originalmente no hacían distinción en la forma entre O y 0; algunos modelos ni siquiera tenían una clave separada para el dígito 0.
El dígito 0 con un punto en el centro parece haberse originado como una opción en las pantallas IBM 3270 y ha continuado con algunos tipos de letra modernos como Andalé Mono, y en algunos sistemas de reserva de aerolíneas.
1 (también llamado unidad y unidad) es un número y un dígito numérico utilizado para representar ese número en números.
En las convenciones de signo donde cero no se considera ni positivo ni negativo, 1 es el primer y más pequeño número entero positivo.
La mayoría, si no todas las propiedades de 1 se pueden deducir de esto.
Por lo tanto, es el entero después de cero.
Fue transmitida a Europa a través del Magreb y Andalucía durante la Edad Media, a través de obras académicas escritas en árabe.
Los estilos que no usan el trazo ascendente largo en el dígito 1 generalmente tampoco usan el trazo horizontal a través de la vertical del dígito 7.
Por definición, 1 es la magnitud, el valor absoluto o la norma de un número complejo unitario, un vector unitario y una matriz unitaria (más generalmente llamada matriz de identidad).
En la teoría de categorías, 1 se utiliza a veces para denotar el objeto terminal de una categoría.
Dado que la función exponencial base 1 (1x) siempre es igual a 1, su inversa no existe (que se llamaría el logaritmo base 1 si existiera).
Del mismo modo, los vectores a menudo se normalizan en vectores unitarios (es decir, vectores de magnitud uno), porque estos a menudo tienen propiedades más deseables.
También es el primer y segundo número en la secuencia de Fibonacci (0 siendo el cero) y es el primer número en muchas otras secuencias matemáticas.
Sin embargo, el álgebra abstracta puede considerar el campo con un elemento, que no es un singleton y no es un conjunto en absoluto.
Un código binario es una secuencia de 1 y 0 que se utiliza en las computadoras para representar cualquier tipo de datos.
+1 es la carga eléctrica de positrones y protones.
El filósofo neopitagórico Nicómaco de Gerasa afirmó que uno no es un número, sino la fuente del número.
We Are Number One es una canción de 2014 del programa de televisión infantil LazyTown, que ganó popularidad como meme.
En el fútbol de la asociación (fútbol) el número 1 a menudo se da al portero.
1 es el número más bajo permitido para el uso por jugadores de la Liga de Hockey Nacional (NHL); la liga prohibió el uso de 00 y 0 a finales de los años 1990 (el número más alto permitido ser 98).
Cualquier secuencia aleatoria de dígitos contiene subsecuencias arbitrariamente largas que parecen no aleatorias, por el teorema del mono infinito.
Segundo, puesto que ningún número trascendental puede ser construido con compás y regla, no es posible "cuadrar el círculo".
El astrónomo indio Aryabhata usó un valor de 3.1416 en su ?ryabha?ya (499 d.C.).
El astrónomo persa Jamshid al-Kish produjo 9 dígitos sexagesimal, aproximadamente el equivalente a 16 dígitos decimales, en 1424 usando un polígono con 3-228 lados, que estuvo de pie como el registro mundial durante aproximadamente 180 años.
Estos evitan la dependencia de series infinitas.
Según lo modificado por Salamin y Brent, también se conoce como el algoritmo Brent-Salamin.
Esto está en contraste con series infinitas o algoritmos iterativos, que retienen y usan todos los dígitos intermedios hasta que se produce el resultado final.
Tales ayudas de memorización se llaman mnemotécnicos.
Los dígitos son grandes caracteres de madera unidos al techo en forma de cúpula.
Un dígito numérico es un símbolo simple usado solo (tal como "2") o en combinaciones (tal como "25"), para representar números en un sistema numérico posicional.
Un sistema de numeración posicional tiene un dígito único para cada número entero desde cero hasta, pero sin incluir, la raíz del sistema de numeración.
Los números originales eran muy similares a los modernos, incluso hasta los glifos utilizados para representar dígitos.
Los mayas usaban un símbolo de concha para representar el cero.
El sistema numérico tailandés es idéntico al sistema numérico hindú-árabe, excepto por los símbolos utilizados para representar dígitos.
Ambos son sistemas base 3.
Varios autores en los últimos 300 años han notado una facilidad de notación posicional que equivale a una representación decimal modificada.
Por ejemplo, 1111 (mil, ciento once) es una repetición.
Además de contar diez dedos, algunas culturas han contado nudillos, el espacio entre los dedos de las manos y los pies, así como los dedos de las manos.
Las culturas de la edad de piedra, incluidos los antiguos grupos indígenas americanos, usaban cuentas para juegos de azar, servicios personales y bienes comerciales.
Comenzando aproximadamente 3.500 A.C., las fichas de arcilla fueron gradualmente sustituidas por signos del número impresionados con un estilo redondo en ángulos diferentes en tablillas de arcilla (al principio recipientes para fichas) que entonces se hornearon.
Estos signos de números cuneiformes se parecían a los signos de números redondos que reemplazaron y conservaron la notación de valor de signo aditivo de los signos de números redondos.
Los números sexagesimales eran un sistema de radix mixto que retenía la base alterna 10 y la base 6 en una secuencia de cuñas y chevrons verticales cuneiformes.
Un número único de tropas y medidas de arroz aparecen como combinaciones únicas de estos recuentos.
Los recuentos convencionales son bastante difíciles de multiplicar y dividir.
Los judíos comenzaron a usar un sistema similar (numerales hebreos), con los ejemplos más antiguos conocidos siendo monedas de alrededor del 100 aC.
Los mayas de América Central utilizaron una base mixta 18 y un sistema de base 20, posiblemente heredados de los olmecas, incluyendo características avanzadas como la notación posicional y un cero.
El conocimiento de las codificaciones de los nudos y colores fue suprimido por los conquistadores españoles en el siglo XVI, y no ha sobrevivido, aunque todavía se utilizan dispositivos de grabación simples como quipu en la región andina.
Zero fue utilizado por primera vez en la India en el siglo VII por Brahmagupta.
Los matemáticos árabes ampliaron el sistema para incluir fracciones decimales, y Mu?ammad ibn M?s? al-?w?rizm? escribió un trabajo importante sobre ello en el 9no siglo.
El sistema binario (base 2), fue propagado en el siglo XVII por Gottfried Leibniz.
Las variables para las que la ecuación debe resolverse también se llaman incógnitas, y los valores de las incógnitas que satisfacen la igualdad se llaman soluciones de la ecuación.
Una ecuación condicional solo es verdadera para valores particulares de las variables.
Muy a menudo, el lado derecho de una ecuación se supone que es cero.
Una ecuación es análoga a una escala en la que se colocan los pesos.
Esta es la idea inicial de la geometría algebraica, un área importante de las matemáticas.
Para resolver ecuaciones de cualquier familia, se utilizan técnicas algorítmicas o geométricas que se originan a partir de álgebra lineal o análisis matemático.
Estas ecuaciones son difíciles en general; uno a menudo busca sólo para encontrar la existencia o ausencia de una solución, y, si existen, para contar el número de soluciones.
En la ilustración, x, y y z son todas cantidades diferentes (en este caso números reales) representadas como pesos circulares, y cada uno de x, y y z tiene un peso diferente.
Por lo tanto, la ecuación con R no especificado es la ecuación general para el círculo.
El proceso de encontrar las soluciones, o, en el caso de los parámetros, expresar las incógnitas en términos de los parámetros, se llama resolver la ecuación.
Multiplicar o dividir ambos lados de una ecuación por una cantidad distinta de cero.
Una ecuación algebraica es univariante si involucra solo una variable.
En matemáticas, la teoría de los sistemas lineales es la base y una parte fundamental del álgebra lineal, un tema que se utiliza en la mayoría de las partes de las matemáticas modernas.
Este formalismo permite determinar las posiciones y las propiedades de los focos de una cónica.
Este punto de vista, esbozado por Descartes, enriquece y modifica el tipo de geometría concebida por los antiguos matemáticos griegos.
Una ecuación diofántica exponencial es aquella para la cual los exponentes de los términos de la ecuación pueden ser desconocidos.
La geometría algebraica moderna se basa en técnicas más abstractas del álgebra abstracta, especialmente el álgebra conmutativa, con el lenguaje y los problemas de la geometría.
Un punto del plano pertenece a una curva algebraica si sus coordenadas satisfacen una ecuación polinómica dada.
En matemáticas puras, las ecuaciones diferenciales se estudian desde varias perspectivas diferentes, principalmente relacionadas con sus soluciones: el conjunto de funciones que satisfacen la ecuación.
Las ecuaciones diferenciales lineales, que tienen soluciones que se pueden agregar y multiplicar por coeficientes, están bien definidas y comprendidas, y se obtienen soluciones exactas de forma cerrada.
Las PDE se pueden usar para describir una amplia variedad de fenómenos como el sonido, el calor, la electrostática, la electrodinámica, el flujo de fluidos, la elasticidad o la mecánica cuántica.
Una solución es una asignación de valores a las variables desconocidas que hace que la igualdad en la ecuación sea verdadera.
El conjunto de todas las soluciones de una ecuación es su conjunto de soluciones.
Dependiendo del contexto, resolver una ecuación puede consistir en encontrar cualquier solución (encontrar una sola solución es suficiente), todas las soluciones o una solución que satisfaga otras propiedades, como pertenecer a un intervalo dado.
En este caso, las soluciones no se pueden enumerar.
La variedad de tipos de ecuaciones es grande, y también lo son los métodos correspondientes.
Esto puede deberse a la falta de conocimiento matemático; algunos problemas solo se resolvieron después de siglos de esfuerzo.
Los polinomios aparecen en muchas áreas de las matemáticas y la ciencia.
Muchos autores usan estas dos palabras indistintamente.
Formalmente, el nombre del polinomio es P, no P(x), pero el uso de la notación funcional P(x) data de un momento en que la distinción entre un polinomio y la función asociada no estaba clara.
Sin embargo, uno puede usarlo sobre cualquier dominio donde se definan la suma y la multiplicación (es decir, cualquier anillo).
A los polinomios de pequeño grado se les han dado nombres específicos.
El polinomio 0, que puede considerarse que no tiene términos en absoluto, se llama polinomio cero.
Debido a que el grado de un polinomio distinto de cero es el grado más grande de cualquier término, este polinomio tiene grado dos.
Los polinomios se pueden clasificar por el número de términos con coeficientes distintos de cero, de modo que un polinomio de un término se llama monomio, un polinomio de dos términos se llama binomial y un polinomio de tres términos se llama trinomio.
Cuando se utiliza para definir una función, el dominio no está tan restringido.
Un polinomio en un indeterminado se llama un polinomio univariado, un polinomio en más de un indeterminado se llama un polinomio multivariado.
En el caso de los números complejos, los factores irreducibles son lineales.
Si el grado es mayor que uno, el gráfico no tiene ninguna asíntota.
En álgebra elemental, los métodos como la fórmula cuadrática se enseñan para resolver todas las ecuaciones polinómicas de primer grado y segundo grado en una variable.
Sin embargo, se pueden usar algoritmos de búsqueda de raíces para encontrar aproximaciones numéricas de las raíces de una expresión polinómica de cualquier grado.
Desde el 16to siglo, las fórmulas similares (usando raíces cúbicas además de raíces cuadradas), pero mucho más complicadas se conocen para ecuaciones del grado tres y cuatro (ver la ecuación cúbica y la ecuación cuartica).
En 1830, Évariste Galois demostró que la mayoría de las ecuaciones de grado superior a cuatro no pueden ser resueltas por radicales, y demostró que para cada ecuación, uno puede decidir si es solucionable por radicales y, si lo es, resolverlo.
Sin embargo, se han publicado fórmulas para ecuaciones solucionables de grados 5 y 6 (véase la función quintica y la ecuación sextica).
Los algoritmos más eficientes permiten resolver fácilmente (en una computadora) ecuaciones polinómicas de grado superior a 1,000 (ver Algoritmo de búsqueda de raíces).
Para un conjunto de ecuaciones polinómicas en varias incógnitas, hay algoritmos para decidir si tienen un número finito de soluciones complejas y, si este número es finito, para calcular las soluciones.
Una ecuación polinómica para la que uno está interesado sólo en las soluciones que son enteros se llama una ecuación diofántica.
Los coeficientes pueden tomarse como números reales, para funciones de valor real.
Esta equivalencia explica por qué las combinaciones lineales se llaman polinomios.
En el caso de coeficientes en un anillo, "no constante" se debe sustituir por "no constante o no unidad" (ambas definiciones concuerdan en el caso de coeficientes en un campo).
Cuando los coeficientes pertenecen a enteros, números racionales o un campo finito, existen algoritmos para probar la irreductibilidad y calcular la factorización en polinomios irreducibles (ver Factorización de polinomios).
El polinomio característico de una matriz u operador lineal contiene información sobre los valores propios del operador.
Sin embargo, la notación elegante y práctica que utilizamos hoy en día solo se desarrolló a partir del siglo XV.
Esto "completa el cuadrado", convirtiendo el lado izquierdo en un cuadrado perfecto.
El teorema de Descartes establece que por cada cuatro círculos de besos (mutuamente tangentes), sus radios satisfacen una ecuación cuadrática particular.
Los matemáticos babilónicos de alrededor del 400 aC y los matemáticos chinos de alrededor del 200 aC utilizaron métodos geométricos de disección para resolver ecuaciones cuadráticas con raíces positivas.
Euclides, el matemático griego, produjo un método geométrico más abstracto alrededor del 300 aC.
Al-Khwarizmi va más allá al proporcionar una solución completa a la ecuación cuadrática general, aceptando una o dos respuestas numéricas para cada ecuación cuadrática, al tiempo que proporciona pruebas geométricas en el proceso.
Abá Kímil Shujá ibn Aslam (Egipto, siglo X) en particular fue el primero en aceptar números irracionales (a menudo en forma de raíz cuadrada, raíz cúbica o cuarta raíz) como soluciones a ecuaciones cuadráticas o como coeficientes en una ecuación.
Su solución se basó en gran medida en el trabajo de Al-Khwarizmi.
Sin embargo, en algún momento la fórmula cuadrática comienza a perder precisión debido al error de redondeo, mientras que el método aproximado continúa mejorando.
Existían métodos de aproximación numérica, llamados prosthaphaeresis, que ofrecían atajos en torno a operaciones que consumían mucho tiempo, como la multiplicación y la toma de poderes y raíces.
Los algoritmos computacionales para encontrar las soluciones son una parte importante del álgebra lineal numérica y juegan un papel prominente en ingeniería, física, química, ciencias de la computación y economía.
Para soluciones en un dominio integral como el anillo de los enteros, o en otras estructuras algebraicas, se han desarrollado otras teorías, véase Ecuación lineal sobre un anillo.
Esto permite que todo el lenguaje y la teoría de los espacios vectoriales (o más en general, los módulos) se apliquen.
Un sistema de este tipo se conoce como un sistema infradeterminado.
El segundo sistema tiene una única solución, a saber, la intersección de las dos líneas.
Cualquiera de estas dos ecuaciones tiene una solución común.
Un sistema de ecuaciones cuyos lados izquierdos son linealmente independientes es siempre consistente.
Esto produce un sistema de ecuaciones con una ecuación menos y una menos desconocida.
Tipo 3: Añadir a una fila un múltiplo escalar de otra.
Por ejemplo, los sistemas con una matriz definida positiva simétrica se pueden resolver dos veces más rápido con la descomposición de Cholesky.
A menudo se adopta un enfoque completamente diferente para sistemas muy grandes, lo que de otro modo tomaría demasiado tiempo o memoria.
Esto conduce a la clase de métodos iterativos.
En matemáticas, una serie es, hablando en términos generales, una descripción de la operación de sumar infinitas cantidades, una tras otra, a una cantidad inicial dada.
Además de su ubicuidad en matemáticas, las series infinitas también son ampliamente utilizadas en otras disciplinas cuantitativas como la física, la informática, la estadística y las finanzas.
La paradoja de Zenón de Aquiles y la tortuga ilustra esta propiedad contraintuitiva de sumas infinitas: Aquiles corre tras una tortuga, pero cuando alcanza la posición de la tortuga al comienzo de la carrera, la tortuga ha alcanzado una segunda posición; cuando alcanza esta segunda posición, la tortuga está en una tercera posición, y así sucesivamente.
Este argumento no prueba que la suma sea igual a 2 (aunque lo es), pero sí prueba que es como máximo 2.
Las pruebas para la convergencia uniforme incluyen la prueba M de Weierstrass, la prueba de convergencia uniforme de Abel, la prueba de Dini y el criterio de Cauchy.
La convergencia es uniforme en subconjuntos cerrados y acotados (es decir, compactos) del interior del disco de convergencia: a saber, es uniformemente convergente en conjuntos compactos.
La serie de Hilbert-Poincaré es una serie de poder formal usada para estudiar álgebras graduadas.
En el 17mo siglo, James Gregory trabajó en el nuevo sistema decimal en series infinitas y publicó varias series de Maclaurin.
Cauchy (1821) insistió en pruebas estrictas de convergencia; demostró que si dos series son convergentes su producto no es necesariamente así, y con él comienza el descubrimiento de criterios efectivos.
Un método de sumabilidad es una asignación de un límite a un subconjunto del conjunto de series divergentes que extiende adecuadamente la noción clásica de convergencia.
Los eruditos indios han estado usando fórmulas factoriales desde al menos el 12do siglo.
En los lenguajes funcionales, la definición recursiva a menudo se implementa directamente para ilustrar funciones recursivas.
Otras implementaciones (como software de computadora, como programas de hoja de cálculo) a menudo pueden manejar valores más grandes.
En comparación con la definición de Pickover del superfactorial, el hiperfactorial crece relativamente lentamente.
No hay, relativamente hablando, tales soluciones simples para los factoriales; ninguna combinación finita de sumas, productos, potencias, funciones exponenciales o logaritmos será suficiente para expresar; pero es posible encontrar una fórmula general para los factoriales utilizando herramientas como integrales y límites del cálculo.
Las integrales que hemos discutido hasta ahora involucran funciones trascendentales, pero la función gamma también surge de integrales de funciones puramente algebraicas.
Tomando límites, ciertos productos racionales con infinitamente muchos factores se pueden evaluar en términos de la función gamma también.
Su historia, notablemente documentada por Philip J. Davis en un artículo que le ganó el Premio Chauvenet de 1963, refleja muchos de los principales desarrollos dentro de las matemáticas desde el siglo XVIII.
En lugar de encontrar una prueba especializada para cada fórmula, sería deseable tener un método general para identificar la función gamma.
Sin embargo, la función gamma no parece satisfacer ninguna ecuación diferencial simple.
El teorema de Bohr-Mollerup es útil porque es relativamente fácil probar la convexidad logarítmica para cualquiera de las diferentes fórmulas utilizadas para definir la función gamma.
Como los ordenadores electrónicos se hicieron disponibles para la producción de tablas en los años 1950, varias tablas extensas para la función gamma compleja se publicaron para encontrar la demanda, incluso una tabla exacta a 12 decimales de la Oficina Nacional estadounidense de Estándares.
En ciencia, una fórmula es una forma concisa de expresar información simbólicamente, como en una fórmula matemática o una fórmula química.
En matemáticas, una fórmula generalmente se refiere a una identidad que iguala una expresión matemática a otra, siendo los más importantes teoremas matemáticos.
Esta convención, aunque menos importante en una fórmula relativamente simple, significa que los matemáticos pueden manipular más rápidamente fórmulas que son más grandes y más complejas.
Por ejemplo, H2O es la fórmula química para el agua, especificando que cada molécula consiste en dos átomos de hidrógeno (H) y un átomo de oxígeno (O).
En las fórmulas empíricas, estas proporciones comienzan con un elemento clave y luego asignan números de átomos de los otros elementos en el compuesto, como proporciones al elemento clave.
Algunos tipos de compuestos iónicos, sin embargo, no se pueden escribir como fórmulas empíricas que contienen sólo los números enteros.
Hay varios tipos de estas fórmulas, incluyendo fórmulas moleculares y fórmulas condensadas.
Las funciones fueron originalmente la idealización de cómo una cantidad variable depende de otra cantidad.
Esta definición de "gráfico" se refiere a un conjunto de pares de objetos.
Cuando el dominio y el codominio son conjuntos de números reales, cada par puede considerarse como las coordenadas cartesianas de un punto en el plano.
Ocasionalmente, puede identificarse con la función, pero esto oculta la interpretación habitual de una función como un proceso.
Un mapa puede tener cualquier conjunto como su codominio, mientras que, en algunos contextos, típicamente en libros más antiguos, el codominio de una función es específicamente el conjunto de números reales o complejos.
Otro ejemplo común es la función de error.
Las series de potencia se pueden usar para definir funciones en el dominio en el que convergen.
A continuación, la serie de potencia se puede utilizar para ampliar el dominio de la función.
Partes de esto pueden crear un gráfico que representa (partes de) la función.
Esta es la factorización canónica de .
En ese momento, solo se consideraban las funciones de valor real de una variable real, y se suponía que todas las funciones eran fluidas.
Las funciones se utilizan ahora en todas las áreas de las matemáticas.
Así es como las funciones trigonométricas inversas se definen en términos de funciones trigonométricas, donde las funciones trigonométricas son monótonas.
La utilidad del concepto de funciones multivalor es más clara cuando se consideran funciones complejas, típicamente funciones analíticas.
Esta función se llama el valor principal de la función.
La programación funcional es el paradigma de programación que consiste en la construcción de programas mediante el uso de subrutinas que se comportan como funciones matemáticas.
Excepto por la terminología del lenguaje informático, la "función" tiene el significado matemático habitual en la informática.
Los términos se manipulan a través de algunas reglas (la equivalencia, la reducción y la conversión), que son los axiomas de la teoría y pueden interpretarse como reglas de cálculo.
Nicolas Chuquet usó una forma de notación exponencial en el siglo XV, que más tarde fue utilizada por Henricus Grammateus y Michael Stifel en el siglo XVI.
Así escribirían polinomios, por ejemplo, como .
El resultado es siempre un número real positivo, y las identidades y propiedades mostradas anteriormente para exponentes enteros permanecen verdaderas con estas definiciones para exponentes reales.
Esta función es igual a la raíz th habitual para los radicandos reales positivos.
Este es el punto de partida de la teoría matemática de los semigrupos.
Podemos reemplazar nuevamente el conjunto N con un número cardinal n para obtener Vn, aunque sin elegir un conjunto estándar específico con cardinalidad n, esto se define solo hasta isomorfismo.
Nicolas Bourbaki, Elementos de Matemáticas, Teoría de Conjuntos, Springer-Verlag, 2004, III.
La iteración de la tetración conduce a otra operación, y así sucesivamente, un concepto llamado hiperoperación.
En los ajustes aplicados, las funciones exponenciales modelan una relación en la que un cambio constante en la variable independiente da el mismo cambio proporcional (es decir, aumento o disminución porcentual) en la variable dependiente.
Esta propiedad de función conduce a un crecimiento exponencial o decaimiento exponencial.
Del mismo modo, la composición de las funciones sobre (superyectivas) es siempre sobre.
Entonces uno puede formar cadenas de transformaciones compuestas juntas, como .
Esta notación alternativa se llama notación postfix.
La categoría de conjuntos con funciones como morfismos es la categoría prototípica.
Por ejemplo, el decibelio (dB) es una unidad utilizada para expresar la relación como logaritmos, principalmente para la potencia y la amplitud de la señal (de los cuales la presión del sonido es un ejemplo común).
Ayudan a describir las proporciones de frecuencia de los intervalos musicales, aparecen en fórmulas que cuentan números primos o factoriales aproximados, informan algunos modelos en psicofísica y pueden ayudar en la contabilidad forense.
El siguiente número entero es 4, que es el número de dígitos de 1430.
Antes de la invención de Napier, había habido otras técnicas de alcances similares, como la prosthaphaéresis o el uso de tablas de progresiones, ampliamente desarrolladas por Jost Boergi alrededor de 1600.
Hablar de un número como requiriendo tantas cifras es una alusión áspera al logaritmo común y fue mandado a por Arquímedes como el "orden de un número".
Tales métodos se llaman prostaféresis.
Por ejemplo, cada cámara de la concha de un nautilus es una copia aproximada de la siguiente, escalada por un factor constante.
Los logaritmos también están vinculados a la auto-similitud.
Se utiliza para cuantificar la pérdida de niveles de voltaje en la transmisión de señales eléctricas, para describir los niveles de potencia de los sonidos en la acústica, y la absorbancia de la luz en los campos de la espectrometría y la óptica.
El vinagre tiene típicamente un pH de aproximadamente 3.
Esta "ley", sin embargo, es menos realista que los modelos más recientes, como la ley de poder de Stevens.)
Cuando el logaritmo de una variable aleatoria tiene una distribución normal, se dice que la variable tiene una distribución logarítmica normal.
Para tal modelo, la función de probabilidad depende de al menos un parámetro que debe estimarse.
Del mismo modo, el algoritmo merge sort ordena una lista sin clasificar dividiendo la lista en mitades y ordenando estas primero antes de fusionar los resultados.
Los exponentes de Lyapunov usan logaritmos para medir el grado de caótica de un sistema dinámico.
El triángulo de Sierpinski (en la foto) puede ser cubierto por tres copias de sí mismo, cada uno con lados de la mitad de la longitud original.
Otro ejemplo es el logaritmo p-ádico, la función inversa del exponencial p-ádico.
Llevar a cabo la exponenciación se puede hacer de manera eficiente, pero se cree que el logaritmo discreto es muy difícil de calcular en algunos grupos.
Las raíces cuadradas de números negativos pueden ser discutidas en el marco de números complejos.
En la antigua India, el conocimiento de los aspectos teóricos y aplicados de la raíz cuadrada y cuadrada era al menos tan antiguo como los Sulba Sutras, datado alrededor de 800-500 aC (posiblemente mucho antes).
La letra jom se asemeja a la forma actual de la raíz cuadrada.
Define un concepto importante de desviación estándar utilizado en la teoría de la probabilidad y la estadística.
La mayoría de las calculadoras de bolsillo tienen una clave de raíz cuadrada.
La complejidad temporal para calcular una raíz cuadrada con n dígitos de precisión es equivalente a la de multiplicar dos números de n dígitos.
Los problemas de Hilbert son veintitrés problemas en matemáticas publicados por el matemático alemán David Hilbert en 1900.
Para otros problemas, como el quinto, los expertos han acordado tradicionalmente una sola interpretación, y se ha dado una solución a la interpretación aceptada, pero existen problemas sin resolver estrechamente relacionados.
Hay dos problemas que no sólo están sin resolver, sino que de hecho pueden ser irresolubles para los estándares modernos.
Los otros veintiún problemas han recibido toda la atención significativa, y tarde en el trabajo del siglo veinte en estos problemas todavía se consideraba ser de la mayor importancia.
Hilbert vivió durante 12 años después de que Kurt Godel publicara su teorema, pero no parece haber escrito ninguna respuesta formal al trabajo de Godel.
Al discutir su opinión de que cada problema matemático debe tener una solución, Hilbert permite la posibilidad de que la solución podría ser una prueba de que el problema original es imposible.
El primero de ellos fue demostrado por Bernard Dwork; una prueba completamente diferente de los dos primeros, a través de la cohomología l-ádica, fue dada por Alexander Grothendieck.
Sin embargo, las conjeturas de Weil eran, en su alcance, más como un solo problema de Hilbert, y Weil nunca pretendió que fueran un programa para todas las matemáticas.
A menudo, los erdés ofrecían recompensas monetarias; el tamaño de la recompensa dependía de la dificultad percibida del problema.
Al menos en los principales medios de comunicación, el análogo de facto del siglo XXI de los problemas de Hilbert es la lista de siete problemas del Premio del Milenio elegidos durante 2000 por el Instituto de Matemáticas Clay.
La hipótesis de Riemann es notable por su aparición en la lista de problemas de Hilbert, la lista de Smale, la lista de Problemas del Premio del Milenio e incluso las conjeturas de Weil, en su forma geométrica.
Dados dos poliedros de igual volumen, ¿es siempre posible cortar el primero en un número finito de piezas poliédricas que se pueden volver a montar para producir el segundo?
Extender el teorema de Kronecker-Weber sobre las extensiones abelianas de los números racionales a cualquier campo de números base.
1959 15a Fundación rigurosa del cálculo enumerativo de Schubert.
1927 18 (a) ¿Hay un poliedro que admita solo un mosaico anisoédrico en tres dimensiones? (b) ¿Cuál es el empaque de esfera más denso?
Un número es un objeto matemático utilizado para contar, medir y etiquetar.
Más universalmente, los números individuales se pueden representar por símbolos, llamados números; por ejemplo, "5" es un número que representa el número cinco.
Los cálculos con números se realizan con operaciones aritméticas, siendo las más familiares la suma, resta, multiplicación, división y exponenciación.
Gilsdorf, Thomas E. Introducción a las matemáticas culturales: con estudios de casos en las otomías e incas, John Wiley & Sons, 24 de febrero de 2012.
Durante el siglo XIX, los matemáticos comenzaron a desarrollar muchas abstracciones diferentes que comparten ciertas propiedades de los números, y se puede ver como una extensión del concepto.
Un sistema de conteo no tiene ningún concepto de valor posicional (como en la notación decimal moderna), lo que limita su representación de grandes números.
Brahmagupta es el primer libro que menciona el cero como un número, por lo tanto Brahmagupta es generalmente considerado el primero en formular el concepto de cero.
En una vena similar, Paini (el 5to siglo A.C.) usó el operador nulo (cero) en Ashtadhyayi, un ejemplo temprano de una gramática algebraica para la lengua sánscrita (también ver Pingala).
Por 130 dC, Ptolomeo, influenciado por Hiparco y los babilonios, estaba usando un símbolo para 0 (un pequeño círculo con una barra larga) dentro de un sistema de números sexagesimal de lo contrario usando números griegos alfabéticos.
La referencia anterior de Diophantus fue discutida más explícitamente por el matemático indio Brahmagupta, en Brahmashiasiddhanta en 628, quien usó números negativos para producir la fórmula cuadrática de forma general que permanece en uso hoy en día.
Al mismo tiempo, los chinos indicaban números negativos dibujando un trazo diagonal a través del dígito más a la derecha distinto de cero del número positivo correspondiente.
Los matemáticos clásicos griegos e indios hicieron estudios de la teoría de los números racionales, como parte del estudio general de la teoría de números.
El concepto de fracciones decimales está estrechamente relacionado con la notación del valor de la posición decimal; los dos parecen haberse desarrollado en tándem.
Sin embargo, Pitágoras creía en la absolutidad de los números, y no podía aceptar la existencia de números irracionales.
En el siglo XVII, los matemáticos generalmente usaban fracciones decimales con notación moderna.
En 1872, la publicación de las teorías de Karl Weierstrass (por su alumno E. Kossak), Eduard Heine, Georg Cantor y Richard Dedekind se produjo.
Weierstrass, Cantor y Heine basan sus teorías en series infinitas, mientras que Dedekind funda la suya en la idea de un corte (Schnitt) en el sistema de números reales, separando todos los números racionales en dos grupos que tienen ciertas propiedades características.
Por lo tanto, era necesario considerar el conjunto más amplio de números algebraicos (todas las soluciones a las ecuaciones polinómicas).
Aristóteles definió la noción occidental tradicional del infinito matemático.
Pero el siguiente gran avance en la teoría fue hecho por Georg Cantor; en 1895 publicó un libro sobre su nueva teoría de conjuntos, introduciendo, entre otras cosas, números transfinitos y formulando la hipótesis del continuo.
Una versión geométrica moderna del infinito está dada por la geometría proyectiva, que introduce "puntos ideales en el infinito", uno para cada dirección espacial.
La idea de la representación gráfica de números complejos había aparecido, sin embargo, ya en 1685, en De álgebra tractatus de Wallis.
En 240 aC, Eratóstenes utilizó la criba de Eratóstenes para aislar rápidamente los números primos.
Otros resultados relativos a la distribución de los números primos incluyen la prueba de Euler de que la suma de los recíprocos de los números primos diverge, y la conjetura de Goldbach, que afirma que cualquier número par suficientemente grande es la suma de dos números primos.
Tradicionalmente, la secuencia de números naturales comenzó con 1 (0 ni siquiera se consideraba un número para los antiguos griegos).
En este sistema de base 10, el dígito más a la derecha de un número natural tiene un valor posicional de 1, y cada dos dígitos tiene un valor posicional diez veces mayor que el valor posicional del dígito a su derecha.
Los números negativos generalmente se escriben con un signo negativo (un signo menos).
Aquí viene la letra Z.
Las fracciones pueden ser mayores que, menores que o iguales a 1 y también pueden ser positivas, negativas o 0.
El siguiente párrafo se centrará principalmente en los números reales positivos.
Así, por ejemplo, una mitad es 0,5, una quinta parte es 0,2, una décima parte es 0,1 y una quincuagésima parte es 0,02.
No solo estos ejemplos prominentes, sino que casi todos los números reales son irracionales y, por lo tanto, no tienen patrones repetitivos y, por lo tanto, no tienen un número decimal correspondiente.
Dado que ni siquiera se conserva el segundo dígito después del decimal, los siguientes dígitos no son significativos.
Por ejemplo, 0.999..., 1.0, 1.00, 1.000, ..., todos representan el número natural 1.
Finalmente, si todos los dígitos de un número son 0, el número es 0, y si todos los dígitos de un número son una cadena interminable de 9, puede colocar los nueves a la derecha del decimal y agregar uno a la cadena de 9s a la izquierda del decimal.
Por lo tanto, los números reales son un subconjunto de los números complejos.
El teorema fundamental del álgebra afirma que los números complejos forman un campo algebraicamente cerrado, lo que significa que cada polinomio con coeficientes complejos tiene una raíz en los números complejos.
Los números primos han sido ampliamente estudiados durante más de 2000 años y han dado lugar a muchas preguntas, solo algunas de las cuales han sido respondidas.
Los números reales que no son números racionales se llaman números irracionales.
Los números computables son estables para todas las operaciones aritméticas habituales, incluido el cálculo de las raíces de un polinomio, y por lo tanto forman un campo cerrado real que contiene los números algebraicos reales.
Una razón es que no hay algoritmo para probar la igualdad de dos números computables.
El sistema de números que resulta depende de qué base se utiliza para los dígitos: cualquier base es posible, pero una base de números primos proporciona las mejores propiedades matemáticas.
El primero da el orden del conjunto, mientras que el segundo da su tamaño.
Esta base estándar hace que los números complejos sean un plano cartesiano, llamado el plano complejo.
Los números complejos de valor absoluto uno forman el círculo unitario.
En la coloración de dominio, las dimensiones de salida están representadas por el color y el brillo, respectivamente.
El trabajo sobre el problema de los polinomios generales condujo en última instancia al teorema fundamental del álgebra, que muestra que con números complejos, existe una solución para cada ecuación polinómica de grado uno o superior.
Las memorias de Wessel aparecieron en las Actas de la Academia de Copenhague, pero pasaron en gran medida desapercibidas.
Los escritores clásicos posteriores en la teoría general incluyen a Richard Dedekind, Otto Hoelder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass y muchos otros.
El uso de números imaginarios no fue ampliamente aceptado hasta la obra de Leonhard Euler (1707-1783) y Carl Friedrich Gauss (1777-1855).
Los enteros forman el grupo más pequeño y el anillo más pequeño que contiene los números naturales.
Es el prototipo de todos los objetos de tal estructura algebraica.
Los tipos de datos de aproximación de enteros de longitud fija (o subconjuntos) se denotan int o Integer en varios lenguajes de programación (como Algol68, C, Java, Delphi, etc.).
Estas son propiedades demostrables de los números racionales y sistemas numéricos posicionales, y no se utilizan como definiciones en matemáticas.
Dado que el triángulo es isósceles, a - b).
Puesto que c es par, dividiendo c por 2 se obtiene un entero.
Sustituyendo 4y2 por c2 en la primera ecuación (c2 x 2b2) nos da 4y2 x 2b2.
Como b2 es par, b debe ser par.
Sin embargo, esto contradice la suposición de que no tienen factores comunes.
Hippasus, sin embargo, no fue elogiado por sus esfuerzos: según una leyenda, hizo su descubrimiento mientras estaba en el mar, y posteriormente fue arrojado por la borda por sus compañeros pitagóricos "... por haber producido un elemento en el universo que negaba la ... doctrina de que todos los fenómenos en el universo pueden reducirse a números enteros y sus proporciones".
Por ejemplo, considere un segmento de línea: este segmento se puede dividir por la mitad, que la mitad se divide por la mitad, la mitad de la mitad por la mitad, y así sucesivamente.
Esto es precisamente lo que Zenón trató de demostrar.
En la mente de los griegos, refutar la validez de un punto de vista no necesariamente prueba la validez de otro, y por lo tanto una mayor investigación tenía que ocurrir.
Una magnitud "... no era un número, sino que representaba entidades como segmentos de línea, ángulos, áreas, volúmenes y tiempo que podrían variar, como diríamos, continuamente.
Debido a que no se asignaron valores cuantitativos a las magnitudes, Eudoxus pudo entonces dar cuenta de las proporciones conmensurables e inconmensurables definiendo una proporción en términos de su magnitud y proporción como una igualdad entre dos proporciones.
Esta inconmensurabilidad se trata en los Elementos de Euclides, Libro X, Proposición 9.
De hecho, en muchos casos las concepciones algebraicas fueron reformuladas en términos geométricos.
La comprensión de que alguna concepción básica dentro de la teoría existente estaba en desacuerdo con la realidad requería una investigación completa y exhaustiva de los axiomas y suposiciones que subyacen en esa teoría.
Sin embargo, el historiador Carl Benjamin Boyer escribe que “tales afirmaciones no están bien fundamentadas y es poco probable que sean ciertas”.
Los matemáticos como Brahmagupta (en 628 dC) y Bhaskara I (en 629 dC) hicieron contribuciones en esta área al igual que otros matemáticos que siguieron.
El año 1872 vio la publicación de las teorías de Karl Weierstrass (por su alumno Ernst Kossak), Eduard Heine (Diario de Crelle, 74), Georg Cantor (Annalen, 5), y Richard Dedekind.
Weierstrass, Cantor y Heine basan sus teorías en series infinitas, mientras que Dedekind funda la suya en la idea de un corte (Schnitt) en el sistema de todos los números racionales, separándolos en dos grupos que tienen ciertas propiedades características.
Dirichlet también agregó a la teoría general, al igual que numerosos contribuyentes a las aplicaciones del tema.
Esto afirma que cada entero tiene una factorización única en primos.
Para mostrar esto, supongamos que dividimos los enteros n por m (donde m es distinto de cero).
Si 0 nunca se produce, entonces el algoritmo puede ejecutarse como máximo m 1 pasos sin usar ningún resto más de una vez.
En matemáticas, los números naturales son los utilizados para contar (como en "hay seis monedas en la mesa") y ordenar (como en "esta es la tercera ciudad más grande del país").
Estas cadenas de extensiones hacen que los números naturales estén canónicamente incrustados (identificados) en los otros sistemas numéricos.
El primer gran avance en la abstracción fue el uso de números para representar números.
